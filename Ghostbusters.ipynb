{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXpsHChxFQYy"
   },
   "source": [
    "**Ghostbusters: Co-occurence of varied class pairs, in the training and test Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cM87qhpOFyij"
   },
   "source": [
    "Setup python libraries\n",
    "\n",
    "1.   TensorFlow\n",
    "2.   Keras\n",
    "3.   sklearn\n",
    "4.   openCV\n",
    "5.   Random\n",
    "6.   NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DtVn4Z3xGWy8"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVyrPHuiGcfz"
   },
   "source": [
    "Setup Variables\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KL5CYADv1vpZ"
   },
   "outputs": [],
   "source": [
    "num_of_letters = 26 # @param {type:\"integer\"}\n",
    "num_of_train_images = 2500 # @param {type:\"integer\"}\n",
    "num_of_train_images_ub = 200 # @param {type:\"integer\"}\n",
    "batch_size = 64 # @param {type:\"integer\"}\n",
    "biased_ratio = 32 # @param {type:\"integer\"}\n",
    "add_unbiased = 0 # @param {type:\"integer\"}\n",
    "test_degree = 3 # @param {type:\"integer\"}\n",
    "train_degree = 3 # @param {type:\"integer\"}\n",
    "max_num = 2 # @param {type:\"integer\"}\n",
    "testOrTrain = 1 # @param {type:\"integer\"}\n",
    "times_addition = 0 # @param {type:\"integer\"}\n",
    "alphabetOrSpots = 1 # @param {type:\"integer\"} Alphabets - 1, Spots - 0\n",
    "f1Values = [0]\n",
    "f1MaxValues = [0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2Y5GE2NIgDN"
   },
   "source": [
    "Image Generation Functions \n",
    "\n",
    "1. basicText - generates a 64, 64 image with the letter and at the locationmentioned as input.\n",
    "2. genAlphabetImage - takes the int value of the character as the input, generates the location of the alphabet on the canvas randomly.\n",
    "3. imageWithDots - generate a 64, 64 image with dots in random locations.\n",
    "4. resizeImageForFour - resizing function used while combining the 4 alphabet images.\n",
    "5. genImage - generate the image based on the letter list given as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ThFzx0B2cPQR"
   },
   "outputs": [],
   "source": [
    "def basicText(text, fontScale, x_direction, y_direction):\n",
    "  image = np.zeros((64,64))\n",
    "  # font \n",
    "  font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "  org = (x_direction,y_direction)  \n",
    "  color = (255, 255)  \n",
    "  thickness = 2\n",
    "  image = cv2.putText(image, text, org, font, fontScale,  \n",
    "                 color, thickness, cv2.LINE_AA, False)\n",
    "  return image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-SkzjPn0y6Mj"
   },
   "outputs": [],
   "source": [
    "def genAlphabetImage(text):\n",
    "  val = chr(text)\n",
    "  x_direction = random.randint(0, 32)\n",
    "  y_direction = random.randint(28,55)\n",
    "  image = basicText(val, 1, x_direction, y_direction)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "AVhIyI0vQrI6"
   },
   "outputs": [],
   "source": [
    "def imageWithDots():\n",
    "  image = np.zeros((64,64))\n",
    "  # font \n",
    "  color = (255, 255)  \n",
    "  thickness = 2\n",
    "  text = '.'\n",
    "  font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "  for i in range(15):\n",
    "    x_direction = random.randint(0, 63)\n",
    "    y_direction = random.randint(0, 63)\n",
    "    org = (x_direction,y_direction)  \n",
    "    image = cv2.putText(image, text, org, font, 1,  \n",
    "                  color, thickness, cv2.LINE_AA, False)\n",
    "  return image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MEu3o881rb74"
   },
   "outputs": [],
   "source": [
    "def resizeImageForFour(image, num):\n",
    "  new2 = cv2.resize(image,None,fx=1/num, fy=1/num, interpolation = cv2.INTER_CUBIC)\n",
    "  new2 = new2.reshape(new2.shape[0], new2.shape[1])\n",
    "  return new2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "RpS27aaSpp4F"
   },
   "outputs": [],
   "source": [
    "def genImage(letterList, numOfTimes):\n",
    "  finalImage = np.zeros((64,64))\n",
    "  valList = [[0,0],[0,32],[32,0],[32,32]]\n",
    "  label = np.zeros(num_of_letters)\n",
    "  temp = ''\n",
    "  letterList = list(dict.fromkeys(letterList))\n",
    "  random.shuffle(valList)\n",
    "  random.shuffle(letterList)\n",
    "  for j in range(len(letterList)):\n",
    "    if alphabetOrSpots == 1:\n",
    "      img = genAlphabetImage((65 + letterList[j]))\n",
    "    elif alphabetOrSpots == 0:\n",
    "      img = imageWithDots()\n",
    "    label[letterList[j]] = 1\n",
    "    resizeImage = resizeImageForFour(img, 2)\n",
    "    finalImage[valList[j][0]:valList[j][0] + 32,valList[j][1]:valList[j][1] + 32] = resizeImage\n",
    "    temp = temp + chr(65 + letterList[j]) + '_'\n",
    "  temp = '../pics/' + temp + str(numOfTimes)+'_'+str(testOrTrain)+'.jpg'\n",
    "  cv2.imwrite('{}'.format(str(temp)), finalImage)\n",
    "  return finalImage, np.array(label), temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Generation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "yDEuMxcygZn6"
   },
   "outputs": [],
   "source": [
    "def imageGenBasedOnMatrix(matrixDegree, arrayVal):\n",
    "  images = []\n",
    "  labels = []\n",
    "  imageNames = []\n",
    "  letterList = []\n",
    "  if matrixDegree == 1:\n",
    "    for i in range(len(arrayVal)):\n",
    "      letterList.clear()\n",
    "      letterList.append(i)\n",
    "      for j in range(int(arrayVal[i])):\n",
    "        image, label, imageName = genImage(letterList, j)\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "        imageNames.append(imageName)\n",
    "  elif matrixDegree == 2:\n",
    "    for i in range(arrayVal.shape[0]):\n",
    "      for j in range(arrayVal.shape[1]):\n",
    "        for k in range(int(arrayVal[i][j])):\n",
    "          letterList = [i, j]\n",
    "          random.shuffle(letterList)\n",
    "          image, label, imageName = genImage(letterList, k)\n",
    "          images.append(image)\n",
    "          labels.append(label)\n",
    "          imageNames.append(imageName)\n",
    "  elif matrixDegree == 3:\n",
    "    for i in range(arrayVal.shape[0]):\n",
    "      for j in range(arrayVal.shape[1]):\n",
    "        for k in range(arrayVal.shape[2]):\n",
    "          for l in range(int(arrayVal[i][j][k])):\n",
    "            letterList = [i,j,k]\n",
    "            random.shuffle(letterList)\n",
    "            image, label, imageName = genImage(letterList, l)\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "            imageNames.append(imageName)\n",
    "  elif matrixDegree == 4:\n",
    "    for i in range(arrayVal.shape[0]):\n",
    "      for j in range(arrayVal.shape[1]):\n",
    "        for k in range(arrayVal.shape[2]):\n",
    "          for m in range(arrayVal.shape[3]):\n",
    "            for l in range(int(arrayVal[i][j][k][m])):\n",
    "              letterList = [i,j,k,m]\n",
    "              random.shuffle(letterList)\n",
    "              image, label, imageName = genImage(letterList, l)\n",
    "              images.append(image)\n",
    "              labels.append(label)\n",
    "              imageNames.append(imageName)\n",
    "  return images, labels, imageNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Z6ohXBh7Xpeq"
   },
   "outputs": [],
   "source": [
    "def imageGenBasedOnCombinations(comboListVal):\n",
    "  imageList = []\n",
    "  labelList = []\n",
    "  imageNameList = []\n",
    "  for i in range(len(comboListVal)):\n",
    "    letterList = []\n",
    "    for j in range(len(list(comboListVal[i]))):\n",
    "      letterList.append( ord (list(comboListVal[i])[j]) - 65)\n",
    "    image, label, imageName = genImage(letterList, 1)\n",
    "    imageList.append(image)\n",
    "    labelList.append(label)\n",
    "    imageNameList.append(imageName)\n",
    "  return imageList, labelList, imageNameList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TkG1w7GQcYYF"
   },
   "outputs": [],
   "source": [
    "def imageGenBasedOnLabels(array, maxValue, numOfUComb):\n",
    "  imageList = []\n",
    "  labelList = []\n",
    "  imageNameList = []\n",
    "  comboListVal = []\n",
    "  maxVal = np.amax(array)\n",
    "  print(maxVal)\n",
    "  while len(comboListVal) < numOfUComb:\n",
    "    for i in range(num_of_letters):\n",
    "      for j in range(i+1, num_of_letters):\n",
    "        if array[i][j] >= (maxVal - maxValue) and len(comboListVal) < numOfUComb and i != j:\n",
    "          k=random.randint(0, 25)\n",
    "          temp = [i, j, k]\n",
    "          if temp not in comboListVal:\n",
    "            comboListVal.append([i, j, k])\n",
    "        elif len(comboListVal) > numOfUComb:\n",
    "          break\n",
    "  for i in range(len(comboListVal)):\n",
    "    letterList = np.array(comboListVal[i])\n",
    "    image, label, imageName = genImage(letterList, 1)\n",
    "    imageList.append(image)\n",
    "    labelList.append(label)\n",
    "    imageNameList.append(imageName)\n",
    "  return imageList, labelList, imageNameList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DMS9fIbYaRrS"
   },
   "outputs": [],
   "source": [
    "def createOddEven(testImages, testLabels):\n",
    "  trainImages = []\n",
    "  trainLabels = []\n",
    "  for i in range(len(testImages)):\n",
    "    indexes = []\n",
    "    for j in range(len(testLabels[i])):\n",
    "      if testLabels[i][j] == 1:\n",
    "        indexes.append(j)\n",
    "    if (indexes[0]%2 == 0 and indexes[1]%2 == 0 and indexes[2]%2 == 0) or  (indexes[0]%2 != 0 and indexes[1]%2 != 0 and indexes[2]%2 != 0):\n",
    "      trainImages.append(testImages[i])\n",
    "      trainLabels.append(testLabels[i])\n",
    "  return trainImages, trainLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "gOoJLoENGGMh"
   },
   "outputs": [],
   "source": [
    "def createRatioBased(ratio, testImages, testLabels):\n",
    "  trainImages = []\n",
    "  trainLabels = []\n",
    "  for i in range(len(testImages)):\n",
    "    if i%ratio == 0:\n",
    "      trainImages.append(testImages[i])\n",
    "      trainLabels.append(testLabels[i])\n",
    "  print(len(trainImages))\n",
    "  return trainImages, trainLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "MzTo08vd530t"
   },
   "outputs": [],
   "source": [
    "def dataSetGen():\n",
    "  global testOrTrain\n",
    "  if test_degree == 1:\n",
    "    testArrayVal = np.array([1]*num_of_letters)\n",
    "  if test_degree == 2:\n",
    "    testArrayVal = np.array([[1]*num_of_letters]*num_of_letters)\n",
    "    for i in range(num_of_letters):\n",
    "      for j in range(num_of_letters):\n",
    "        if i == j:\n",
    "          testArrayVal[i][j] = 0\n",
    "        if i > j:\n",
    "          testArrayVal[i][j] = 0\n",
    "  if test_degree == 3:\n",
    "    testArrayVal = np.array([[[1]*num_of_letters]*num_of_letters]*num_of_letters)\n",
    "    for i in range(num_of_letters):\n",
    "      for j in range(num_of_letters):\n",
    "        for k in range(num_of_letters):\n",
    "          if i == j or j == k or i == k:\n",
    "            testArrayVal[i][j][k] = 0\n",
    "          if i > j or j > k or i > k:\n",
    "            testArrayVal[i][j][k] = 0\n",
    "  imageTesting, testLabels, testImages = imageGenBasedOnMatrix(test_degree, testArrayVal)\n",
    "  TestDataset = create_dataset(testImages, testLabels, False)\n",
    "  return TestDataset, testLabels, testImages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing pipeline\n",
    "\n",
    "1. Functions - \n",
    "    - parse_function (takes the input of filename and provides post processed and normalised images)\n",
    "    - create_dataset (used for generating tf dataset for both training and test images)\n",
    "2. Input required is the Image names and the data labels\n",
    "3. TensorFlow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wzfUUrURJPiC"
   },
   "outputs": [],
   "source": [
    "def parse_function(filename, label):\n",
    "    # Read an image from a file\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    # Decode it into a dense vector\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    # Resize it to fixed shapee \n",
    "    image_resized = tf.image.resize(image_decoded, [96, 96])\n",
    "    # Normalize it from [0, 255] to [0.0, 1.0]\n",
    "    image_normalized = image_resized / 255.0\n",
    "    image_normalized = tf.image.convert_image_dtype(image_normalized, dtype=tf.float32)\n",
    "    return image_normalized, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "HXYHNuvWIyF3"
   },
   "outputs": [],
   "source": [
    "def create_dataset(filenames, labels, is_training=True):    \n",
    "    # Create a first dataset of file paths and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    # Parse and preprocess observations in parallel\n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE )\n",
    "    dataset = dataset.cache()\n",
    "    if (is_training):\n",
    "      dataset = dataset.shuffle(20000, reshuffle_each_iteration=True)\n",
    "    # Batch the data for multiple steps\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    # Fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE) \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model used - \n",
    "\n",
    "1. Input shape - (32, 32, 1)\n",
    "2. Activation function used - RELU\n",
    "3. Last layer activation function - Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "gLapDIz4cSZv"
   },
   "outputs": [],
   "source": [
    "def modelDef():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\", input_shape=(64,64,1)))\n",
    "  model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(layers.Conv2D(filters=48, kernel_size=(3, 3), activation='relu'))\n",
    "  model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "  model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(layers.Flatten())\n",
    "  model.add(layers.Dense(64, activation='relu'))\n",
    "  model.add(layers.Dropout(.3))\n",
    "  model.add(layers.Dense(num_of_letters, activation='sigmoid'))\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra functions - \n",
    "\n",
    "1. Accuracy calculation functions\n",
    "2. F1 score calculation functions\n",
    "3. Correlationn matrix gen functions\n",
    "4. Distance matrix gen functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "t6k19dH7ujGq"
   },
   "outputs": [],
   "source": [
    "def Sort(sub_li): \n",
    "    l = len(sub_li) \n",
    "    for i in range(0, l): \n",
    "        for j in range(0, l-i-1): \n",
    "            if (sub_li[j][1] > sub_li[j + 1][1]): \n",
    "                tempo = sub_li[j] \n",
    "                sub_li[j]= sub_li[j + 1] \n",
    "                sub_li[j + 1]= tempo \n",
    "    return sub_li "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Gp6O7KRGMw3V"
   },
   "outputs": [],
   "source": [
    "def calculate_labels(pred):\n",
    "  predict_labels = []\n",
    "  for i in range(0, len(pred)):\n",
    "    temp = pred[i]\n",
    "    val = 0.5\n",
    "    labels = np.zeros(num_of_letters)\n",
    "    for j in range(0, num_of_letters):\n",
    "      if(temp[j] >= val):\n",
    "        labels[j] = 1\n",
    "    predict_labels.append(labels)\n",
    "  return predict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "KVBOq5UPksOJ"
   },
   "outputs": [],
   "source": [
    "def label_to_alphabet(cal):\n",
    "  predictedLabels = []\n",
    "  for i in range(0, len(cal)):\n",
    "    setList = []\n",
    "    for j in range(0, num_of_letters):\n",
    "      if(cal[i][j] == 1):\n",
    "        temp = chr(j + 65)\n",
    "        setList.append(temp)\n",
    "    predictedLabels.append(set(setList))\n",
    "  return predictedLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "MfzjXauQDRR0"
   },
   "outputs": [],
   "source": [
    "def f1score(y_test, y_predicted, array):\n",
    "  global f1Values\n",
    "  global f1MaxValues\n",
    "  temp_f1 = 0\n",
    "  temp_recall = 0\n",
    "  temp_precision = 0\n",
    "  temp_acc = 0\n",
    "  f1Val = []\n",
    "  distVal = []\n",
    "  for i in range(len(y_test)):\n",
    "    temp_f1 = temp_f1 + f1_score(y_test[i], y_predicted[i])\n",
    "    temp_recall = temp_recall + recall_score(y_test[i], y_predicted[i])\n",
    "    temp_precision = temp_precision + precision_score(y_test[i], y_predicted[i])\n",
    "    temp_acc = temp_acc + accuracy_score(y_test[i], y_predicted[i])\n",
    "    dist_temp = imageValueFunc(array, y_test[i])\n",
    "    dist_temp_max = imageValueMaxFunc(array, y_test[i])\n",
    "    f1Val.append([f1_score(y_test[i], y_predicted[i]), dist_temp])\n",
    "    distVal.append([f1_score(y_test[i], y_predicted[i]), dist_temp_max])\n",
    "  f1score = temp_f1 / len(y_test)\n",
    "  precision = temp_precision / len(y_test)\n",
    "  recall = temp_recall / len(y_test)\n",
    "  acc = temp_acc / len(y_test)\n",
    "  f1Values = f1Val\n",
    "  f1MaxValues = distVal\n",
    "  return acc, f1score, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "M_4yy6_q37oa"
   },
   "outputs": [],
   "source": [
    "def accuracyCalc(labelVals, predictedLabelVals):\n",
    "  Sum = 0 \n",
    "  accValList = []\n",
    "  labelsList = []\n",
    "  for i in range(0, len(labelVals)):\n",
    "    currentLabelValue = labelVals[i]\n",
    "    currentPredLabelValue = predictedLabelVals[i]\n",
    "    setIntersection = currentPredLabelValue.intersection(currentLabelValue)\n",
    "    x = float(len(setIntersection)/len(currentLabelValue))\n",
    "    #print(len(currentPredLabelValue), len(currentLabelValue))\n",
    "    if len(currentLabelValue) == len(currentPredLabelValue) and x == 1:\n",
    "      labelsList.append(currentLabelValue)\n",
    "      accValList.append(1)\n",
    "      Sum = Sum + x\n",
    "    else:\n",
    "      labelsList.append(currentLabelValue)\n",
    "      accValList.append(0)\n",
    "  Accuracy = Sum / len(labelVals)\n",
    "  return Accuracy, labelsList, accValList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "o1tuvY9tk3xh"
   },
   "outputs": [],
   "source": [
    "def accuracyFunc(y_pred, y_test, array):\n",
    "  y_predicted = calculate_labels(y_pred)\n",
    "  accuracy, f1_score, precision, recall  = f1score(y_test, y_predicted, array)\n",
    "  y_predicted = label_to_alphabet(y_predicted)\n",
    "  y_tested = label_to_alphabet(y_test)\n",
    "  count, labelList, accValList = accuracyCalc(y_tested, y_predicted)\n",
    "  print(round(count, 3), round(accuracy, 3), round(f1_score, 3), round(precision, 3), round(recall,3))\n",
    "  return round(f1_score, 3), labelList, accValList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "99NqoXQzH1yD"
   },
   "outputs": [],
   "source": [
    "def sumOfListVals(matrixDegree, arrayValue):\n",
    "  sumVal = 0\n",
    "  if matrixDegree == 1:\n",
    "    for i in range(arrayValue.shape[0]):\n",
    "      sumVal = sumVal + arrayValue[i]\n",
    "  elif matrixDegree == 2:\n",
    "    for i in range(arrayValue.shape[0]):\n",
    "      for j in range(arrayValue.shape[1]):\n",
    "        sumVal = sumVal + arrayValue[i][j]\n",
    "  elif matrixDegree == 3:\n",
    "    for i in range(arrayValue.shape[0]):\n",
    "      for j in range(arrayValue.shape[1]):\n",
    "        for k in range(arrayValue.shape[2]):\n",
    "          sumVal = sumVal + arrayValue[i][j][k]\n",
    "  return sumVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "X46kUncXHmbY"
   },
   "outputs": [],
   "source": [
    "def probabilityGenMatrix(matrixDegree, arrayValue):\n",
    "  probability = []\n",
    "  combo = [] \n",
    "  sumVal = sumOfListVals(matrixDegree, arrayValue)\n",
    "  if matrixDegree == 1:\n",
    "    for i in range(num_of_letters):\n",
    "      temp = arrayValue[i]/(sumVal)\n",
    "      probability.append(temp)\n",
    "      combo.append([chr(65+i)])\n",
    "  elif matrixDegree == 2:\n",
    "    for i in range(num_of_letters):\n",
    "      for j in range(num_of_letters):\n",
    "        temp = arrayValue[i][j]/(sumVal)\n",
    "        if(i != j):\n",
    "          probability.append(temp)\n",
    "          combo.append([chr(65+i), chr(65+j)])\n",
    "  elif matrixDegree == 3:\n",
    "    for i in range(arrayValue.shape[0]):\n",
    "      for j in range(arrayValue.shape[1]):\n",
    "        for k in range(arrayValue.shape[2]):\n",
    "          temp = arrayValue[i][j][k]/(sumVal)\n",
    "          if not (i == j or j == k or i == k):\n",
    "            probability.append(temp)\n",
    "            combo.append([chr(65+i), chr(65+j), chr(65+k)])\n",
    "  return probability, combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "8lFDpRqcuqG6"
   },
   "outputs": [],
   "source": [
    "def createComboList(listValues, probability):\n",
    "  comboListVal = []\n",
    "  listVal = Sort(listValues)\n",
    "  for i in range(len(listVal)):\n",
    "    if (listVal[i][1] <= probability):\n",
    "      comboListVal.append(listVal[i][2])\n",
    "  return comboListVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "aI4BXZlEJAeU"
   },
   "outputs": [],
   "source": [
    "def probabilityListAndCorrectness(arrayValue, TestDataset, testLabels):\n",
    "  probabilityList, comboList = probabilityGenMatrix(test_degree, arrayValue)\n",
    "  ypred = model.predict(TestDataset)\n",
    "  acc , combList, correctness = accuracyFunc(ypred, testLabels)\n",
    "  combinedList = []\n",
    "  print(arrayValue.shape)\n",
    "  for i in range(len(probabilityList)):\n",
    "    #print(combList[i], comboList[i])\n",
    "    correctVal = correctness[combList.index(set(comboList[i]))]\n",
    "    combinedList.append([correctVal, probabilityList[i], comboList[i]])\n",
    "  return acc, probabilityList, correctness, combinedList, combList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "cnMiT6M5G9-P"
   },
   "outputs": [],
   "source": [
    "def numOfDataInc(trainImages, trainLabels, totalLength):\n",
    "  global testOrTrain\n",
    "  trainLabelsNoice = trainLabels\n",
    "  combinationList = label_to_alphabet(trainLabelsNoice)\n",
    "  numOfVals = int(totalLength/len(trainLabels)) + 1\n",
    "  for i in range(numOfVals):\n",
    "    testOrTrain = i + 2\n",
    "    _, trainLabelsComb, trainImagesComb = imageGenBasedOnCombinations(combinationList)\n",
    "    for j in range(len(trainLabelsComb)):\n",
    "      if len(trainImages) == totalLength:\n",
    "        break\n",
    "      trainImages.append(trainImagesComb[j])\n",
    "      trainLabels.append(trainLabelsComb[j])\n",
    "  return trainImages, trainLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "l-gRStGgUvSF"
   },
   "outputs": [],
   "source": [
    "def correlationMatrix(trainLabels):\n",
    "  if test_degree == 2:\n",
    "    w = np.vstack(trainLabels)\n",
    "    arr = np.zeros([num_of_letters,num_of_letters])\n",
    "    for i in range(0, w.shape[0]):\n",
    "      for j in range(0, w.shape[1]):\n",
    "        if(w[i][j] == 1):\n",
    "          for k in range(j+1, w.shape[1]):\n",
    "            if(w[i][k] == 1):\n",
    "              arr[j][k] += 1\n",
    "              arr[k][j] += 1\n",
    "  elif test_degree == 1:\n",
    "    arr = np.zeros((num_of_letters))\n",
    "    for i in range(len(trainLabels)):\n",
    "      for j in range(len(trainLabels[i])):\n",
    "        if trainLabels[i][j] == 1:\n",
    "          arr[j] = arr[j] + 1\n",
    "  elif test_degree == 3:\n",
    "    w = np.vstack(trainLabels)\n",
    "    arr = np.zeros([num_of_letters,num_of_letters,num_of_letters])\n",
    "    for i in range(0, w.shape[0]):\n",
    "      for j in range(0, w.shape[1]):\n",
    "        if(w[i][j] == 1):\n",
    "          for k in range(j+1, w.shape[1]):\n",
    "            if(w[i][k] == 1):\n",
    "              for l in range(k+1, w.shape[1]):\n",
    "                if(w[i][l] == 1):\n",
    "                  arr[j][k][l] += 1\n",
    "                  arr[k][j][l] += 1\n",
    "                  arr[j][l][k] += 1\n",
    "                  arr[k][l][j] += 1\n",
    "                  arr[l][k][j] += 1\n",
    "                  arr[l][j][k] += 1\n",
    "  return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "TqrQLToq_CNZ"
   },
   "outputs": [],
   "source": [
    "def correlationmt(labels):\n",
    "  arr = np.zeros([num_of_letters,num_of_letters])\n",
    "  for i in range(len(labels)):\n",
    "    for j in range(num_of_letters):\n",
    "      if labels[i][j] == 1:\n",
    "        for k in range(num_of_letters):\n",
    "          if labels[i][k] == 1:\n",
    "            arr[j][k] = 1\n",
    "            arr[k][j] = 1\n",
    "  return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "fS_mlcYUAjiZ"
   },
   "outputs": [],
   "source": [
    "def calculateDist(corr):\n",
    "  dist = np.zeros([num_of_letters,num_of_letters])\n",
    "  for i in range(num_of_letters):\n",
    "    for j in range(num_of_letters):\n",
    "      if i != j:\n",
    "        if corr[i][j] == 1 and dist[i][j] == 0:\n",
    "          dist[i][j] = 1\n",
    "          dist[j][i] = 1\n",
    "        elif dist[i][j] == 0:\n",
    "          for k in range(num_of_letters):\n",
    "            if corr[k][j] == 1 and k != j and k != i:\n",
    "              #print(\"i:\", i, \"j:\", j, \"k:\", k)\n",
    "              if corr[i][k] == 1:\n",
    "                temp1 = 2\n",
    "                if temp1 < dist[i][j] or dist[i][j] == 0:\n",
    "                  dist[i][j] = int(temp1)\n",
    "                  dist[j][i] = int(temp1)\n",
    "              elif dist[i][k] != 0:\n",
    "                temp = dist[i][k] + 1\n",
    "                if temp < dist[i][j] or dist[i][j] == 0:\n",
    "                  dist[i][j] = int(temp)\n",
    "                  dist[j][i] = int(temp)\n",
    "              #print(dist[i][j])\n",
    "  maxVal = np.amax(dist)\n",
    "  for i in range(num_of_letters):\n",
    "    for j in range(num_of_letters):\n",
    "      if dist[i][j] == 0 and i !=j :\n",
    "        dist[i][j] = maxVal + 1\n",
    "      elif i == j:\n",
    "        dist[i][j] = 0 \n",
    "  return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "9L8qxIi0r6pn"
   },
   "outputs": [],
   "source": [
    "def imageValueMaxFunc(array, label):\n",
    "  dist = 0\n",
    "  for i in range(len(label)):\n",
    "    if label[i] == 1:\n",
    "      for j in range(i+1, len(label)):\n",
    "        if label[j] == 1:\n",
    "          if array[i][j] == 0:\n",
    "            if dist < 10:\n",
    "              dist = 10\n",
    "          else:\n",
    "            if dist < array[i][j]:\n",
    "              dist = array[i][j]\n",
    "  return int(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "QkoffGFHwFP2"
   },
   "outputs": [],
   "source": [
    "def imageValueFunc(array, label):\n",
    "  dist = 0\n",
    "  for i in range(len(label)):\n",
    "    if label[i] == 1:\n",
    "      for j in range(i+1, len(label)):\n",
    "        if label[j] == 1:\n",
    "          if array[i][j] == 0:\n",
    "            dist = dist + 10\n",
    "          else:\n",
    "            dist = dist + array[i][j]\n",
    "  dist = dist / 3 \n",
    "  return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "BgvaJDfg7kRe"
   },
   "outputs": [],
   "source": [
    "def convertLabelsToSingleAlphabet(alphabet, trainImages, trainLabels):\n",
    "  trainLabelsNew = np.zeros((len(trainImages)))\n",
    "  count = 0 \n",
    "  for i in range(len(trainImages)):\n",
    "    if trainLabels[i][alphabet] == 1:\n",
    "      trainLabelsNew[i] = 1\n",
    "      count = count + 1\n",
    "  return trainImages, trainLabelsNew, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ryQruWs_UqKX"
   },
   "outputs": [],
   "source": [
    "def pickRandom(trainImages, trainLabels, numOfComb):\n",
    "  trainImagesNew = []\n",
    "  trainLabelsNew = []\n",
    "  for i in range(numOfComb):\n",
    "    temp = random.randint(0, 2599)\n",
    "    if trainImages[temp] not in trainImagesNew:\n",
    "      trainImagesNew.append(trainImages[temp])\n",
    "      trainLabelsNew.append(trainLabels[temp])\n",
    "  return _, trainLabelsNew, trainImagesNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incNumOfImages(trainImages, trainLabels, totalLength):\n",
    "    numOfVals = int(totalLength/len(trainLabels)) + 1\n",
    "    for i in range(numOfVals):\n",
    "        mnist_base.testOrTrain = i + 2\n",
    "        for j in range(len(trainLabels)):\n",
    "            combinationList = trainLabels[j]\n",
    "            _, trainLabelsComb, trainImagesComb = mnist_base.randomLocationGen(combinationList, imagesList, labelsList)\n",
    "            if len(trainImages) >= totalLength:\n",
    "                break\n",
    "            trainImages.append(trainImagesComb)\n",
    "            trainLabels.append(trainLabelsComb)\n",
    "    return trainImages, trainLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToArray(labelVals):\n",
    "    labels = []\n",
    "    for i in range(len(labelVals)):\n",
    "        label = np.zeros(10)\n",
    "        for j in range(len(labelVals[i])):\n",
    "            label[labelVals[i][j]] = 1\n",
    "        labels.append(label)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToNum(labelVals):\n",
    "    labels = []\n",
    "    for i in range(len(labelVals)):\n",
    "        label = []\n",
    "        for j in range(len(labelVals[i])):\n",
    "            if labelVals[i][j] == 1:\n",
    "                label.append(j)\n",
    "        labels.append(label)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRollingLabels(numOfObjects, start, end):\n",
    "    labels = []\n",
    "    if numOfObjects == 3:\n",
    "        for i in range(start, end):\n",
    "            for j in range(i+1, end):\n",
    "                for k in range(j+1, end):\n",
    "                    labels.append([i,j,k])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRollingWindow(windowSize, numOfObjects, testImages, testLabels):\n",
    "    trainImages = []\n",
    "    trainLabels = []\n",
    "    if len(testLabels[0]) == num_of_letters:\n",
    "        tempLabels = convertToNum(testLabels)\n",
    "    else:\n",
    "        tempLabels = testLabels\n",
    "    for i in range(0, (num_of_letters - windowSize + 1)):\n",
    "        selectLabels = createRollingLabels(numOfObjects, i , (i + windowSize))\n",
    "        print(selectLabels)\n",
    "        for j in range(len(selectLabels)):\n",
    "            indexVal = tempLabels.index(selectLabels[j])\n",
    "            trainImages.append(testImages[indexVal])\n",
    "            trainLabels.append(testLabels[indexVal])\n",
    "    return trainImages, trainLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createOddEvenNew(testImages, testLabels):\n",
    "  trainImages = []\n",
    "  trainLabels = []\n",
    "  for i in range(len(testImages)):\n",
    "    indexes = []\n",
    "    for j in range(len(testLabels[i])):\n",
    "      indexes.append(testLabels[i][j])\n",
    "    if (indexes[0]%2 == 0 and indexes[1]%2 == 0 and indexes[2]%2 == 0) or  (indexes[0]%2 != 0 and indexes[1]%2 != 0 and indexes[2]%2 != 0):\n",
    "      trainImages.append(testImages[i])\n",
    "      trainLabels.append(testLabels[i])\n",
    "  return trainImages, trainLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addLessConnectedComb(combinationListVals, trainImages, trainLabels, totalLength):\n",
    "    numOfVals = int(totalLength/len(combinationListVals)) + 1\n",
    "    for i in range(numOfVals):\n",
    "        for j in range(len(combinationListVals)):\n",
    "            mnist_base.testOrTrain = i + j + 2\n",
    "            combinationList = combinationListVals[j]\n",
    "            _, trainLabelsComb, trainImagesComb = mnist_base.randomLocationGen(combinationList, imagesList, labelsList)\n",
    "            if len(trainImages) >= totalLength:\n",
    "                break\n",
    "            trainImages.append(trainImagesComb)\n",
    "            trainLabels.append(trainLabelsComb)\n",
    "    return trainImages, trainLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectednessMeasure(trainLabels, testLabelsVal):\n",
    "    arrayVal = correlationmt(trainLabels)\n",
    "    #print(arrayVal)\n",
    "    connectedness = []\n",
    "    y_test_num = convertToNum(testLabelsVal)\n",
    "    for w in range(120):\n",
    "        '''i = y_test_num[w][0]\n",
    "        j = y_test_num[w][1]\n",
    "        k = y_test_num[w][2]'''\n",
    "        count = 0\n",
    "        for m in range(10):\n",
    "            counter = 0\n",
    "            for i in range(len(y_test_num[w])):\n",
    "                if arrayVal[y_test_num[w][i]][m] == 1:\n",
    "                    counter = counter + 1\n",
    "            '''if (arrayVal[i][m] == 1 and arrayVal[j][m] == 1 and arrayVal[k][m] == 1):\n",
    "                count = count + 1\n",
    "            elif (arrayVal[i][m] == 1 and arrayVal[j][m] == 1) or (arrayVal[j][m] == 1 and arrayVal[k][m] == 1) or (arrayVal[i][m] == 1 and arrayVal[k][m] == 1):\n",
    "                count = count + 2/3\n",
    "            elif (arrayVal[i][m] == 1 or arrayVal[j][m] == 1 or arrayVal[k][m] == 1):'''\n",
    "            count = count + counter/len(y_test_num[w])\n",
    "        connectedness.append([y_test_num[w],round(count/10, 2)])\n",
    "    return connectedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCombinationList(connectedness, numOfCombinations):\n",
    "    temp = connectedness\n",
    "    sortedVal = Sort(temp)\n",
    "    combList = []\n",
    "    for i in range(numOfCombinations):\n",
    "        combList.append(sortedVal[i][0])\n",
    "    return combList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "19/19 [==============================] - 2s 41ms/step - loss: 0.9791 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    " def modelLatest():\n",
    "    IMG_SIZE = (96, 96)\n",
    "    IMG_SHAPE = IMG_SIZE + (3,)\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                                   include_top=False)\n",
    "    inputs = tf.keras.Input(shape=(96, 96, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = global_average_layer(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = prediction_layer(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCombinationListRand(connectedness, numOfCombinations):\n",
    "    combList = []\n",
    "    tempList = []\n",
    "    for i in range(numOfCombinations):\n",
    "        temp = random.randint(0,119)\n",
    "        if temp not in tempList:\n",
    "            tempList.append(temp)\n",
    "            combList.append(connectedness[temp][0])\n",
    "    return combList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlationmtcount(labels):\n",
    "  arr = np.zeros([num_of_letters,num_of_letters])\n",
    "  for i in range(len(labels)):\n",
    "    for j in range(num_of_letters):\n",
    "      if labels[i][j] == 1:\n",
    "        for k in range(j + 1, num_of_letters):\n",
    "          if labels[i][k] == 1:\n",
    "            arr[j][k] = arr[j][k] + 1\n",
    "            arr[k][j] = arr[k][j] + 1\n",
    "  return arr\n",
    "\n",
    "def connectednessMeasureCount(trainLabels, testLabelsVal):\n",
    "    arrayVal = correlationmtcount(trainLabels)\n",
    "    print(arrayVal)\n",
    "    #print(arrayVal)\n",
    "    connectedness = []\n",
    "    y_test_num = convertToNum(testLabelsVal)\n",
    "    for w in range(120):\n",
    "        '''i = y_test_num[w][0]\n",
    "        j = y_test_num[w][1]\n",
    "        k = y_test_num[w][2]'''\n",
    "        count = 0\n",
    "        for m in range(10):\n",
    "            counter = 0\n",
    "            for i in range(len(y_test_num[w])):\n",
    "                if arrayVal[y_test_num[w][i]][m] != 0:\n",
    "                    counter = counter + arrayVal[y_test_num[w][i]][m]\n",
    "            '''if (arrayVal[i][m] == 1 and arrayVal[j][m] == 1 and arrayVal[k][m] == 1):\n",
    "                count = count + 1\n",
    "            elif (arrayVal[i][m] == 1 and arrayVal[j][m] == 1) or (arrayVal[j][m] == 1 and arrayVal[k][m] == 1) or (arrayVal[i][m] == 1 and arrayVal[k][m] == 1):\n",
    "                count = count + 2/3\n",
    "            elif (arrayVal[i][m] == 1 or arrayVal[j][m] == 1 or arrayVal[k][m] == 1):'''\n",
    "            count = count + counter/len(y_test_num[w])\n",
    "        connectedness.append([y_test_num[w],round(count/10, 2)])\n",
    "    return connectedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IxfyNCMWK8fR",
    "outputId": "8fdcd605-3061-4b3a-d712-9ebcc003fa29"
   },
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "test_degree=3\n",
    "alphabetOrSpots = 1\n",
    "testOrTrain = 0\n",
    "num_of_letters = 26\n",
    "_, testLabelsVal, testImagesVal = dataSetGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "MKPicQr2f1a8"
   },
   "outputs": [],
   "source": [
    "random.seed(100)\n",
    "test_degree=3\n",
    "alphabetOrSpots = 1\n",
    "testOrTrain = 1\n",
    "num_of_letters = 26\n",
    "_, trainLabelsVal, trainImagesVal = dataSetGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "from mnistBase import *\n",
    "\n",
    "mnist_base = mnistBase()\n",
    "images, labels = mnist_base.mnistBaseDef('train', False)\n",
    "mnist_base.imageSize = 96\n",
    "imagesList, labelsList = mnist_base.convertToList(images, labels)\n",
    "print(len(images))\n",
    "#print(labelsList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(10)\n",
    "\n",
    "model = modelLatest()\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "TestDataset = create_dataset(testImagesVal, testLabelsVal, False)\n",
    "model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "loss0, accuracy0 = model.evaluate(TestDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "_, trainLabelsVal, trainImagesVal = mnist_base.generateCompleteDataset(imagesList, labelsList) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)\n",
    "mnist_base.testOrTrain = 2\n",
    "_, testLabelsVal, testImagesVal = mnist_base.generateCompleteDataset(imagesList, labelsList) \n",
    "testImagesVal, testLabelsVal = incNumOfImages(testImagesVal, testLabelsVal, 1200)\n",
    "testLabelsVal = convertToArray(testLabelsVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[[[0, 1, 2], 0.37], [[0, 1, 3], 0.3], [[0, 1, 4], 0.3], [[0, 1, 5], 0.2], [[0, 1, 6], 0.2], [[0, 1, 7], 0.2], [[0, 1, 8], 0.2], [[0, 1, 9], 0.2], [[0, 2, 3], 0.37], [[0, 2, 4], 0.37], [[0, 2, 5], 0.27], [[0, 2, 6], 0.27], [[0, 2, 7], 0.27], [[0, 2, 8], 0.27], [[0, 2, 9], 0.27], [[0, 3, 4], 0.3], [[0, 3, 5], 0.2], [[0, 3, 6], 0.2], [[0, 3, 7], 0.2], [[0, 3, 8], 0.2], [[0, 3, 9], 0.2], [[0, 4, 5], 0.2], [[0, 4, 6], 0.2], [[0, 4, 7], 0.2], [[0, 4, 8], 0.2], [[0, 4, 9], 0.2], [[0, 5, 6], 0.1], [[0, 5, 7], 0.1], [[0, 5, 8], 0.1], [[0, 5, 9], 0.1], [[0, 6, 7], 0.1], [[0, 6, 8], 0.1], [[0, 6, 9], 0.1], [[0, 7, 8], 0.1], [[0, 7, 9], 0.1], [[0, 8, 9], 0.1], [[1, 2, 3], 0.37], [[1, 2, 4], 0.37], [[1, 2, 5], 0.27], [[1, 2, 6], 0.27], [[1, 2, 7], 0.27], [[1, 2, 8], 0.27], [[1, 2, 9], 0.27], [[1, 3, 4], 0.3], [[1, 3, 5], 0.2], [[1, 3, 6], 0.2], [[1, 3, 7], 0.2], [[1, 3, 8], 0.2], [[1, 3, 9], 0.2], [[1, 4, 5], 0.2], [[1, 4, 6], 0.2], [[1, 4, 7], 0.2], [[1, 4, 8], 0.2], [[1, 4, 9], 0.2], [[1, 5, 6], 0.1], [[1, 5, 7], 0.1], [[1, 5, 8], 0.1], [[1, 5, 9], 0.1], [[1, 6, 7], 0.1], [[1, 6, 8], 0.1], [[1, 6, 9], 0.1], [[1, 7, 8], 0.1], [[1, 7, 9], 0.1], [[1, 8, 9], 0.1], [[2, 3, 4], 0.37], [[2, 3, 5], 0.27], [[2, 3, 6], 0.27], [[2, 3, 7], 0.27], [[2, 3, 8], 0.27], [[2, 3, 9], 0.27], [[2, 4, 5], 0.27], [[2, 4, 6], 0.27], [[2, 4, 7], 0.27], [[2, 4, 8], 0.27], [[2, 4, 9], 0.27], [[2, 5, 6], 0.17], [[2, 5, 7], 0.17], [[2, 5, 8], 0.17], [[2, 5, 9], 0.17], [[2, 6, 7], 0.17], [[2, 6, 8], 0.17], [[2, 6, 9], 0.17], [[2, 7, 8], 0.17], [[2, 7, 9], 0.17], [[2, 8, 9], 0.17], [[3, 4, 5], 0.2], [[3, 4, 6], 0.2], [[3, 4, 7], 0.2], [[3, 4, 8], 0.2], [[3, 4, 9], 0.2], [[3, 5, 6], 0.1], [[3, 5, 7], 0.1], [[3, 5, 8], 0.1], [[3, 5, 9], 0.1], [[3, 6, 7], 0.1], [[3, 6, 8], 0.1], [[3, 6, 9], 0.1], [[3, 7, 8], 0.1], [[3, 7, 9], 0.1], [[3, 8, 9], 0.1], [[4, 5, 6], 0.1], [[4, 5, 7], 0.1], [[4, 5, 8], 0.1], [[4, 5, 9], 0.1], [[4, 6, 7], 0.1], [[4, 6, 8], 0.1], [[4, 6, 9], 0.1], [[4, 7, 8], 0.1], [[4, 7, 9], 0.1], [[4, 8, 9], 0.1], [[5, 6, 7], 0.0], [[5, 6, 8], 0.0], [[5, 6, 9], 0.0], [[5, 7, 8], 0.0], [[5, 7, 9], 0.0], [[5, 8, 9], 0.0], [[6, 7, 8], 0.0], [[6, 7, 9], 0.0], [[6, 8, 9], 0.0], [[7, 8, 9], 0.0]]\n",
      "[[2, 4, 5], [5, 7, 8], [2, 5, 8], [4, 5, 6], [3, 4, 9], [4, 5, 8], [1, 5, 7], [1, 2, 9], [2, 7, 9], [2, 4, 7], [0, 2, 4], [3, 4, 8], [5, 7, 9], [5, 6, 8]]\n"
     ]
    }
   ],
   "source": [
    "random.seed(1000)\n",
    "biased_ratio = 64\n",
    "num_of_letters = 10\n",
    "TestDataset = create_dataset(testImagesVal, testLabelsVal, False)\n",
    "f1scoreVal = []\n",
    "num_of_train_images = 1000\n",
    "\n",
    "trainImagesComb, trainLabelsComb = createRatioBased(biased_ratio,trainImagesVal, trainLabelsVal)\n",
    "#trainImagesComb, trainLabelsComb =createOddEvenNew(trainImagesVal, trainLabelsVal)\n",
    "trainImages, trainLabels = incNumOfImages(trainImagesComb, trainLabelsComb, num_of_train_images)\n",
    "trainLabels = convertToArray(trainLabels)\n",
    "connectedness = connectednessMeasure(trainLabels, testLabelsVal)\n",
    "combList = createCombinationListRand(connectedness, 15)\n",
    "print(connectedness)\n",
    "print(combList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0, 1, 2], [0, 1, 3], [0, 1, 4], [0, 2, 3], [0, 2, 4], [0, 3, 4], [1, 2, 3], [1, 2, 4], [1, 3, 4], [2, 3, 4]]\n[[1, 2, 3], [1, 2, 4], [1, 2, 5], [1, 3, 4], [1, 3, 5], [1, 4, 5], [2, 3, 4], [2, 3, 5], [2, 4, 5], [3, 4, 5]]\n[[2, 3, 4], [2, 3, 5], [2, 3, 6], [2, 4, 5], [2, 4, 6], [2, 5, 6], [3, 4, 5], [3, 4, 6], [3, 5, 6], [4, 5, 6]]\n[[3, 4, 5], [3, 4, 6], [3, 4, 7], [3, 5, 6], [3, 5, 7], [3, 6, 7], [4, 5, 6], [4, 5, 7], [4, 6, 7], [5, 6, 7]]\n[[4, 5, 6], [4, 5, 7], [4, 5, 8], [4, 6, 7], [4, 6, 8], [4, 7, 8], [5, 6, 7], [5, 6, 8], [5, 7, 8], [6, 7, 8]]\n[[5, 6, 7], [5, 6, 8], [5, 6, 9], [5, 7, 8], [5, 7, 9], [5, 8, 9], [6, 7, 8], [6, 7, 9], [6, 8, 9], [7, 8, 9]]\n[[[0, 1, 2], 0.6], [[0, 1, 3], 0.63], [[0, 1, 4], 0.67], [[0, 1, 5], 0.67], [[0, 1, 6], 0.63], [[0, 1, 7], 0.6], [[0, 1, 8], 0.57], [[0, 1, 9], 0.53], [[0, 2, 3], 0.67], [[0, 2, 4], 0.7], [[0, 2, 5], 0.7], [[0, 2, 6], 0.67], [[0, 2, 7], 0.63], [[0, 2, 8], 0.6], [[0, 2, 9], 0.57], [[0, 3, 4], 0.73], [[0, 3, 5], 0.73], [[0, 3, 6], 0.7], [[0, 3, 7], 0.67], [[0, 3, 8], 0.63], [[0, 3, 9], 0.6], [[0, 4, 5], 0.77], [[0, 4, 6], 0.73], [[0, 4, 7], 0.7], [[0, 4, 8], 0.67], [[0, 4, 9], 0.63], [[0, 5, 6], 0.73], [[0, 5, 7], 0.7], [[0, 5, 8], 0.67], [[0, 5, 9], 0.63], [[0, 6, 7], 0.67], [[0, 6, 8], 0.63], [[0, 6, 9], 0.6], [[0, 7, 8], 0.6], [[0, 7, 9], 0.57], [[0, 8, 9], 0.53], [[1, 2, 3], 0.7], [[1, 2, 4], 0.73], [[1, 2, 5], 0.73], [[1, 2, 6], 0.7], [[1, 2, 7], 0.67], [[1, 2, 8], 0.63], [[1, 2, 9], 0.6], [[1, 3, 4], 0.77], [[1, 3, 5], 0.77], [[1, 3, 6], 0.73], [[1, 3, 7], 0.7], [[1, 3, 8], 0.67], [[1, 3, 9], 0.63], [[1, 4, 5], 0.8], [[1, 4, 6], 0.77], [[1, 4, 7], 0.73], [[1, 4, 8], 0.7], [[1, 4, 9], 0.67], [[1, 5, 6], 0.77], [[1, 5, 7], 0.73], [[1, 5, 8], 0.7], [[1, 5, 9], 0.67], [[1, 6, 7], 0.7], [[1, 6, 8], 0.67], [[1, 6, 9], 0.63], [[1, 7, 8], 0.63], [[1, 7, 9], 0.6], [[1, 8, 9], 0.57], [[2, 3, 4], 0.8], [[2, 3, 5], 0.8], [[2, 3, 6], 0.77], [[2, 3, 7], 0.73], [[2, 3, 8], 0.7], [[2, 3, 9], 0.67], [[2, 4, 5], 0.83], [[2, 4, 6], 0.8], [[2, 4, 7], 0.77], [[2, 4, 8], 0.73], [[2, 4, 9], 0.7], [[2, 5, 6], 0.8], [[2, 5, 7], 0.77], [[2, 5, 8], 0.73], [[2, 5, 9], 0.7], [[2, 6, 7], 0.73], [[2, 6, 8], 0.7], [[2, 6, 9], 0.67], [[2, 7, 8], 0.67], [[2, 7, 9], 0.63], [[2, 8, 9], 0.6], [[3, 4, 5], 0.87], [[3, 4, 6], 0.83], [[3, 4, 7], 0.8], [[3, 4, 8], 0.77], [[3, 4, 9], 0.73], [[3, 5, 6], 0.83], [[3, 5, 7], 0.8], [[3, 5, 8], 0.77], [[3, 5, 9], 0.73], [[3, 6, 7], 0.77], [[3, 6, 8], 0.73], [[3, 6, 9], 0.7], [[3, 7, 8], 0.7], [[3, 7, 9], 0.67], [[3, 8, 9], 0.63], [[4, 5, 6], 0.87], [[4, 5, 7], 0.83], [[4, 5, 8], 0.8], [[4, 5, 9], 0.77], [[4, 6, 7], 0.8], [[4, 6, 8], 0.77], [[4, 6, 9], 0.73], [[4, 7, 8], 0.73], [[4, 7, 9], 0.7], [[4, 8, 9], 0.67], [[5, 6, 7], 0.8], [[5, 6, 8], 0.77], [[5, 6, 9], 0.73], [[5, 7, 8], 0.73], [[5, 7, 9], 0.7], [[5, 8, 9], 0.67], [[6, 7, 8], 0.7], [[6, 7, 9], 0.67], [[6, 8, 9], 0.63], [[7, 8, 9], 0.6]]\n"
     ]
    }
   ],
   "source": [
    "random.seed(1000)\n",
    "biased_ratio = 8\n",
    "num_of_letters = 10\n",
    "TestDataset = create_dataset(testImagesVal, testLabelsVal, False)\n",
    "f1scoreVal = []\n",
    "num_of_train_images = 1000\n",
    "\n",
    "#trainImages, trainLabels = createRatioBased(biased_ratio,trainImagesVal, trainLabelsVal)\n",
    "trainImages, trainLabels = createRollingWindow(5, 3,trainImagesVal, trainLabelsVal)\n",
    "#trainImagesComb, trainLabelsComb =createOddEvenNew(trainImagesVal, trainLabelsVal)\n",
    "#= incNumOfImages(trainImagesComb, trainLabelsComb, num_of_train_images)\n",
    "trainLabels = convertToArray(trainLabels)\n",
    "connectedness = connectednessMeasure(trainLabels, testLabelsVal)\n",
    "print(connectedness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "JEnIdJP_HTfG",
    "outputId": "ef8e8298-808b-4a8c-9a09-c008e3486910",
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "Epoch 1/400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-34d69cade645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinaryCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTestDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTestDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 725\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    726\u001b[0m             *args, **kwds))\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3196\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    964\u001b[0m           \u001b[0;31m# TODO(mdan): Push this block higher in tf.function's call stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             return autograph.converted_call(\n\u001b[0m\u001b[1;32m    967\u001b[0m                 \u001b[0moriginal_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    477\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m       outputs = reduce_per_replica(\n\u001b[1;32m    797\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1257\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1258\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1259\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2728\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2729\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2732\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3415\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3416\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3417\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3419\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    755\u001b[0m       loss = self.compiled_loss(\n\u001b[1;32m    756\u001b[0m           y, y_pred, sample_weight, regularization_losses=self.losses)\n\u001b[0;32m--> 757\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \"\"\"\n\u001b[0;32m--> 496\u001b[0;31m     grads_and_vars = self._compute_gradients(\n\u001b[0m\u001b[1;32m    497\u001b[0m         loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/gradients\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m       \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     self._assert_valid_dtypes([\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_get_gradients\u001b[0;34m(self, tape, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    439\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0;34m\"\"\"Called in `minimize` to compute gradients from loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1078\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1081\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_Log1pGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    659\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreciprocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mr_binary_op_wrapper\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m   1192\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mr_binary_op_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[0;32m--> 264\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    265\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m   \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtensor_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdtype_value\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m   const_tensor = g._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    287\u001b[0m       \"Const\", [], [dtype_value.type], attrs=attrs, name=name).outputs[0]\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    588\u001b[0m       \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    591\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         compute_device)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3526\u001b[0m     \u001b[0;31m# Session.run call cannot occur between creating and mutating the op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3528\u001b[0;31m       ret = Operation(\n\u001b[0m\u001b[1;32m   3529\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3530\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1988\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1989\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1990\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[0;31m# List of _UserDevSpecs holding code location of device context manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/tf_stack.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(limit)\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;31m# traversing the stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m   \u001b[0mthread_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_thread_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m   return _tf_stack.extract_stack(\n\u001b[0m\u001b[1;32m    151\u001b[0m       \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0m_source_mapper_stacks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthread_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random.seed(1000)\n",
    "biased_ratio = 1\n",
    "num_of_letters = 10\n",
    "array = [500,1000,2000,2500,3000,5000,10000,20000]\n",
    "TestDataset = create_dataset(testImagesVal, testLabelsVal, False)\n",
    "f1scoreVal = []\n",
    "precVal = []\n",
    "recaVal = []\n",
    "for i in range(len(array)):\n",
    "    num_of_train_images = array[i]\n",
    "    \n",
    "    trainImagesComb, trainLabelsComb = createRatioBased(biased_ratio, trainImagesVal, trainLabelsVal)\n",
    "    trainImages, trainLabels = incNumOfImages(trainImagesComb, trainLabelsComb, num_of_train_images)\n",
    "    trainLabels = convertToArray(trainLabels)\n",
    "    TrainDataset = create_dataset(trainImages, trainLabels)\n",
    "\n",
    "\n",
    "    model = modelLatest()\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, min_delta=0.0001, restore_best_weights=True)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    history = model.fit(TrainDataset, epochs=400, validation_data=TestDataset, callbacks=[callback], batch_size = batch_size)\n",
    "\n",
    "    ypred = model.predict(TestDataset)\n",
    "    #acc , combList, correctness = accuracyFunc(ypred, testLabelsVal, array)\n",
    "\n",
    "    temp_f1 = 0\n",
    "    temp_recall = 0\n",
    "    temp_precision = 0\n",
    "    temp_acc = 0\n",
    "    f1Val = []\n",
    "    distVal = []\n",
    "    y_test = testLabelsVal\n",
    "    y_predicted = calculate_labels(ypred)\n",
    "    for i in range(len(y_test)):\n",
    "        temp_f1 = temp_f1 + f1_score(y_test[i], y_predicted[i])\n",
    "        temp_recall = temp_recall + recall_score(y_test[i], y_predicted[i])\n",
    "        temp_precision = temp_precision + precision_score(y_test[i], y_predicted[i])\n",
    "        temp_acc = temp_acc + accuracy_score(y_test[i], y_predicted[i])\n",
    "        f1Val.append([f1_score(y_test[i], y_predicted[i])])\n",
    "        distVal.append([f1_score(y_test[i], y_predicted[i])])\n",
    "    f1score = temp_f1 / len(y_test)\n",
    "    precision = temp_precision / len(y_test)\n",
    "    recall = temp_recall / len(y_test)\n",
    "    acc = temp_acc / len(y_test)\n",
    "    print(f1score, precision, recall, acc)\n",
    "    f1scoreVal.append(f1score)\n",
    "    precVal.append(precision)\n",
    "    recaVal.append(recall)\n",
    "\n",
    "#arrayVal = correlationmt(trainLabelsComb)\n",
    "#array = calculateDist(arrayVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1.0, 0.8324206349206347, 0.8645436507936505, 0.8790476190476187, 0.8645634920634914, 0.9057936507936508, 0.9539285714285715, 0.9623809523809526]\n",
      "[1.0, 0.9006944444444447, 0.931388888888889, 0.9375000000000002, 0.9173611111111112, 0.9326388888888891, 0.9770833333333334, 0.96875]\n",
      "[1.0, 0.7972222222222224, 0.8250000000000005, 0.8444444444444447, 0.8361111111111118, 0.8944444444444445, 0.9416666666666668, 0.9611111111111111]\n"
     ]
    }
   ],
   "source": [
    "print(\"1\")\n",
    "print(f1scoreVal)\n",
    "print(precVal)\n",
    "print(recaVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "8/8 [==============================] - 5s 264ms/step - loss: 0.6950 - accuracy: 0.2330 - val_loss: 0.5753 - val_accuracy: 0.4083\n",
      "Epoch 2/400\n",
      "8/8 [==============================] - 1s 178ms/step - loss: 0.4398 - accuracy: 0.4320 - val_loss: 0.6097 - val_accuracy: 0.4750\n",
      "Epoch 3/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 0.2970 - accuracy: 0.5075 - val_loss: 0.6075 - val_accuracy: 0.3833\n",
      "Epoch 4/400\n",
      "8/8 [==============================] - 1s 177ms/step - loss: 0.2030 - accuracy: 0.4535 - val_loss: 0.6061 - val_accuracy: 0.3667\n",
      "Epoch 5/400\n",
      "8/8 [==============================] - 1s 176ms/step - loss: 0.1185 - accuracy: 0.5048 - val_loss: 0.6117 - val_accuracy: 0.4083\n",
      "Epoch 6/400\n",
      "8/8 [==============================] - 1s 176ms/step - loss: 0.0774 - accuracy: 0.5179 - val_loss: 0.6117 - val_accuracy: 0.3667\n",
      "Epoch 7/400\n",
      "8/8 [==============================] - 1s 177ms/step - loss: 0.0495 - accuracy: 0.4418 - val_loss: 0.6396 - val_accuracy: 0.4417\n",
      "Epoch 8/400\n",
      "8/8 [==============================] - 1s 178ms/step - loss: 0.0322 - accuracy: 0.4940 - val_loss: 0.6134 - val_accuracy: 0.3750\n",
      "Epoch 9/400\n",
      "8/8 [==============================] - 1s 178ms/step - loss: 0.0190 - accuracy: 0.4523 - val_loss: 0.6521 - val_accuracy: 0.4417\n",
      "Epoch 10/400\n",
      "8/8 [==============================] - 1s 177ms/step - loss: 0.0130 - accuracy: 0.4995 - val_loss: 0.6506 - val_accuracy: 0.4667\n",
      "Epoch 11/400\n",
      "8/8 [==============================] - 1s 177ms/step - loss: 0.0104 - accuracy: 0.4925 - val_loss: 0.6464 - val_accuracy: 0.4167\n",
      "Epoch 12/400\n",
      "8/8 [==============================] - 1s 175ms/step - loss: 0.0057 - accuracy: 0.3816 - val_loss: 0.6725 - val_accuracy: 0.4167\n",
      "Epoch 13/400\n",
      "8/8 [==============================] - 1s 176ms/step - loss: 0.0039 - accuracy: 0.3716 - val_loss: 0.6788 - val_accuracy: 0.4333\n",
      "Epoch 14/400\n",
      "8/8 [==============================] - 1s 176ms/step - loss: 0.0029 - accuracy: 0.4229 - val_loss: 0.6941 - val_accuracy: 0.4000\n",
      "Epoch 15/400\n",
      "8/8 [==============================] - 1s 180ms/step - loss: 0.0023 - accuracy: 0.3128 - val_loss: 0.7022 - val_accuracy: 0.4250\n",
      "Epoch 16/400\n",
      "8/8 [==============================] - 1s 176ms/step - loss: 0.0020 - accuracy: 0.4116 - val_loss: 0.7060 - val_accuracy: 0.4250\n",
      "Epoch 17/400\n",
      "8/8 [==============================] - 1s 177ms/step - loss: 0.0016 - accuracy: 0.4129 - val_loss: 0.7187 - val_accuracy: 0.4000\n",
      "Epoch 18/400\n",
      "8/8 [==============================] - 1s 176ms/step - loss: 0.0013 - accuracy: 0.3853 - val_loss: 0.7301 - val_accuracy: 0.4250\n",
      "Epoch 19/400\n",
      "8/8 [==============================] - 1s 177ms/step - loss: 0.0013 - accuracy: 0.4219 - val_loss: 0.7420 - val_accuracy: 0.4167\n",
      "Epoch 20/400\n",
      "8/8 [==============================] - 1s 176ms/step - loss: 0.0013 - accuracy: 0.3270 - val_loss: 0.7422 - val_accuracy: 0.3833\n",
      "Epoch 21/400\n",
      "8/8 [==============================] - 1s 176ms/step - loss: 0.0011 - accuracy: 0.4111 - val_loss: 0.7452 - val_accuracy: 0.4333\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f858518fe50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f858518fe50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "/home/sidharth/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1525 0.3 0.1027777777777778 0.7266666666666677 0.18407960199004975\n",
      "Epoch 1/400\n",
      "16/16 [==============================] - 7s 207ms/step - loss: 0.6265 - accuracy: 0.3278 - val_loss: 0.6339 - val_accuracy: 0.3583\n",
      "Epoch 2/400\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.2426 - accuracy: 0.4321 - val_loss: 0.6147 - val_accuracy: 0.3167\n",
      "Epoch 3/400\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.1086 - accuracy: 0.4724 - val_loss: 0.6491 - val_accuracy: 0.3583\n",
      "Epoch 4/400\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0405 - accuracy: 0.4658 - val_loss: 0.6955 - val_accuracy: 0.3500\n",
      "Epoch 5/400\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0221 - accuracy: 0.4272 - val_loss: 0.7104 - val_accuracy: 0.3750\n",
      "Epoch 6/400\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0144 - accuracy: 0.4714 - val_loss: 0.6780 - val_accuracy: 0.3833\n",
      "Epoch 7/400\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0072 - accuracy: 0.4883 - val_loss: 0.7351 - val_accuracy: 0.3500\n",
      "Epoch 8/400\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0054 - accuracy: 0.3973 - val_loss: 0.7928 - val_accuracy: 0.3333\n",
      "Epoch 9/400\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.0034 - accuracy: 0.4408 - val_loss: 0.7714 - val_accuracy: 0.3667\n",
      "Epoch 10/400\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0018 - accuracy: 0.4320 - val_loss: 0.8114 - val_accuracy: 0.2917\n",
      "Epoch 11/400\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 0.0013 - accuracy: 0.3606 - val_loss: 0.8290 - val_accuracy: 0.3583\n",
      "Epoch 12/400\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 9.8066e-04 - accuracy: 0.4662 - val_loss: 0.8397 - val_accuracy: 0.3750\n",
      "Epoch 13/400\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 8.1075e-04 - accuracy: 0.3812 - val_loss: 0.8672 - val_accuracy: 0.3583\n",
      "Epoch 14/400\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 8.0238e-04 - accuracy: 0.4258 - val_loss: 0.8683 - val_accuracy: 0.3917\n",
      "Epoch 15/400\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 7.1308e-04 - accuracy: 0.3995 - val_loss: 0.8899 - val_accuracy: 0.4000\n",
      "Epoch 16/400\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 4.7672e-04 - accuracy: 0.4619 - val_loss: 0.8903 - val_accuracy: 0.4167\n",
      "Epoch 17/400\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 4.5160e-04 - accuracy: 0.5038 - val_loss: 0.9190 - val_accuracy: 0.3667\n",
      "Epoch 18/400\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 3.3597e-04 - accuracy: 0.3622 - val_loss: 0.9038 - val_accuracy: 0.4083\n",
      "Epoch 19/400\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 3.0457e-04 - accuracy: 0.4270 - val_loss: 0.9340 - val_accuracy: 0.4083\n",
      "Epoch 20/400\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 2.5393e-04 - accuracy: 0.4556 - val_loss: 0.9499 - val_accuracy: 0.3833\n",
      "Epoch 21/400\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 2.5204e-04 - accuracy: 0.4484 - val_loss: 0.9534 - val_accuracy: 0.4250\n",
      "Epoch 22/400\n",
      "16/16 [==============================] - 3s 169ms/step - loss: 2.5174e-04 - accuracy: 0.4313 - val_loss: 0.9665 - val_accuracy: 0.3750\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f85852088b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f85852088b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.424722222222222 0.5972222222222222 0.35833333333333345 0.7583333333333332 0.4708029197080292\n",
      "Epoch 1/400\n",
      "32/32 [==============================] - 9s 184ms/step - loss: 0.4888 - accuracy: 0.3946 - val_loss: 0.6583 - val_accuracy: 0.4083\n",
      "Epoch 2/400\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 0.0739 - accuracy: 0.5628 - val_loss: 0.6941 - val_accuracy: 0.4583\n",
      "Epoch 3/400\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 0.0151 - accuracy: 0.4574 - val_loss: 0.7681 - val_accuracy: 0.4333\n",
      "Epoch 4/400\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 0.0075 - accuracy: 0.4560 - val_loss: 0.7532 - val_accuracy: 0.4750\n",
      "Epoch 5/400\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 0.0031 - accuracy: 0.4824 - val_loss: 0.8095 - val_accuracy: 0.3917\n",
      "Epoch 6/400\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 0.0011 - accuracy: 0.4225 - val_loss: 0.8623 - val_accuracy: 0.4500\n",
      "Epoch 7/400\n",
      "32/32 [==============================] - 5s 167ms/step - loss: 6.7968e-04 - accuracy: 0.4359 - val_loss: 0.8603 - val_accuracy: 0.4167\n",
      "Epoch 8/400\n",
      "32/32 [==============================] - 5s 167ms/step - loss: 6.5765e-04 - accuracy: 0.4247 - val_loss: 0.9384 - val_accuracy: 0.4417\n",
      "Epoch 9/400\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 0.0031 - accuracy: 0.4666 - val_loss: 0.9511 - val_accuracy: 0.4250\n",
      "Epoch 10/400\n",
      "32/32 [==============================] - 5s 167ms/step - loss: 0.0046 - accuracy: 0.4657 - val_loss: 0.8304 - val_accuracy: 0.4667\n",
      "Epoch 11/400\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 0.0013 - accuracy: 0.4302 - val_loss: 0.9151 - val_accuracy: 0.4583\n",
      "Epoch 12/400\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 4.1865e-04 - accuracy: 0.4784 - val_loss: 0.9569 - val_accuracy: 0.4667\n",
      "Epoch 13/400\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 1.4655e-04 - accuracy: 0.5436 - val_loss: 0.9444 - val_accuracy: 0.4667\n",
      "Epoch 14/400\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 8.5332e-05 - accuracy: 0.4277 - val_loss: 0.9592 - val_accuracy: 0.4500\n",
      "Epoch 15/400\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 6.9879e-05 - accuracy: 0.4405 - val_loss: 0.9643 - val_accuracy: 0.4667\n",
      "Epoch 16/400\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 5.3979e-05 - accuracy: 0.4661 - val_loss: 0.9838 - val_accuracy: 0.4417\n",
      "Epoch 17/400\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 5.0428e-05 - accuracy: 0.4807 - val_loss: 0.9837 - val_accuracy: 0.4417\n",
      "Epoch 18/400\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 4.5418e-05 - accuracy: 0.4638 - val_loss: 0.9900 - val_accuracy: 0.4667\n",
      "Epoch 19/400\n",
      "32/32 [==============================] - 5s 167ms/step - loss: 1.2149e-04 - accuracy: 0.4526 - val_loss: 1.0416 - val_accuracy: 0.4083\n",
      "Epoch 20/400\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 5.8163e-05 - accuracy: 0.5095 - val_loss: 1.0030 - val_accuracy: 0.4500\n",
      "Epoch 21/400\n",
      "32/32 [==============================] - 5s 167ms/step - loss: 4.4218e-05 - accuracy: 0.4884 - val_loss: 1.0183 - val_accuracy: 0.4250\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8585b43ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8585b43ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44912698412698376 0.6055555555555556 0.3888888888888889 0.7508333333333336 0.4835924006908463\n",
      "Epoch 1/400\n",
      "40/40 [==============================] - 11s 183ms/step - loss: 0.4397 - accuracy: 0.4314 - val_loss: 0.6358 - val_accuracy: 0.5500\n",
      "Epoch 2/400\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 0.0421 - accuracy: 0.4887 - val_loss: 0.6811 - val_accuracy: 0.5000\n",
      "Epoch 3/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 0.0115 - accuracy: 0.4954 - val_loss: 0.6829 - val_accuracy: 0.5167\n",
      "Epoch 4/400\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 0.0039 - accuracy: 0.4329 - val_loss: 0.7214 - val_accuracy: 0.5000\n",
      "Epoch 5/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 0.0020 - accuracy: 0.5104 - val_loss: 0.8002 - val_accuracy: 0.4583\n",
      "Epoch 6/400\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 0.0061 - accuracy: 0.4269 - val_loss: 0.7506 - val_accuracy: 0.4917\n",
      "Epoch 7/400\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 0.0012 - accuracy: 0.5115 - val_loss: 0.7793 - val_accuracy: 0.5250\n",
      "Epoch 8/400\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 6.6838e-04 - accuracy: 0.4976 - val_loss: 0.7722 - val_accuracy: 0.4917\n",
      "Epoch 9/400\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 2.2901e-04 - accuracy: 0.5158 - val_loss: 0.7743 - val_accuracy: 0.4917\n",
      "Epoch 10/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 1.7116e-04 - accuracy: 0.4657 - val_loss: 0.7900 - val_accuracy: 0.4833\n",
      "Epoch 11/400\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 1.2010e-04 - accuracy: 0.5042 - val_loss: 0.8063 - val_accuracy: 0.4583\n",
      "Epoch 12/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 1.1979e-04 - accuracy: 0.4728 - val_loss: 0.8320 - val_accuracy: 0.4833\n",
      "Epoch 13/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 1.0195e-04 - accuracy: 0.4290 - val_loss: 0.8243 - val_accuracy: 0.4833\n",
      "Epoch 14/400\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 7.9454e-05 - accuracy: 0.4966 - val_loss: 0.8319 - val_accuracy: 0.5083\n",
      "Epoch 15/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 6.8675e-05 - accuracy: 0.4839 - val_loss: 0.8442 - val_accuracy: 0.4750\n",
      "Epoch 16/400\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 7.5557e-05 - accuracy: 0.4898 - val_loss: 0.8462 - val_accuracy: 0.4667\n",
      "Epoch 17/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 5.6314e-05 - accuracy: 0.4294 - val_loss: 0.8556 - val_accuracy: 0.4750\n",
      "Epoch 18/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 5.5191e-05 - accuracy: 0.4210 - val_loss: 0.8634 - val_accuracy: 0.4833\n",
      "Epoch 19/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 4.7463e-05 - accuracy: 0.4901 - val_loss: 0.8673 - val_accuracy: 0.4667\n",
      "Epoch 20/400\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 4.5095e-05 - accuracy: 0.4428 - val_loss: 0.8721 - val_accuracy: 0.5167\n",
      "Epoch 21/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 4.3170e-05 - accuracy: 0.4367 - val_loss: 0.8786 - val_accuracy: 0.4917\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f843169c550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f843169c550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4908531746031741 0.6938888888888889 0.40833333333333355 0.7766666666666663 0.5231316725978647\n",
      "Epoch 1/400\n",
      "47/47 [==============================] - 13s 182ms/step - loss: 0.3961 - accuracy: 0.4426 - val_loss: 0.6724 - val_accuracy: 0.3667\n",
      "Epoch 2/400\n",
      "47/47 [==============================] - 8s 168ms/step - loss: 0.0238 - accuracy: 0.5018 - val_loss: 0.7324 - val_accuracy: 0.4000\n",
      "Epoch 3/400\n",
      "47/47 [==============================] - 8s 170ms/step - loss: 0.0074 - accuracy: 0.5371 - val_loss: 0.7424 - val_accuracy: 0.4083\n",
      "Epoch 4/400\n",
      "47/47 [==============================] - 8s 168ms/step - loss: 0.0033 - accuracy: 0.4618 - val_loss: 0.8070 - val_accuracy: 0.4417\n",
      "Epoch 5/400\n",
      "47/47 [==============================] - 8s 169ms/step - loss: 0.0016 - accuracy: 0.5226 - val_loss: 0.8798 - val_accuracy: 0.4000\n",
      "Epoch 6/400\n",
      "47/47 [==============================] - 8s 169ms/step - loss: 8.6784e-04 - accuracy: 0.4013 - val_loss: 0.8435 - val_accuracy: 0.3833\n",
      "Epoch 7/400\n",
      "47/47 [==============================] - 8s 174ms/step - loss: 3.3525e-04 - accuracy: 0.4817 - val_loss: 0.8571 - val_accuracy: 0.4167\n",
      "Epoch 8/400\n",
      "47/47 [==============================] - 8s 173ms/step - loss: 2.4659e-04 - accuracy: 0.4428 - val_loss: 0.9205 - val_accuracy: 0.3500\n",
      "Epoch 9/400\n",
      "47/47 [==============================] - 8s 170ms/step - loss: 3.0678e-04 - accuracy: 0.3897 - val_loss: 0.8864 - val_accuracy: 0.3917\n",
      "Epoch 10/400\n",
      "47/47 [==============================] - 8s 170ms/step - loss: 1.2975e-04 - accuracy: 0.4447 - val_loss: 0.9057 - val_accuracy: 0.4167\n",
      "Epoch 11/400\n",
      "47/47 [==============================] - 8s 174ms/step - loss: 1.0952e-04 - accuracy: 0.4716 - val_loss: 0.9233 - val_accuracy: 0.4167\n",
      "Epoch 12/400\n",
      "47/47 [==============================] - 8s 175ms/step - loss: 7.3217e-05 - accuracy: 0.4390 - val_loss: 0.9834 - val_accuracy: 0.4000\n",
      "Epoch 13/400\n",
      "47/47 [==============================] - 8s 177ms/step - loss: 1.7991e-04 - accuracy: 0.4311 - val_loss: 0.9579 - val_accuracy: 0.3500\n",
      "Epoch 14/400\n",
      "47/47 [==============================] - 8s 174ms/step - loss: 1.1522e-04 - accuracy: 0.3657 - val_loss: 0.9587 - val_accuracy: 0.3833\n",
      "Epoch 15/400\n",
      "47/47 [==============================] - 8s 175ms/step - loss: 4.5458e-05 - accuracy: 0.4024 - val_loss: 0.9660 - val_accuracy: 0.4083\n",
      "Epoch 16/400\n",
      "47/47 [==============================] - 8s 172ms/step - loss: 4.4087e-05 - accuracy: 0.3775 - val_loss: 0.9775 - val_accuracy: 0.4000\n",
      "Epoch 17/400\n",
      "47/47 [==============================] - 8s 177ms/step - loss: 3.7026e-05 - accuracy: 0.4299 - val_loss: 1.0018 - val_accuracy: 0.3917\n",
      "Epoch 18/400\n",
      "47/47 [==============================] - 8s 179ms/step - loss: 3.3531e-05 - accuracy: 0.4006 - val_loss: 0.9991 - val_accuracy: 0.4167\n",
      "Epoch 19/400\n",
      "47/47 [==============================] - 8s 177ms/step - loss: 3.6021e-05 - accuracy: 0.4281 - val_loss: 1.0048 - val_accuracy: 0.4000\n",
      "Epoch 20/400\n",
      "47/47 [==============================] - 8s 174ms/step - loss: 3.4953e-05 - accuracy: 0.3964 - val_loss: 1.0083 - val_accuracy: 0.4000\n",
      "Epoch 21/400\n",
      "47/47 [==============================] - 8s 173ms/step - loss: 2.7470e-05 - accuracy: 0.4260 - val_loss: 1.0082 - val_accuracy: 0.4000\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8585c34dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8585c34dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5071825396825392 0.7123611111111113 0.43055555555555564 0.7791666666666666 0.5391304347826087\n",
      "Epoch 1/400\n",
      "79/79 [==============================] - 17s 175ms/step - loss: 0.3099 - accuracy: 0.4477 - val_loss: 0.7577 - val_accuracy: 0.4167\n",
      "Epoch 2/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 0.0107 - accuracy: 0.4855 - val_loss: 0.8229 - val_accuracy: 0.3917\n",
      "Epoch 3/400\n",
      "79/79 [==============================] - 13s 170ms/step - loss: 0.0034 - accuracy: 0.4197 - val_loss: 0.8562 - val_accuracy: 0.3250\n",
      "Epoch 4/400\n",
      "79/79 [==============================] - 13s 169ms/step - loss: 0.0026 - accuracy: 0.4085 - val_loss: 0.8748 - val_accuracy: 0.3750\n",
      "Epoch 5/400\n",
      "79/79 [==============================] - 14s 174ms/step - loss: 3.2388e-04 - accuracy: 0.3888 - val_loss: 0.9440 - val_accuracy: 0.3500\n",
      "Epoch 6/400\n",
      "79/79 [==============================] - 13s 170ms/step - loss: 2.1062e-04 - accuracy: 0.3343 - val_loss: 0.9483 - val_accuracy: 0.3917\n",
      "Epoch 7/400\n",
      "79/79 [==============================] - 13s 167ms/step - loss: 1.1536e-04 - accuracy: 0.4018 - val_loss: 0.9739 - val_accuracy: 0.3667\n",
      "Epoch 8/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 8.7123e-05 - accuracy: 0.3288 - val_loss: 0.9928 - val_accuracy: 0.3833\n",
      "Epoch 9/400\n",
      "79/79 [==============================] - 13s 167ms/step - loss: 1.1449e-04 - accuracy: 0.3778 - val_loss: 0.9842 - val_accuracy: 0.3667\n",
      "Epoch 10/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 6.5432e-05 - accuracy: 0.4451 - val_loss: 1.0161 - val_accuracy: 0.3833\n",
      "Epoch 11/400\n",
      "79/79 [==============================] - 13s 169ms/step - loss: 3.7461e-05 - accuracy: 0.3766 - val_loss: 1.0307 - val_accuracy: 0.3917\n",
      "Epoch 12/400\n",
      "79/79 [==============================] - 13s 167ms/step - loss: 3.7415e-05 - accuracy: 0.4073 - val_loss: 1.0483 - val_accuracy: 0.3917\n",
      "Epoch 13/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 2.5161e-05 - accuracy: 0.3560 - val_loss: 1.0629 - val_accuracy: 0.3833\n",
      "Epoch 14/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 2.4132e-05 - accuracy: 0.3843 - val_loss: 1.0793 - val_accuracy: 0.3750\n",
      "Epoch 15/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 2.0875e-05 - accuracy: 0.3888 - val_loss: 1.0962 - val_accuracy: 0.3833\n",
      "Epoch 16/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 2.1488e-05 - accuracy: 0.3837 - val_loss: 1.1101 - val_accuracy: 0.3750\n",
      "Epoch 17/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 1.6536e-05 - accuracy: 0.3869 - val_loss: 1.1269 - val_accuracy: 0.3750\n",
      "Epoch 18/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 1.3853e-05 - accuracy: 0.4205 - val_loss: 1.1387 - val_accuracy: 0.3750\n",
      "Epoch 19/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 2.1024e-05 - accuracy: 0.3873 - val_loss: 1.1477 - val_accuracy: 0.3833\n",
      "Epoch 20/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 1.0496e-05 - accuracy: 0.3779 - val_loss: 1.1633 - val_accuracy: 0.3750\n",
      "Epoch 21/400\n",
      "79/79 [==============================] - 14s 172ms/step - loss: 1.0546e-05 - accuracy: 0.3839 - val_loss: 1.1742 - val_accuracy: 0.3750\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8433973e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8433973e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5682936507936504 0.7130555555555559 0.5055555555555554 0.7849999999999998 0.5852090032154341\n",
      "Epoch 1/400\n",
      "157/157 [==============================] - 32s 173ms/step - loss: 0.1975 - accuracy: 0.4637 - val_loss: 0.6844 - val_accuracy: 0.3750\n",
      "Epoch 2/400\n",
      "157/157 [==============================] - 27s 169ms/step - loss: 0.0070 - accuracy: 0.4342 - val_loss: 0.7566 - val_accuracy: 0.3917\n",
      "Epoch 3/400\n",
      "157/157 [==============================] - 27s 170ms/step - loss: 0.0019 - accuracy: 0.3526 - val_loss: 0.7882 - val_accuracy: 0.3500\n",
      "Epoch 4/400\n",
      "157/157 [==============================] - 26s 168ms/step - loss: 0.0015 - accuracy: 0.3380 - val_loss: 0.9397 - val_accuracy: 0.3417\n",
      "Epoch 5/400\n",
      "157/157 [==============================] - 27s 170ms/step - loss: 4.5595e-04 - accuracy: 0.3008 - val_loss: 0.9371 - val_accuracy: 0.4167\n",
      "Epoch 6/400\n",
      "157/157 [==============================] - 27s 169ms/step - loss: 5.6686e-05 - accuracy: 0.3283 - val_loss: 0.9319 - val_accuracy: 0.3500\n",
      "Epoch 7/400\n",
      "157/157 [==============================] - 26s 168ms/step - loss: 6.4802e-04 - accuracy: 0.3129 - val_loss: 0.9206 - val_accuracy: 0.4083\n",
      "Epoch 8/400\n",
      "157/157 [==============================] - 27s 169ms/step - loss: 3.5413e-05 - accuracy: 0.3177 - val_loss: 0.9449 - val_accuracy: 0.3917\n",
      "Epoch 9/400\n",
      "157/157 [==============================] - 27s 169ms/step - loss: 0.0267 - accuracy: 0.4067 - val_loss: 0.7943 - val_accuracy: 0.4750\n",
      "Epoch 10/400\n",
      "157/157 [==============================] - 26s 168ms/step - loss: 0.0013 - accuracy: 0.3477 - val_loss: 0.8331 - val_accuracy: 0.4167\n",
      "Epoch 11/400\n",
      "157/157 [==============================] - 27s 171ms/step - loss: 1.9768e-04 - accuracy: 0.3742 - val_loss: 0.8275 - val_accuracy: 0.4333\n",
      "Epoch 12/400\n",
      "157/157 [==============================] - 27s 170ms/step - loss: 1.6628e-04 - accuracy: 0.3552 - val_loss: 0.9177 - val_accuracy: 0.4167\n",
      "Epoch 13/400\n",
      "157/157 [==============================] - 27s 172ms/step - loss: 8.8361e-05 - accuracy: 0.3756 - val_loss: 0.8842 - val_accuracy: 0.4500\n",
      "Epoch 14/400\n",
      "157/157 [==============================] - 27s 174ms/step - loss: 1.4811e-05 - accuracy: 0.3998 - val_loss: 0.8932 - val_accuracy: 0.4833\n",
      "Epoch 15/400\n",
      "157/157 [==============================] - 27s 172ms/step - loss: 1.1797e-05 - accuracy: 0.3777 - val_loss: 0.9055 - val_accuracy: 0.4750\n",
      "Epoch 16/400\n",
      "157/157 [==============================] - 27s 172ms/step - loss: 8.8258e-06 - accuracy: 0.3861 - val_loss: 0.9144 - val_accuracy: 0.4750\n",
      "Epoch 17/400\n",
      "157/157 [==============================] - 27s 173ms/step - loss: 7.4302e-06 - accuracy: 0.3796 - val_loss: 0.9253 - val_accuracy: 0.4750\n",
      "Epoch 18/400\n",
      "157/157 [==============================] - 27s 171ms/step - loss: 5.8773e-06 - accuracy: 0.3758 - val_loss: 0.9362 - val_accuracy: 0.4833\n",
      "Epoch 19/400\n",
      "157/157 [==============================] - 27s 173ms/step - loss: 6.6191e-06 - accuracy: 0.3864 - val_loss: 0.9454 - val_accuracy: 0.4833\n",
      "Epoch 20/400\n",
      "157/157 [==============================] - 27s 171ms/step - loss: 4.8201e-06 - accuracy: 0.3974 - val_loss: 0.9545 - val_accuracy: 0.4833\n",
      "Epoch 21/400\n",
      "157/157 [==============================] - 27s 171ms/step - loss: 4.5302e-06 - accuracy: 0.3828 - val_loss: 0.9652 - val_accuracy: 0.4917\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8585616940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8585616940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6149206349206345 0.7673611111111113 0.5444444444444445 0.8066666666666668 0.6282051282051281\n",
      "Epoch 1/400\n",
      "313/313 [==============================] - 59s 175ms/step - loss: 0.1316 - accuracy: 0.4695 - val_loss: 0.7200 - val_accuracy: 0.4917\n",
      "Epoch 2/400\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 0.0013 - accuracy: 0.4115 - val_loss: 0.7497 - val_accuracy: 0.4500\n",
      "Epoch 3/400\n",
      "313/313 [==============================] - 53s 171ms/step - loss: 0.0173 - accuracy: 0.3826 - val_loss: 0.8661 - val_accuracy: 0.5083\n",
      "Epoch 4/400\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 0.0012 - accuracy: 0.4002 - val_loss: 0.8543 - val_accuracy: 0.4500\n",
      "Epoch 5/400\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 2.0900e-04 - accuracy: 0.3462 - val_loss: 0.8776 - val_accuracy: 0.4750\n",
      "Epoch 6/400\n",
      "313/313 [==============================] - 53s 169ms/step - loss: 5.6972e-05 - accuracy: 0.3468 - val_loss: 0.9464 - val_accuracy: 0.4583\n",
      "Epoch 7/400\n",
      "313/313 [==============================] - 53s 169ms/step - loss: 2.9733e-05 - accuracy: 0.3498 - val_loss: 0.9537 - val_accuracy: 0.4583\n",
      "Epoch 8/400\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 1.6857e-05 - accuracy: 0.3510 - val_loss: 0.9746 - val_accuracy: 0.4833\n",
      "Epoch 9/400\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 1.1020e-05 - accuracy: 0.3469 - val_loss: 0.9906 - val_accuracy: 0.4667\n",
      "Epoch 10/400\n",
      "313/313 [==============================] - 53s 169ms/step - loss: 7.8892e-06 - accuracy: 0.3554 - val_loss: 1.0180 - val_accuracy: 0.4667\n",
      "Epoch 11/400\n",
      "313/313 [==============================] - 53s 169ms/step - loss: 1.9606e-05 - accuracy: 0.3625 - val_loss: 1.0408 - val_accuracy: 0.4583\n",
      "Epoch 12/400\n",
      "313/313 [==============================] - 53s 169ms/step - loss: 6.6689e-06 - accuracy: 0.3636 - val_loss: 1.0595 - val_accuracy: 0.4833\n",
      "Epoch 13/400\n",
      "313/313 [==============================] - 53s 169ms/step - loss: 4.9838e-06 - accuracy: 0.3595 - val_loss: 1.0608 - val_accuracy: 0.4750\n",
      "Epoch 14/400\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 3.9603e-06 - accuracy: 0.3673 - val_loss: 1.0715 - val_accuracy: 0.4750\n",
      "Epoch 15/400\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 3.5294e-06 - accuracy: 0.3595 - val_loss: 1.0836 - val_accuracy: 0.4583\n",
      "Epoch 16/400\n",
      "313/313 [==============================] - 53s 169ms/step - loss: 3.4413e-06 - accuracy: 0.3306 - val_loss: 1.0954 - val_accuracy: 0.4667\n",
      "Epoch 17/400\n",
      "313/313 [==============================] - 53s 169ms/step - loss: 2.0398e-06 - accuracy: 0.3753 - val_loss: 1.1133 - val_accuracy: 0.4833\n",
      "Epoch 18/400\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 2.0265e-06 - accuracy: 0.3938 - val_loss: 1.1183 - val_accuracy: 0.4833\n",
      "Epoch 19/400\n",
      "313/313 [==============================] - 53s 170ms/step - loss: 2.0948e-06 - accuracy: 0.3478 - val_loss: 1.1306 - val_accuracy: 0.4750\n",
      "Epoch 20/400\n",
      "313/313 [==============================] - 54s 171ms/step - loss: 1.7301e-06 - accuracy: 0.3471 - val_loss: 1.1332 - val_accuracy: 0.4833\n",
      "Epoch 21/400\n",
      "313/313 [==============================] - 53s 169ms/step - loss: 1.4375e-06 - accuracy: 0.3596 - val_loss: 1.1393 - val_accuracy: 0.4667\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8433aba790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8433aba790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6761309523809518 0.8445833333333336 0.5972222222222219 0.8383333333333339 0.689102564102564\n"
     ]
    }
   ],
   "source": [
    "random.seed(1000)\n",
    "biased_ratio = 1\n",
    "num_of_letters = 10\n",
    "array = [500,1000,2000,2500,3000,5000,10000,20000]\n",
    "f1scoreVal = []\n",
    "precVal = []\n",
    "recaVal = []\n",
    "TestDataset = create_dataset(testImagesVal, testLabelsVal, False)\n",
    "for i in range(len(array)):\n",
    "    num_of_train_images = array[i]\n",
    "\n",
    "    trainImagesComb, trainLabelsComb = createOddEvenNew(trainImagesVal, trainLabelsVal)\n",
    "    trainImages, trainLabels = incNumOfImages(trainImagesComb, trainLabelsComb, num_of_train_images)\n",
    "    trainLabels = convertToArray(trainLabels)\n",
    "    TrainDataset = create_dataset(trainImages, trainLabels)\n",
    "\n",
    "    model = modelLatest()\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, min_delta=0.0001, restore_best_weights=True)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    history = model.fit(TrainDataset, epochs=400, validation_data=TestDataset, callbacks=[callback], batch_size = batch_size)\n",
    "\n",
    "    ypred = model.predict(TestDataset)\n",
    "    #acc , combList, correctness = accuracyFunc(ypred, testLabelsVal, array)\n",
    "\n",
    "    temp_f1 = 0\n",
    "    temp_recall = 0\n",
    "    temp_precision = 0\n",
    "    temp_acc = 0\n",
    "    f1Val = []\n",
    "    distVal = []\n",
    "    y_test = testLabelsVal\n",
    "    y_predicted = calculate_labels(ypred)\n",
    "    for i in range(len(y_test)):\n",
    "        temp_f1 = temp_f1 + f1_score(y_test[i], y_predicted[i])\n",
    "        temp_recall = temp_recall + recall_score(y_test[i], y_predicted[i])\n",
    "        temp_precision = temp_precision + precision_score(y_test[i], y_predicted[i])\n",
    "        temp_acc = temp_acc + accuracy_score(y_test[i], y_predicted[i])\n",
    "        f1Val.append([f1_score(y_test[i], y_predicted[i])])\n",
    "        distVal.append([f1_score(y_test[i], y_predicted[i])])\n",
    "    f1score = temp_f1 / len(y_test)\n",
    "    precision = temp_precision / len(y_test)\n",
    "    recall = temp_recall / len(y_test)\n",
    "    acc = temp_acc / len(y_test)\n",
    "    f1scoreVal.append(f1score)\n",
    "    precVal.append(precision)\n",
    "    recaVal.append(recall)\n",
    "    print(f1score, precision, recall, acc, f1_score(y_test, y_predicted, average='micro'))\n",
    "\n",
    "#arrayVal = correlationmt(trainLabelsComb)\n",
    "#array = calculateDist(arrayVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odd+even\n",
      "[0.1525, 0.424722222222222, 0.44912698412698376, 0.4908531746031741, 0.5071825396825392, 0.5682936507936504, 0.6149206349206345, 0.6761309523809518]\n",
      "[0.3, 0.5972222222222222, 0.6055555555555556, 0.6938888888888889, 0.7123611111111113, 0.7130555555555559, 0.7673611111111113, 0.8445833333333336]\n",
      "[0.1027777777777778, 0.35833333333333345, 0.3888888888888889, 0.40833333333333355, 0.43055555555555564, 0.5055555555555554, 0.5444444444444445, 0.5972222222222219]\n"
     ]
    }
   ],
   "source": [
    "print(\"odd+even\")\n",
    "print(f1scoreVal)\n",
    "print(precVal)\n",
    "print(recaVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "8/8 [==============================] - 6s 271ms/step - loss: 0.6096 - accuracy: 0.2507 - val_loss: 0.5955 - val_accuracy: 0.3250\n",
      "Epoch 2/400\n",
      "8/8 [==============================] - 1s 178ms/step - loss: 0.2902 - accuracy: 0.5095 - val_loss: 0.6233 - val_accuracy: 0.4000\n",
      "Epoch 3/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 0.1759 - accuracy: 0.5314 - val_loss: 0.5777 - val_accuracy: 0.4083\n",
      "Epoch 4/400\n",
      "8/8 [==============================] - 1s 178ms/step - loss: 0.0933 - accuracy: 0.4820 - val_loss: 0.6187 - val_accuracy: 0.4583\n",
      "Epoch 5/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 0.0573 - accuracy: 0.4700 - val_loss: 0.6294 - val_accuracy: 0.4000\n",
      "Epoch 6/400\n",
      "8/8 [==============================] - 1s 178ms/step - loss: 0.0346 - accuracy: 0.4140 - val_loss: 0.6384 - val_accuracy: 0.4417\n",
      "Epoch 7/400\n",
      "8/8 [==============================] - 1s 182ms/step - loss: 0.0221 - accuracy: 0.4247 - val_loss: 0.6608 - val_accuracy: 0.5083\n",
      "Epoch 8/400\n",
      "8/8 [==============================] - 1s 178ms/step - loss: 0.0120 - accuracy: 0.4763 - val_loss: 0.6694 - val_accuracy: 0.4000\n",
      "Epoch 9/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 0.0088 - accuracy: 0.3339 - val_loss: 0.6935 - val_accuracy: 0.4750\n",
      "Epoch 10/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 0.0062 - accuracy: 0.5031 - val_loss: 0.6765 - val_accuracy: 0.4000\n",
      "Epoch 11/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 0.0055 - accuracy: 0.3215 - val_loss: 0.7172 - val_accuracy: 0.4917\n",
      "Epoch 12/400\n",
      "8/8 [==============================] - 1s 178ms/step - loss: 0.0030 - accuracy: 0.4510 - val_loss: 0.7021 - val_accuracy: 0.4167\n",
      "Epoch 13/400\n",
      "8/8 [==============================] - 1s 178ms/step - loss: 0.0026 - accuracy: 0.2772 - val_loss: 0.7080 - val_accuracy: 0.4500\n",
      "Epoch 14/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 0.0023 - accuracy: 0.4157 - val_loss: 0.7583 - val_accuracy: 0.5167\n",
      "Epoch 15/400\n",
      "8/8 [==============================] - 1s 178ms/step - loss: 0.0018 - accuracy: 0.4186 - val_loss: 0.7265 - val_accuracy: 0.4167\n",
      "Epoch 16/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 0.0016 - accuracy: 0.3226 - val_loss: 0.7391 - val_accuracy: 0.4667\n",
      "Epoch 17/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 0.0011 - accuracy: 0.4058 - val_loss: 0.7389 - val_accuracy: 0.4417\n",
      "Epoch 18/400\n",
      "8/8 [==============================] - 1s 178ms/step - loss: 7.4800e-04 - accuracy: 0.3591 - val_loss: 0.7514 - val_accuracy: 0.4417\n",
      "Epoch 19/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 8.1623e-04 - accuracy: 0.3265 - val_loss: 0.7611 - val_accuracy: 0.4917\n",
      "Epoch 20/400\n",
      "8/8 [==============================] - 1s 177ms/step - loss: 9.7067e-04 - accuracy: 0.5226 - val_loss: 0.7580 - val_accuracy: 0.4167\n",
      "Epoch 21/400\n",
      "8/8 [==============================] - 1s 175ms/step - loss: 7.1210e-04 - accuracy: 0.3461 - val_loss: 0.7616 - val_accuracy: 0.4417\n",
      "Epoch 22/400\n",
      "8/8 [==============================] - 1s 178ms/step - loss: 4.9273e-04 - accuracy: 0.3895 - val_loss: 0.7719 - val_accuracy: 0.4583\n",
      "Epoch 23/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 5.6737e-04 - accuracy: 0.4273 - val_loss: 0.7804 - val_accuracy: 0.4500\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8450598790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8450598790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4671230158730156 0.6498611111111111 0.4055555555555557 0.7716666666666665 0.5159010600706714\n",
      "Epoch 1/400\n",
      "16/16 [==============================] - 7s 211ms/step - loss: 0.5096 - accuracy: 0.3889 - val_loss: 0.6122 - val_accuracy: 0.4333\n",
      "Epoch 2/400\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.1618 - accuracy: 0.5229 - val_loss: 0.6427 - val_accuracy: 0.4000\n",
      "Epoch 3/400\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0623 - accuracy: 0.5198 - val_loss: 0.6731 - val_accuracy: 0.4333\n",
      "Epoch 4/400\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0305 - accuracy: 0.5107 - val_loss: 0.7007 - val_accuracy: 0.4917\n",
      "Epoch 5/400\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0201 - accuracy: 0.4931 - val_loss: 0.7216 - val_accuracy: 0.4167\n",
      "Epoch 6/400\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0101 - accuracy: 0.4247 - val_loss: 0.7540 - val_accuracy: 0.4083\n",
      "Epoch 7/400\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0040 - accuracy: 0.4250 - val_loss: 0.7660 - val_accuracy: 0.4333\n",
      "Epoch 8/400\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 0.0024 - accuracy: 0.4894 - val_loss: 0.7663 - val_accuracy: 0.4167\n",
      "Epoch 9/400\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0018 - accuracy: 0.4939 - val_loss: 0.7954 - val_accuracy: 0.4000\n",
      "Epoch 10/400\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 0.0014 - accuracy: 0.4588 - val_loss: 0.8357 - val_accuracy: 0.4333\n",
      "Epoch 11/400\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.0011 - accuracy: 0.4636 - val_loss: 0.8291 - val_accuracy: 0.4250\n",
      "Epoch 12/400\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 9.0384e-04 - accuracy: 0.5072 - val_loss: 0.8562 - val_accuracy: 0.4167\n",
      "Epoch 13/400\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 5.6754e-04 - accuracy: 0.4108 - val_loss: 0.8571 - val_accuracy: 0.4333\n",
      "Epoch 14/400\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 4.1825e-04 - accuracy: 0.4429 - val_loss: 0.8683 - val_accuracy: 0.4417\n",
      "Epoch 15/400\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 3.5719e-04 - accuracy: 0.4764 - val_loss: 0.8691 - val_accuracy: 0.4333\n",
      "Epoch 16/400\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 2.9985e-04 - accuracy: 0.4431 - val_loss: 0.8798 - val_accuracy: 0.4500\n",
      "Epoch 17/400\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 2.9886e-04 - accuracy: 0.4438 - val_loss: 0.8800 - val_accuracy: 0.4250\n",
      "Epoch 18/400\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 2.7438e-04 - accuracy: 0.4389 - val_loss: 0.8787 - val_accuracy: 0.4333\n",
      "Epoch 19/400\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 2.3988e-04 - accuracy: 0.4849 - val_loss: 0.8968 - val_accuracy: 0.4083\n",
      "Epoch 20/400\n",
      "16/16 [==============================] - 3s 170ms/step - loss: 2.4675e-04 - accuracy: 0.3926 - val_loss: 0.9047 - val_accuracy: 0.4333\n",
      "Epoch 21/400\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 2.3139e-04 - accuracy: 0.4402 - val_loss: 0.9125 - val_accuracy: 0.4333\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84333104c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84333104c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38742063492063483 0.6023611111111111 0.31111111111111117 0.7583333333333331 0.4357976653696498\n",
      "Epoch 1/400\n",
      "32/32 [==============================] - 10s 186ms/step - loss: 0.4463 - accuracy: 0.4680 - val_loss: 0.6467 - val_accuracy: 0.4917\n",
      "Epoch 2/400\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 0.0467 - accuracy: 0.5596 - val_loss: 0.7658 - val_accuracy: 0.4250\n",
      "Epoch 3/400\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 0.0118 - accuracy: 0.4703 - val_loss: 0.8274 - val_accuracy: 0.4250\n",
      "Epoch 4/400\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 0.0060 - accuracy: 0.4625 - val_loss: 0.8337 - val_accuracy: 0.4417\n",
      "Epoch 5/400\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 0.0033 - accuracy: 0.4759 - val_loss: 0.8776 - val_accuracy: 0.4833\n",
      "Epoch 6/400\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 0.0018 - accuracy: 0.4961 - val_loss: 0.9004 - val_accuracy: 0.4083\n",
      "Epoch 7/400\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 0.0019 - accuracy: 0.4446 - val_loss: 0.8775 - val_accuracy: 0.4667\n",
      "Epoch 8/400\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 0.0020 - accuracy: 0.5992 - val_loss: 0.9749 - val_accuracy: 0.4333\n",
      "Epoch 9/400\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 0.0011 - accuracy: 0.4773 - val_loss: 0.9142 - val_accuracy: 0.4583\n",
      "Epoch 10/400\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 5.0452e-04 - accuracy: 0.5043 - val_loss: 0.9194 - val_accuracy: 0.4583\n",
      "Epoch 11/400\n",
      "32/32 [==============================] - 5s 165ms/step - loss: 1.4525e-04 - accuracy: 0.4605 - val_loss: 0.9384 - val_accuracy: 0.4583\n",
      "Epoch 12/400\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 1.2266e-04 - accuracy: 0.4608 - val_loss: 0.9424 - val_accuracy: 0.4417\n",
      "Epoch 13/400\n",
      "32/32 [==============================] - 5s 167ms/step - loss: 1.3585e-04 - accuracy: 0.3652 - val_loss: 0.9433 - val_accuracy: 0.4750\n",
      "Epoch 14/400\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 8.7252e-05 - accuracy: 0.4567 - val_loss: 0.9673 - val_accuracy: 0.4583\n",
      "Epoch 15/400\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 6.3588e-05 - accuracy: 0.4455 - val_loss: 0.9723 - val_accuracy: 0.4667\n",
      "Epoch 16/400\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 5.6798e-05 - accuracy: 0.4510 - val_loss: 0.9845 - val_accuracy: 0.4750\n",
      "Epoch 17/400\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 7.3500e-05 - accuracy: 0.4383 - val_loss: 1.0018 - val_accuracy: 0.4500\n",
      "Epoch 18/400\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 4.5345e-05 - accuracy: 0.4199 - val_loss: 0.9995 - val_accuracy: 0.4750\n",
      "Epoch 19/400\n",
      "32/32 [==============================] - 5s 165ms/step - loss: 4.8665e-05 - accuracy: 0.4216 - val_loss: 1.0097 - val_accuracy: 0.4500\n",
      "Epoch 20/400\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 4.0231e-05 - accuracy: 0.4385 - val_loss: 1.0186 - val_accuracy: 0.4500\n",
      "Epoch 21/400\n",
      "32/32 [==============================] - 5s 166ms/step - loss: 3.9533e-05 - accuracy: 0.4472 - val_loss: 1.0212 - val_accuracy: 0.4667\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8585682ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8585682ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5030753968253964 0.7611111111111112 0.4055555555555556 0.783333333333333 0.5289855072463768\n",
      "Epoch 1/400\n",
      "40/40 [==============================] - 11s 180ms/step - loss: 0.3995 - accuracy: 0.4689 - val_loss: 0.6190 - val_accuracy: 0.4333\n",
      "Epoch 2/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 0.0322 - accuracy: 0.5136 - val_loss: 0.6547 - val_accuracy: 0.4333\n",
      "Epoch 3/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 0.0079 - accuracy: 0.4877 - val_loss: 1.0691 - val_accuracy: 0.3000\n",
      "Epoch 4/400\n",
      "40/40 [==============================] - 7s 165ms/step - loss: 0.0320 - accuracy: 0.4741 - val_loss: 0.7491 - val_accuracy: 0.4667\n",
      "Epoch 5/400\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 0.0020 - accuracy: 0.4612 - val_loss: 0.7851 - val_accuracy: 0.4417\n",
      "Epoch 6/400\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 9.0156e-04 - accuracy: 0.4289 - val_loss: 0.8175 - val_accuracy: 0.4250\n",
      "Epoch 7/400\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 5.4559e-04 - accuracy: 0.4820 - val_loss: 0.8210 - val_accuracy: 0.4000\n",
      "Epoch 8/400\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 3.9090e-04 - accuracy: 0.4553 - val_loss: 0.8443 - val_accuracy: 0.4167\n",
      "Epoch 9/400\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 3.6800e-04 - accuracy: 0.4186 - val_loss: 0.8434 - val_accuracy: 0.4250\n",
      "Epoch 10/400\n",
      "40/40 [==============================] - 7s 165ms/step - loss: 1.7140e-04 - accuracy: 0.4650 - val_loss: 0.8614 - val_accuracy: 0.4417\n",
      "Epoch 11/400\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 1.4847e-04 - accuracy: 0.4667 - val_loss: 0.8698 - val_accuracy: 0.4333\n",
      "Epoch 12/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 1.2155e-04 - accuracy: 0.4596 - val_loss: 0.8905 - val_accuracy: 0.4250\n",
      "Epoch 13/400\n",
      "40/40 [==============================] - 7s 165ms/step - loss: 1.0364e-04 - accuracy: 0.4472 - val_loss: 0.8995 - val_accuracy: 0.4250\n",
      "Epoch 14/400\n",
      "40/40 [==============================] - 7s 165ms/step - loss: 8.1929e-05 - accuracy: 0.4634 - val_loss: 0.9055 - val_accuracy: 0.4000\n",
      "Epoch 15/400\n",
      "40/40 [==============================] - 7s 165ms/step - loss: 7.6205e-05 - accuracy: 0.4431 - val_loss: 0.9154 - val_accuracy: 0.4250\n",
      "Epoch 16/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 6.6018e-05 - accuracy: 0.4522 - val_loss: 0.9265 - val_accuracy: 0.4167\n",
      "Epoch 17/400\n",
      "40/40 [==============================] - 7s 165ms/step - loss: 6.5413e-05 - accuracy: 0.4392 - val_loss: 0.9375 - val_accuracy: 0.4250\n",
      "Epoch 18/400\n",
      "40/40 [==============================] - 7s 165ms/step - loss: 6.1347e-05 - accuracy: 0.4215 - val_loss: 0.9440 - val_accuracy: 0.4167\n",
      "Epoch 19/400\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 7.8324e-05 - accuracy: 0.4785 - val_loss: 0.9459 - val_accuracy: 0.4083\n",
      "Epoch 20/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 5.0124e-05 - accuracy: 0.4461 - val_loss: 0.9525 - val_accuracy: 0.4250\n",
      "Epoch 21/400\n",
      "40/40 [==============================] - 7s 166ms/step - loss: 4.0157e-05 - accuracy: 0.4662 - val_loss: 0.9602 - val_accuracy: 0.4250\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84339deee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84339deee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5182738095238091 0.7250000000000001 0.4416666666666666 0.7866666666666663 0.554006968641115\n",
      "Epoch 1/400\n",
      "47/47 [==============================] - 13s 183ms/step - loss: 0.3621 - accuracy: 0.4627 - val_loss: 0.7401 - val_accuracy: 0.4750\n",
      "Epoch 2/400\n",
      "47/47 [==============================] - 8s 170ms/step - loss: 0.0228 - accuracy: 0.4897 - val_loss: 0.8434 - val_accuracy: 0.3917\n",
      "Epoch 3/400\n",
      "47/47 [==============================] - 8s 169ms/step - loss: 0.0056 - accuracy: 0.4574 - val_loss: 0.8823 - val_accuracy: 0.3750\n",
      "Epoch 4/400\n",
      "47/47 [==============================] - 8s 170ms/step - loss: 0.0021 - accuracy: 0.3969 - val_loss: 0.9232 - val_accuracy: 0.4333\n",
      "Epoch 5/400\n",
      "47/47 [==============================] - 8s 172ms/step - loss: 9.3165e-04 - accuracy: 0.4258 - val_loss: 0.9582 - val_accuracy: 0.4250\n",
      "Epoch 6/400\n",
      "47/47 [==============================] - 8s 171ms/step - loss: 9.1125e-04 - accuracy: 0.4583 - val_loss: 0.9928 - val_accuracy: 0.3500\n",
      "Epoch 7/400\n",
      "47/47 [==============================] - 8s 171ms/step - loss: 3.5840e-04 - accuracy: 0.4007 - val_loss: 1.0096 - val_accuracy: 0.3750\n",
      "Epoch 8/400\n",
      "47/47 [==============================] - 8s 169ms/step - loss: 1.8861e-04 - accuracy: 0.4021 - val_loss: 1.0275 - val_accuracy: 0.4000\n",
      "Epoch 9/400\n",
      "47/47 [==============================] - 8s 168ms/step - loss: 1.5580e-04 - accuracy: 0.4228 - val_loss: 1.0407 - val_accuracy: 0.4083\n",
      "Epoch 10/400\n",
      "47/47 [==============================] - 8s 169ms/step - loss: 9.9507e-05 - accuracy: 0.4032 - val_loss: 1.0583 - val_accuracy: 0.3833\n",
      "Epoch 11/400\n",
      "47/47 [==============================] - 8s 168ms/step - loss: 9.1831e-05 - accuracy: 0.4337 - val_loss: 1.0649 - val_accuracy: 0.3917\n",
      "Epoch 12/400\n",
      "47/47 [==============================] - 8s 170ms/step - loss: 8.6377e-05 - accuracy: 0.4484 - val_loss: 1.0798 - val_accuracy: 0.3917\n",
      "Epoch 13/400\n",
      "47/47 [==============================] - 8s 169ms/step - loss: 6.5258e-05 - accuracy: 0.4256 - val_loss: 1.0969 - val_accuracy: 0.4000\n",
      "Epoch 14/400\n",
      "47/47 [==============================] - 8s 169ms/step - loss: 6.2407e-05 - accuracy: 0.4675 - val_loss: 1.1038 - val_accuracy: 0.3833\n",
      "Epoch 15/400\n",
      "47/47 [==============================] - 8s 169ms/step - loss: 6.4138e-05 - accuracy: 0.4520 - val_loss: 1.1150 - val_accuracy: 0.3917\n",
      "Epoch 16/400\n",
      "47/47 [==============================] - 8s 169ms/step - loss: 4.5553e-05 - accuracy: 0.4149 - val_loss: 1.1271 - val_accuracy: 0.3917\n",
      "Epoch 17/400\n",
      "47/47 [==============================] - 8s 170ms/step - loss: 5.6044e-05 - accuracy: 0.4262 - val_loss: 1.1405 - val_accuracy: 0.3917\n",
      "Epoch 18/400\n",
      "47/47 [==============================] - 8s 171ms/step - loss: 4.2701e-05 - accuracy: 0.4005 - val_loss: 1.1493 - val_accuracy: 0.3917\n",
      "Epoch 19/400\n",
      "47/47 [==============================] - 8s 170ms/step - loss: 3.0533e-05 - accuracy: 0.4306 - val_loss: 1.1465 - val_accuracy: 0.3917\n",
      "Epoch 20/400\n",
      "47/47 [==============================] - 8s 168ms/step - loss: 2.7536e-05 - accuracy: 0.3953 - val_loss: 1.1597 - val_accuracy: 0.3917\n",
      "Epoch 21/400\n",
      "47/47 [==============================] - 8s 169ms/step - loss: 3.6537e-05 - accuracy: 0.3969 - val_loss: 1.1560 - val_accuracy: 0.3917\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8433794c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8433794c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5349206349206344 0.7548611111111111 0.45000000000000007 0.7891666666666666 0.5615251299826689\n",
      "Epoch 1/400\n",
      "79/79 [==============================] - 17s 174ms/step - loss: 0.2429 - accuracy: 0.4906 - val_loss: 0.7874 - val_accuracy: 0.4417\n",
      "Epoch 2/400\n",
      "79/79 [==============================] - 13s 167ms/step - loss: 0.0104 - accuracy: 0.4818 - val_loss: 0.7999 - val_accuracy: 0.4167\n",
      "Epoch 3/400\n",
      "79/79 [==============================] - 13s 169ms/step - loss: 0.0032 - accuracy: 0.4521 - val_loss: 0.8380 - val_accuracy: 0.4250\n",
      "Epoch 4/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 6.6769e-04 - accuracy: 0.3901 - val_loss: 0.9079 - val_accuracy: 0.3917\n",
      "Epoch 5/400\n",
      "79/79 [==============================] - 13s 167ms/step - loss: 2.6992e-04 - accuracy: 0.3696 - val_loss: 0.9496 - val_accuracy: 0.3833\n",
      "Epoch 6/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 2.4635e-04 - accuracy: 0.4532 - val_loss: 0.9616 - val_accuracy: 0.4167\n",
      "Epoch 7/400\n",
      "79/79 [==============================] - 13s 169ms/step - loss: 9.7808e-05 - accuracy: 0.4111 - val_loss: 0.9799 - val_accuracy: 0.4167\n",
      "Epoch 8/400\n",
      "79/79 [==============================] - 13s 167ms/step - loss: 1.1998e-04 - accuracy: 0.4037 - val_loss: 0.9572 - val_accuracy: 0.4250\n",
      "Epoch 9/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 4.6677e-05 - accuracy: 0.4395 - val_loss: 0.9828 - val_accuracy: 0.4167\n",
      "Epoch 10/400\n",
      "79/79 [==============================] - 13s 169ms/step - loss: 3.2352e-05 - accuracy: 0.4346 - val_loss: 1.0025 - val_accuracy: 0.4250\n",
      "Epoch 11/400\n",
      "79/79 [==============================] - 13s 169ms/step - loss: 2.7543e-05 - accuracy: 0.4394 - val_loss: 1.0127 - val_accuracy: 0.4167\n",
      "Epoch 12/400\n",
      "79/79 [==============================] - 13s 167ms/step - loss: 2.2806e-05 - accuracy: 0.4308 - val_loss: 1.0336 - val_accuracy: 0.4167\n",
      "Epoch 13/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 1.8056e-05 - accuracy: 0.4205 - val_loss: 1.0443 - val_accuracy: 0.4167\n",
      "Epoch 14/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 1.5886e-05 - accuracy: 0.4275 - val_loss: 1.0580 - val_accuracy: 0.4000\n",
      "Epoch 15/400\n",
      "79/79 [==============================] - 13s 167ms/step - loss: 1.7827e-05 - accuracy: 0.4084 - val_loss: 1.1139 - val_accuracy: 0.4083\n",
      "Epoch 16/400\n",
      "79/79 [==============================] - 13s 169ms/step - loss: 2.7011e-05 - accuracy: 0.3673 - val_loss: 1.0645 - val_accuracy: 0.4250\n",
      "Epoch 17/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 1.0788e-05 - accuracy: 0.4275 - val_loss: 1.0788 - val_accuracy: 0.4250\n",
      "Epoch 18/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 1.1092e-05 - accuracy: 0.4083 - val_loss: 1.0851 - val_accuracy: 0.4167\n",
      "Epoch 19/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 1.2542e-05 - accuracy: 0.4094 - val_loss: 1.1020 - val_accuracy: 0.4083\n",
      "Epoch 20/400\n",
      "79/79 [==============================] - 14s 172ms/step - loss: 9.0821e-06 - accuracy: 0.3840 - val_loss: 1.1066 - val_accuracy: 0.4083\n",
      "Epoch 21/400\n",
      "79/79 [==============================] - 13s 169ms/step - loss: 1.0145e-05 - accuracy: 0.4038 - val_loss: 1.1087 - val_accuracy: 0.4083\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8584f0e940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8584f0e940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5595238095238091 0.7770833333333336 0.47222222222222215 0.7974999999999995 0.5831903945111492\n",
      "Epoch 1/400\n",
      "157/157 [==============================] - 32s 172ms/step - loss: 0.1622 - accuracy: 0.4665 - val_loss: 0.7952 - val_accuracy: 0.4667\n",
      "Epoch 2/400\n",
      "157/157 [==============================] - 26s 169ms/step - loss: 0.0036 - accuracy: 0.4148 - val_loss: 1.0294 - val_accuracy: 0.4167\n",
      "Epoch 3/400\n",
      "157/157 [==============================] - 26s 168ms/step - loss: 0.0050 - accuracy: 0.3634 - val_loss: 0.8799 - val_accuracy: 0.3917\n",
      "Epoch 4/400\n",
      "157/157 [==============================] - 26s 167ms/step - loss: 0.0011 - accuracy: 0.4071 - val_loss: 0.8729 - val_accuracy: 0.4000\n",
      "Epoch 5/400\n",
      "157/157 [==============================] - 26s 167ms/step - loss: 2.0379e-04 - accuracy: 0.3696 - val_loss: 0.9655 - val_accuracy: 0.3417\n",
      "Epoch 6/400\n",
      "157/157 [==============================] - 26s 167ms/step - loss: 0.0020 - accuracy: 0.3657 - val_loss: 0.9196 - val_accuracy: 0.3667\n",
      "Epoch 7/400\n",
      "157/157 [==============================] - 27s 169ms/step - loss: 9.9842e-05 - accuracy: 0.3411 - val_loss: 0.9557 - val_accuracy: 0.3833\n",
      "Epoch 8/400\n",
      "157/157 [==============================] - 26s 167ms/step - loss: 0.0487 - accuracy: 0.4334 - val_loss: 0.8251 - val_accuracy: 0.3250\n",
      "Epoch 9/400\n",
      "157/157 [==============================] - 26s 167ms/step - loss: 9.1860e-04 - accuracy: 0.3526 - val_loss: 0.7949 - val_accuracy: 0.4167\n",
      "Epoch 10/400\n",
      "157/157 [==============================] - 26s 169ms/step - loss: 9.9864e-05 - accuracy: 0.4364 - val_loss: 0.8710 - val_accuracy: 0.4250\n",
      "Epoch 11/400\n",
      "157/157 [==============================] - 26s 167ms/step - loss: 4.5903e-05 - accuracy: 0.4585 - val_loss: 0.8753 - val_accuracy: 0.4250\n",
      "Epoch 12/400\n",
      "157/157 [==============================] - 27s 170ms/step - loss: 4.2758e-05 - accuracy: 0.4142 - val_loss: 0.9111 - val_accuracy: 0.4667\n",
      "Epoch 13/400\n",
      "157/157 [==============================] - 27s 173ms/step - loss: 5.1786e-05 - accuracy: 0.4104 - val_loss: 0.8999 - val_accuracy: 0.4333\n",
      "Epoch 14/400\n",
      "157/157 [==============================] - 27s 172ms/step - loss: 2.2584e-05 - accuracy: 0.3965 - val_loss: 0.9175 - val_accuracy: 0.4250\n",
      "Epoch 15/400\n",
      "157/157 [==============================] - 27s 172ms/step - loss: 1.0854e-05 - accuracy: 0.4481 - val_loss: 0.9322 - val_accuracy: 0.4000\n",
      "Epoch 16/400\n",
      "157/157 [==============================] - 27s 172ms/step - loss: 1.0478e-05 - accuracy: 0.4085 - val_loss: 0.9411 - val_accuracy: 0.4000\n",
      "Epoch 17/400\n",
      "157/157 [==============================] - 27s 171ms/step - loss: 8.1326e-06 - accuracy: 0.4086 - val_loss: 0.9513 - val_accuracy: 0.4000\n",
      "Epoch 18/400\n",
      "157/157 [==============================] - 27s 171ms/step - loss: 6.8602e-06 - accuracy: 0.4154 - val_loss: 0.9639 - val_accuracy: 0.4000\n",
      "Epoch 19/400\n",
      "157/157 [==============================] - 27s 171ms/step - loss: 6.2047e-06 - accuracy: 0.3985 - val_loss: 0.9725 - val_accuracy: 0.4000\n",
      "Epoch 20/400\n",
      "157/157 [==============================] - 27s 171ms/step - loss: 4.7748e-06 - accuracy: 0.4032 - val_loss: 0.9809 - val_accuracy: 0.4000\n",
      "Epoch 21/400\n",
      "157/157 [==============================] - 27s 173ms/step - loss: 4.4721e-06 - accuracy: 0.4044 - val_loss: 0.9900 - val_accuracy: 0.4083\n",
      "Epoch 22/400\n",
      "157/157 [==============================] - 27s 172ms/step - loss: 4.2310e-06 - accuracy: 0.4081 - val_loss: 0.9953 - val_accuracy: 0.4083\n",
      "Epoch 23/400\n",
      "157/157 [==============================] - 27s 171ms/step - loss: 4.3690e-06 - accuracy: 0.4331 - val_loss: 1.0033 - val_accuracy: 0.4000\n",
      "Epoch 24/400\n",
      "157/157 [==============================] - 27s 172ms/step - loss: 4.0574e-06 - accuracy: 0.4170 - val_loss: 1.0129 - val_accuracy: 0.4000\n",
      "Epoch 25/400\n",
      "157/157 [==============================] - 27s 174ms/step - loss: 2.9969e-06 - accuracy: 0.4037 - val_loss: 1.0202 - val_accuracy: 0.4167\n",
      "Epoch 26/400\n",
      "157/157 [==============================] - 27s 171ms/step - loss: 2.7076e-06 - accuracy: 0.4106 - val_loss: 1.0271 - val_accuracy: 0.4167\n",
      "Epoch 27/400\n",
      "157/157 [==============================] - 27s 174ms/step - loss: 3.1016e-06 - accuracy: 0.3978 - val_loss: 1.0345 - val_accuracy: 0.4000\n",
      "Epoch 28/400\n",
      "157/157 [==============================] - 27s 173ms/step - loss: 2.3359e-06 - accuracy: 0.4232 - val_loss: 1.0396 - val_accuracy: 0.3917\n",
      "Epoch 29/400\n",
      "157/157 [==============================] - 27s 171ms/step - loss: 1.9504e-06 - accuracy: 0.4174 - val_loss: 1.0485 - val_accuracy: 0.4083\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8585ba53a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8585ba53a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6693253968253966 0.8006944444444447 0.6083333333333332 0.8283333333333331 0.6801242236024845\n"
     ]
    }
   ],
   "source": [
    "random.seed(1000)\n",
    "biased_ratio = 32\n",
    "num_of_letters = 10\n",
    "array = [500,1000,2000,2500,3000,5000,10000]\n",
    "f1scoreVal = []\n",
    "precVal = []\n",
    "recaVal = []\n",
    "TestDataset = create_dataset(testImagesVal, testLabelsVal, False)\n",
    "for i in range(len(array)):\n",
    "    num_of_train_images = array[i]\n",
    "\n",
    "    trainImagesComb, trainLabelsComb = createOddEvenNew(trainImagesVal, trainLabelsVal)\n",
    "    trainImages, trainLabels = incNumOfImages(trainImagesComb, trainLabelsComb, num_of_train_images)\n",
    "    trainLabels = convertToArray(trainLabels)\n",
    "    TrainDataset = create_dataset(trainImages, trainLabels)\n",
    "\n",
    "    model = modelLatest()\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, min_delta=0.0001, restore_best_weights=True)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    history = model.fit(TrainDataset, epochs=400, validation_data=TestDataset, callbacks=[callback], batch_size = batch_size)\n",
    "\n",
    "    ypred = model.predict(TestDataset)\n",
    "    #acc , combList, correctness = accuracyFunc(ypred, testLabelsVal, array)\n",
    "\n",
    "    temp_f1 = 0\n",
    "    temp_recall = 0\n",
    "    temp_precision = 0\n",
    "    temp_acc = 0\n",
    "    f1Val = []\n",
    "    distVal = []\n",
    "    y_test = testLabelsVal\n",
    "    y_predicted = calculate_labels(ypred)\n",
    "    for i in range(len(y_test)):\n",
    "        temp_f1 = temp_f1 + f1_score(y_test[i], y_predicted[i])\n",
    "        temp_recall = temp_recall + recall_score(y_test[i], y_predicted[i])\n",
    "        temp_precision = temp_precision + precision_score(y_test[i], y_predicted[i])\n",
    "        temp_acc = temp_acc + accuracy_score(y_test[i], y_predicted[i])\n",
    "        f1Val.append([f1_score(y_test[i], y_predicted[i])])\n",
    "        distVal.append([f1_score(y_test[i], y_predicted[i])])\n",
    "    f1score = temp_f1 / len(y_test)\n",
    "    precision = temp_precision / len(y_test)\n",
    "    recall = temp_recall / len(y_test)\n",
    "    acc = temp_acc / len(y_test)\n",
    "    f1scoreVal.append(f1score)\n",
    "    precVal.append(precision)\n",
    "    recaVal.append(recall)\n",
    "    print(f1score, precision, recall, acc, f1_score(y_test, y_predicted, average='micro'))\n",
    "\n",
    "#arrayVal = correlationmt(trainLabelsComb)\n",
    "#array = calculateDist(arrayVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odd+even\n",
      "[0.4671230158730156, 0.38742063492063483, 0.5030753968253964, 0.5182738095238091, 0.5349206349206344, 0.5595238095238091, 0.6693253968253966]\n",
      "[0.6498611111111111, 0.6023611111111111, 0.7611111111111112, 0.7250000000000001, 0.7548611111111111, 0.7770833333333336, 0.8006944444444447]\n",
      "[0.4055555555555557, 0.31111111111111117, 0.4055555555555556, 0.4416666666666666, 0.45000000000000007, 0.47222222222222215, 0.6083333333333332]\n"
     ]
    }
   ],
   "source": [
    "print(\"odd+even\")\n",
    "print(f1scoreVal)\n",
    "print(precVal)\n",
    "print(recaVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "8/8 [==============================] - 5s 259ms/step - loss: 0.5581 - accuracy: 0.3695 - val_loss: 0.5878 - val_accuracy: 0.4333\n",
      "Epoch 2/400\n",
      "8/8 [==============================] - 1s 181ms/step - loss: 0.2521 - accuracy: 0.4776 - val_loss: 0.6752 - val_accuracy: 0.3500\n",
      "Epoch 3/400\n",
      "8/8 [==============================] - 1s 180ms/step - loss: 0.1649 - accuracy: 0.4814 - val_loss: 0.6091 - val_accuracy: 0.4250\n",
      "Epoch 4/400\n",
      "8/8 [==============================] - 1s 180ms/step - loss: 0.0865 - accuracy: 0.5041 - val_loss: 0.6661 - val_accuracy: 0.4583\n",
      "Epoch 5/400\n",
      "8/8 [==============================] - 1s 181ms/step - loss: 0.0519 - accuracy: 0.5355 - val_loss: 0.6921 - val_accuracy: 0.4333\n",
      "Epoch 6/400\n",
      "8/8 [==============================] - 1s 177ms/step - loss: 0.0283 - accuracy: 0.4997 - val_loss: 0.6969 - val_accuracy: 0.4583\n",
      "Epoch 7/400\n",
      "8/8 [==============================] - 1s 178ms/step - loss: 0.0183 - accuracy: 0.4459 - val_loss: 0.7003 - val_accuracy: 0.3667\n",
      "Epoch 8/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 0.0106 - accuracy: 0.4637 - val_loss: 0.7048 - val_accuracy: 0.4500\n",
      "Epoch 9/400\n",
      "8/8 [==============================] - 1s 180ms/step - loss: 0.0068 - accuracy: 0.5483 - val_loss: 0.7148 - val_accuracy: 0.4333\n",
      "Epoch 10/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 0.0046 - accuracy: 0.4187 - val_loss: 0.7608 - val_accuracy: 0.4167\n",
      "Epoch 11/400\n",
      "8/8 [==============================] - 1s 180ms/step - loss: 0.0045 - accuracy: 0.4293 - val_loss: 0.7529 - val_accuracy: 0.4083\n",
      "Epoch 12/400\n",
      "8/8 [==============================] - 1s 178ms/step - loss: 0.0023 - accuracy: 0.4347 - val_loss: 0.7794 - val_accuracy: 0.4417\n",
      "Epoch 13/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 0.0020 - accuracy: 0.4722 - val_loss: 0.7664 - val_accuracy: 0.4333\n",
      "Epoch 14/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 0.0015 - accuracy: 0.4493 - val_loss: 0.8079 - val_accuracy: 0.4500\n",
      "Epoch 15/400\n",
      "8/8 [==============================] - 1s 180ms/step - loss: 0.0010 - accuracy: 0.3866 - val_loss: 0.8179 - val_accuracy: 0.4167\n",
      "Epoch 16/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 0.0012 - accuracy: 0.4395 - val_loss: 0.8245 - val_accuracy: 0.4250\n",
      "Epoch 17/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 0.0012 - accuracy: 0.4672 - val_loss: 0.8012 - val_accuracy: 0.4250\n",
      "Epoch 18/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 8.3861e-04 - accuracy: 0.4277 - val_loss: 0.8203 - val_accuracy: 0.4167\n",
      "Epoch 19/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 7.8924e-04 - accuracy: 0.4154 - val_loss: 0.8108 - val_accuracy: 0.3833\n",
      "Epoch 20/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 6.7195e-04 - accuracy: 0.4125 - val_loss: 0.8139 - val_accuracy: 0.4083\n",
      "Epoch 21/400\n",
      "8/8 [==============================] - 1s 179ms/step - loss: 4.6478e-04 - accuracy: 0.3720 - val_loss: 0.8316 - val_accuracy: 0.4083\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f842bcfbd30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f842bcfbd30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3419047619047618 0.52875 0.2749999999999998 0.7474999999999998 0.39520958083832336\n",
      "Epoch 1/400\n",
      "16/16 [==============================] - 7s 214ms/step - loss: 0.5379 - accuracy: 0.3753 - val_loss: 0.6213 - val_accuracy: 0.4083\n",
      "Epoch 2/400\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.1601 - accuracy: 0.5167 - val_loss: 0.6634 - val_accuracy: 0.4000\n",
      "Epoch 3/400\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0637 - accuracy: 0.5448 - val_loss: 0.7131 - val_accuracy: 0.4750\n",
      "Epoch 4/400\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0235 - accuracy: 0.5445 - val_loss: 0.7833 - val_accuracy: 0.4333\n",
      "Epoch 5/400\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0146 - accuracy: 0.5083 - val_loss: 0.8219 - val_accuracy: 0.4250\n",
      "Epoch 6/400\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 0.0064 - accuracy: 0.4958 - val_loss: 0.8319 - val_accuracy: 0.4917\n",
      "Epoch 7/400\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0049 - accuracy: 0.5449 - val_loss: 0.8522 - val_accuracy: 0.3917\n",
      "Epoch 8/400\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 0.0027 - accuracy: 0.4656 - val_loss: 0.9084 - val_accuracy: 0.4333\n",
      "Epoch 9/400\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0030 - accuracy: 0.4575 - val_loss: 0.8867 - val_accuracy: 0.4083\n",
      "Epoch 10/400\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0017 - accuracy: 0.3847 - val_loss: 0.8845 - val_accuracy: 0.4167\n",
      "Epoch 11/400\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.0011 - accuracy: 0.4937 - val_loss: 0.8766 - val_accuracy: 0.4667\n",
      "Epoch 12/400\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 0.0013 - accuracy: 0.5022 - val_loss: 0.8951 - val_accuracy: 0.4250\n",
      "Epoch 13/400\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 8.5248e-04 - accuracy: 0.4251 - val_loss: 0.8914 - val_accuracy: 0.4250\n",
      "Epoch 14/400\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 5.1552e-04 - accuracy: 0.4226 - val_loss: 0.9226 - val_accuracy: 0.4333\n",
      "Epoch 15/400\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 4.4815e-04 - accuracy: 0.4166 - val_loss: 0.9299 - val_accuracy: 0.4250\n",
      "Epoch 16/400\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 3.3886e-04 - accuracy: 0.4634 - val_loss: 0.9527 - val_accuracy: 0.4500\n",
      "Epoch 17/400\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 2.3612e-04 - accuracy: 0.4741 - val_loss: 0.9648 - val_accuracy: 0.4500\n",
      "Epoch 18/400\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 2.4411e-04 - accuracy: 0.4284 - val_loss: 0.9787 - val_accuracy: 0.4500\n",
      "Epoch 19/400\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 1.7748e-04 - accuracy: 0.4451 - val_loss: 0.9795 - val_accuracy: 0.4583\n",
      "Epoch 20/400\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 1.6843e-04 - accuracy: 0.4600 - val_loss: 0.9891 - val_accuracy: 0.4417\n",
      "Epoch 21/400\n",
      "16/16 [==============================] - 3s 174ms/step - loss: 1.8782e-04 - accuracy: 0.4526 - val_loss: 1.0003 - val_accuracy: 0.4417\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84332f9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84332f9a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3811507936507935 0.5755555555555555 0.31388888888888894 0.7566666666666664 0.4362934362934363\n",
      "Epoch 1/400\n",
      "32/32 [==============================] - 9s 187ms/step - loss: 0.3932 - accuracy: 0.4923 - val_loss: 0.6480 - val_accuracy: 0.4083\n",
      "Epoch 2/400\n",
      "32/32 [==============================] - 5s 170ms/step - loss: 0.0440 - accuracy: 0.5361 - val_loss: 0.7148 - val_accuracy: 0.4583\n",
      "Epoch 3/400\n",
      "32/32 [==============================] - 5s 167ms/step - loss: 0.0141 - accuracy: 0.5158 - val_loss: 0.7506 - val_accuracy: 0.4583\n",
      "Epoch 4/400\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 0.0039 - accuracy: 0.4933 - val_loss: 0.7684 - val_accuracy: 0.4333\n",
      "Epoch 5/400\n",
      "32/32 [==============================] - 5s 167ms/step - loss: 0.0019 - accuracy: 0.4655 - val_loss: 0.8594 - val_accuracy: 0.4250\n",
      "Epoch 6/400\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 0.0011 - accuracy: 0.5033 - val_loss: 0.8491 - val_accuracy: 0.4333\n",
      "Epoch 7/400\n",
      "32/32 [==============================] - 5s 167ms/step - loss: 5.7381e-04 - accuracy: 0.4295 - val_loss: 0.8728 - val_accuracy: 0.4333\n",
      "Epoch 8/400\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 4.5501e-04 - accuracy: 0.4594 - val_loss: 0.9046 - val_accuracy: 0.3917\n",
      "Epoch 9/400\n",
      "32/32 [==============================] - 5s 167ms/step - loss: 3.2860e-04 - accuracy: 0.4491 - val_loss: 0.8844 - val_accuracy: 0.4500\n",
      "Epoch 10/400\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 3.6885e-04 - accuracy: 0.4646 - val_loss: 0.8870 - val_accuracy: 0.4167\n",
      "Epoch 11/400\n",
      "32/32 [==============================] - 5s 167ms/step - loss: 1.9458e-04 - accuracy: 0.4372 - val_loss: 0.9059 - val_accuracy: 0.4417\n",
      "Epoch 12/400\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 1.4279e-04 - accuracy: 0.4690 - val_loss: 0.9243 - val_accuracy: 0.4167\n",
      "Epoch 13/400\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 1.1999e-04 - accuracy: 0.4471 - val_loss: 0.9346 - val_accuracy: 0.4583\n",
      "Epoch 14/400\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 1.0391e-04 - accuracy: 0.4779 - val_loss: 0.9420 - val_accuracy: 0.4250\n",
      "Epoch 15/400\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 1.1220e-04 - accuracy: 0.4571 - val_loss: 0.9587 - val_accuracy: 0.4167\n",
      "Epoch 16/400\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 8.7218e-05 - accuracy: 0.4264 - val_loss: 0.9603 - val_accuracy: 0.4333\n",
      "Epoch 17/400\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 8.5470e-05 - accuracy: 0.4178 - val_loss: 0.9660 - val_accuracy: 0.4083\n",
      "Epoch 18/400\n",
      "32/32 [==============================] - 5s 171ms/step - loss: 6.3402e-05 - accuracy: 0.4236 - val_loss: 0.9860 - val_accuracy: 0.3917\n",
      "Epoch 19/400\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 6.2729e-05 - accuracy: 0.4378 - val_loss: 0.9918 - val_accuracy: 0.4083\n",
      "Epoch 20/400\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 5.1232e-05 - accuracy: 0.4140 - val_loss: 1.0095 - val_accuracy: 0.4083\n",
      "Epoch 21/400\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 5.2670e-05 - accuracy: 0.4785 - val_loss: 1.0100 - val_accuracy: 0.4583\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8432e23a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8432e23a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5403174603174598 0.7493055555555557 0.4555555555555556 0.7866666666666666 0.5616438356164383\n",
      "Epoch 1/400\n",
      "40/40 [==============================] - 11s 183ms/step - loss: 0.4398 - accuracy: 0.4640 - val_loss: 0.6814 - val_accuracy: 0.4750\n",
      "Epoch 2/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 0.0479 - accuracy: 0.5502 - val_loss: 0.7389 - val_accuracy: 0.5167\n",
      "Epoch 3/400\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 0.0084 - accuracy: 0.5620 - val_loss: 0.7918 - val_accuracy: 0.4417\n",
      "Epoch 4/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 0.0040 - accuracy: 0.5601 - val_loss: 0.8009 - val_accuracy: 0.4917\n",
      "Epoch 5/400\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 0.0022 - accuracy: 0.5528 - val_loss: 0.8414 - val_accuracy: 0.4417\n",
      "Epoch 6/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 9.4445e-04 - accuracy: 0.5241 - val_loss: 0.8928 - val_accuracy: 0.4417\n",
      "Epoch 7/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 7.9739e-04 - accuracy: 0.5486 - val_loss: 0.8891 - val_accuracy: 0.5167\n",
      "Epoch 8/400\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 3.3904e-04 - accuracy: 0.5669 - val_loss: 0.9109 - val_accuracy: 0.4583\n",
      "Epoch 9/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 2.3336e-04 - accuracy: 0.4845 - val_loss: 0.9242 - val_accuracy: 0.4750\n",
      "Epoch 10/400\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 1.4451e-04 - accuracy: 0.4560 - val_loss: 0.9511 - val_accuracy: 0.4750\n",
      "Epoch 11/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 1.2826e-04 - accuracy: 0.5035 - val_loss: 0.9675 - val_accuracy: 0.4750\n",
      "Epoch 12/400\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 1.1117e-04 - accuracy: 0.5239 - val_loss: 0.9650 - val_accuracy: 0.4667\n",
      "Epoch 13/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 9.8089e-05 - accuracy: 0.4506 - val_loss: 0.9813 - val_accuracy: 0.4667\n",
      "Epoch 14/400\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 7.5267e-05 - accuracy: 0.4985 - val_loss: 0.9944 - val_accuracy: 0.4667\n",
      "Epoch 15/400\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 6.3960e-05 - accuracy: 0.4763 - val_loss: 1.0098 - val_accuracy: 0.4667\n",
      "Epoch 16/400\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 5.4592e-05 - accuracy: 0.4663 - val_loss: 1.0173 - val_accuracy: 0.4667\n",
      "Epoch 17/400\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 5.8788e-05 - accuracy: 0.4863 - val_loss: 1.0285 - val_accuracy: 0.4583\n",
      "Epoch 18/400\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 4.7134e-05 - accuracy: 0.5162 - val_loss: 1.0314 - val_accuracy: 0.4500\n",
      "Epoch 19/400\n",
      "40/40 [==============================] - 7s 167ms/step - loss: 3.9867e-05 - accuracy: 0.4757 - val_loss: 1.0466 - val_accuracy: 0.4500\n",
      "Epoch 20/400\n",
      "40/40 [==============================] - 7s 168ms/step - loss: 9.2880e-05 - accuracy: 0.4679 - val_loss: 0.9974 - val_accuracy: 0.5000\n",
      "Epoch 21/400\n",
      "40/40 [==============================] - 7s 169ms/step - loss: 6.6458e-05 - accuracy: 0.5424 - val_loss: 1.0292 - val_accuracy: 0.4667\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84319e4820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f84319e4820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4830952380952377 0.6979166666666666 0.40000000000000013 0.7766666666666662 0.5179856115107914\n",
      "Epoch 1/400\n",
      "47/47 [==============================] - 12s 182ms/step - loss: 0.3637 - accuracy: 0.5123 - val_loss: 0.7676 - val_accuracy: 0.4667\n",
      "Epoch 2/400\n",
      "47/47 [==============================] - 8s 170ms/step - loss: 0.0201 - accuracy: 0.5533 - val_loss: 0.8658 - val_accuracy: 0.3750\n",
      "Epoch 3/400\n",
      "47/47 [==============================] - 8s 171ms/step - loss: 0.0060 - accuracy: 0.4419 - val_loss: 0.9548 - val_accuracy: 0.4083\n",
      "Epoch 4/400\n",
      "47/47 [==============================] - 8s 170ms/step - loss: 0.0029 - accuracy: 0.4377 - val_loss: 0.9609 - val_accuracy: 0.4417\n",
      "Epoch 5/400\n",
      "47/47 [==============================] - 8s 170ms/step - loss: 5.4969e-04 - accuracy: 0.5180 - val_loss: 1.0082 - val_accuracy: 0.3750\n",
      "Epoch 6/400\n",
      "47/47 [==============================] - 8s 171ms/step - loss: 7.4355e-04 - accuracy: 0.4766 - val_loss: 1.0331 - val_accuracy: 0.3417\n",
      "Epoch 7/400\n",
      "47/47 [==============================] - 8s 171ms/step - loss: 3.1785e-04 - accuracy: 0.4066 - val_loss: 1.0160 - val_accuracy: 0.4000\n",
      "Epoch 8/400\n",
      "47/47 [==============================] - 8s 173ms/step - loss: 2.8654e-04 - accuracy: 0.4392 - val_loss: 1.0332 - val_accuracy: 0.3750\n",
      "Epoch 9/400\n",
      "47/47 [==============================] - 8s 170ms/step - loss: 1.0687e-04 - accuracy: 0.4500 - val_loss: 1.0732 - val_accuracy: 0.3750\n",
      "Epoch 10/400\n",
      "47/47 [==============================] - 8s 170ms/step - loss: 7.2558e-05 - accuracy: 0.4759 - val_loss: 1.0956 - val_accuracy: 0.3750\n",
      "Epoch 11/400\n",
      "47/47 [==============================] - 8s 174ms/step - loss: 6.4841e-05 - accuracy: 0.4459 - val_loss: 1.0953 - val_accuracy: 0.3833\n",
      "Epoch 12/400\n",
      "47/47 [==============================] - 8s 171ms/step - loss: 5.3443e-05 - accuracy: 0.4207 - val_loss: 1.1118 - val_accuracy: 0.3833\n",
      "Epoch 13/400\n",
      "47/47 [==============================] - 8s 169ms/step - loss: 4.4088e-05 - accuracy: 0.4729 - val_loss: 1.1225 - val_accuracy: 0.3750\n",
      "Epoch 14/400\n",
      "47/47 [==============================] - 8s 167ms/step - loss: 3.9991e-05 - accuracy: 0.4170 - val_loss: 1.1327 - val_accuracy: 0.3833\n",
      "Epoch 15/400\n",
      "47/47 [==============================] - 8s 170ms/step - loss: 6.2531e-05 - accuracy: 0.3913 - val_loss: 1.1249 - val_accuracy: 0.3667\n",
      "Epoch 16/400\n",
      "47/47 [==============================] - 8s 169ms/step - loss: 2.8574e-05 - accuracy: 0.4549 - val_loss: 1.1392 - val_accuracy: 0.3750\n",
      "Epoch 17/400\n",
      "47/47 [==============================] - 8s 169ms/step - loss: 3.0049e-05 - accuracy: 0.4325 - val_loss: 1.1519 - val_accuracy: 0.3833\n",
      "Epoch 18/400\n",
      "47/47 [==============================] - 8s 170ms/step - loss: 2.4615e-05 - accuracy: 0.4722 - val_loss: 1.1584 - val_accuracy: 0.3750\n",
      "Epoch 19/400\n",
      "47/47 [==============================] - 8s 168ms/step - loss: 2.7842e-05 - accuracy: 0.4275 - val_loss: 1.1695 - val_accuracy: 0.3583\n",
      "Epoch 20/400\n",
      "47/47 [==============================] - 8s 169ms/step - loss: 2.3447e-05 - accuracy: 0.4101 - val_loss: 1.1746 - val_accuracy: 0.3750\n",
      "Epoch 21/400\n",
      "47/47 [==============================] - 8s 170ms/step - loss: 1.9392e-05 - accuracy: 0.4898 - val_loss: 1.1924 - val_accuracy: 0.3833\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f843281ec10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f843281ec10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4994047619047614 0.7270833333333335 0.40833333333333344 0.7825000000000003 0.5297297297297298\n",
      "Epoch 1/400\n",
      "79/79 [==============================] - 18s 175ms/step - loss: 0.2394 - accuracy: 0.4949 - val_loss: 0.6905 - val_accuracy: 0.3750\n",
      "Epoch 2/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 0.0075 - accuracy: 0.4607 - val_loss: 0.9627 - val_accuracy: 0.5083\n",
      "Epoch 3/400\n",
      "79/79 [==============================] - 13s 167ms/step - loss: 0.0192 - accuracy: 0.4789 - val_loss: 0.7517 - val_accuracy: 0.4667\n",
      "Epoch 4/400\n",
      "79/79 [==============================] - 13s 165ms/step - loss: 0.0017 - accuracy: 0.4536 - val_loss: 0.7874 - val_accuracy: 0.4583\n",
      "Epoch 5/400\n",
      "79/79 [==============================] - 13s 169ms/step - loss: 0.0011 - accuracy: 0.4790 - val_loss: 0.8733 - val_accuracy: 0.4000\n",
      "Epoch 6/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 3.5782e-04 - accuracy: 0.4546 - val_loss: 0.8375 - val_accuracy: 0.4333\n",
      "Epoch 7/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 3.4183e-04 - accuracy: 0.4767 - val_loss: 0.8531 - val_accuracy: 0.4333\n",
      "Epoch 8/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 8.0403e-05 - accuracy: 0.4369 - val_loss: 0.8736 - val_accuracy: 0.4167\n",
      "Epoch 9/400\n",
      "79/79 [==============================] - 13s 170ms/step - loss: 3.7713e-05 - accuracy: 0.4645 - val_loss: 0.8746 - val_accuracy: 0.4167\n",
      "Epoch 10/400\n",
      "79/79 [==============================] - 13s 166ms/step - loss: 3.4645e-05 - accuracy: 0.4204 - val_loss: 0.8964 - val_accuracy: 0.4500\n",
      "Epoch 11/400\n",
      "79/79 [==============================] - 13s 167ms/step - loss: 3.1063e-05 - accuracy: 0.4525 - val_loss: 0.9004 - val_accuracy: 0.4250\n",
      "Epoch 12/400\n",
      "79/79 [==============================] - 13s 169ms/step - loss: 2.3193e-05 - accuracy: 0.4415 - val_loss: 0.9032 - val_accuracy: 0.4500\n",
      "Epoch 13/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 1.9241e-05 - accuracy: 0.4548 - val_loss: 0.9193 - val_accuracy: 0.4083\n",
      "Epoch 14/400\n",
      "79/79 [==============================] - 13s 167ms/step - loss: 2.1867e-05 - accuracy: 0.4501 - val_loss: 0.9266 - val_accuracy: 0.4083\n",
      "Epoch 15/400\n",
      "79/79 [==============================] - 13s 167ms/step - loss: 1.6824e-05 - accuracy: 0.4169 - val_loss: 0.9405 - val_accuracy: 0.4500\n",
      "Epoch 16/400\n",
      "79/79 [==============================] - 13s 167ms/step - loss: 1.9075e-05 - accuracy: 0.4912 - val_loss: 0.9469 - val_accuracy: 0.4167\n",
      "Epoch 17/400\n",
      "79/79 [==============================] - 13s 165ms/step - loss: 1.5890e-05 - accuracy: 0.4363 - val_loss: 0.9601 - val_accuracy: 0.4500\n",
      "Epoch 18/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 1.0895e-05 - accuracy: 0.4473 - val_loss: 0.9733 - val_accuracy: 0.4250\n",
      "Epoch 19/400\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 1.1712e-05 - accuracy: 0.3873 - val_loss: 0.9701 - val_accuracy: 0.4333\n",
      "Epoch 20/400\n",
      "79/79 [==============================] - 13s 166ms/step - loss: 7.5979e-06 - accuracy: 0.4309 - val_loss: 0.9732 - val_accuracy: 0.4417\n",
      "Epoch 21/400\n",
      "79/79 [==============================] - 13s 169ms/step - loss: 8.8669e-06 - accuracy: 0.4586 - val_loss: 0.9775 - val_accuracy: 0.4250\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8585ae7820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8585ae7820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6009920634920631 0.7819444444444446 0.5277777777777778 0.8058333333333333 0.6199021207177814\n",
      "Epoch 1/400\n",
      "157/157 [==============================] - 31s 171ms/step - loss: 0.1587 - accuracy: 0.4765 - val_loss: 0.7980 - val_accuracy: 0.4250\n",
      "Epoch 2/400\n",
      "157/157 [==============================] - 26s 168ms/step - loss: 0.0060 - accuracy: 0.4385 - val_loss: 0.7499 - val_accuracy: 0.4917\n",
      "Epoch 3/400\n",
      "157/157 [==============================] - 26s 167ms/step - loss: 0.0010 - accuracy: 0.4125 - val_loss: 0.8614 - val_accuracy: 0.3917\n",
      "Epoch 4/400\n",
      "157/157 [==============================] - 26s 168ms/step - loss: 6.0149e-04 - accuracy: 0.3990 - val_loss: 0.8728 - val_accuracy: 0.4500\n",
      "Epoch 5/400\n",
      "157/157 [==============================] - 26s 169ms/step - loss: 3.1889e-04 - accuracy: 0.4377 - val_loss: 0.8867 - val_accuracy: 0.3917\n",
      "Epoch 6/400\n",
      "157/157 [==============================] - 26s 169ms/step - loss: 6.6563e-04 - accuracy: 0.3926 - val_loss: 0.9415 - val_accuracy: 0.3583\n",
      "Epoch 7/400\n",
      "157/157 [==============================] - 27s 173ms/step - loss: 0.0227 - accuracy: 0.4036 - val_loss: 1.0625 - val_accuracy: 0.3917\n",
      "Epoch 8/400\n",
      "157/157 [==============================] - 26s 169ms/step - loss: 0.0016 - accuracy: 0.4377 - val_loss: 0.9076 - val_accuracy: 0.3583\n",
      "Epoch 9/400\n",
      "157/157 [==============================] - 27s 169ms/step - loss: 1.3338e-04 - accuracy: 0.3080 - val_loss: 0.8982 - val_accuracy: 0.3917\n",
      "Epoch 10/400\n",
      "157/157 [==============================] - 27s 169ms/step - loss: 2.6694e-05 - accuracy: 0.3489 - val_loss: 0.9464 - val_accuracy: 0.3750\n",
      "Epoch 11/400\n",
      "157/157 [==============================] - 26s 168ms/step - loss: 1.1783e-05 - accuracy: 0.3220 - val_loss: 0.9476 - val_accuracy: 0.3917\n",
      "Epoch 12/400\n",
      "157/157 [==============================] - 27s 169ms/step - loss: 8.5945e-06 - accuracy: 0.3247 - val_loss: 0.9635 - val_accuracy: 0.3833\n",
      "Epoch 13/400\n",
      "157/157 [==============================] - 26s 167ms/step - loss: 6.9454e-06 - accuracy: 0.3406 - val_loss: 0.9676 - val_accuracy: 0.4000\n",
      "Epoch 14/400\n",
      "157/157 [==============================] - 26s 168ms/step - loss: 6.5553e-06 - accuracy: 0.3332 - val_loss: 0.9761 - val_accuracy: 0.4083\n",
      "Epoch 15/400\n",
      "157/157 [==============================] - 27s 170ms/step - loss: 5.6830e-06 - accuracy: 0.3421 - val_loss: 0.9859 - val_accuracy: 0.3917\n",
      "Epoch 16/400\n",
      "157/157 [==============================] - 26s 168ms/step - loss: 5.5392e-06 - accuracy: 0.3578 - val_loss: 0.9982 - val_accuracy: 0.4083\n",
      "Epoch 17/400\n",
      "157/157 [==============================] - 27s 169ms/step - loss: 5.4206e-06 - accuracy: 0.3457 - val_loss: 1.0068 - val_accuracy: 0.4167\n",
      "Epoch 18/400\n",
      "157/157 [==============================] - 27s 169ms/step - loss: 4.1292e-06 - accuracy: 0.3720 - val_loss: 1.0158 - val_accuracy: 0.3917\n",
      "Epoch 19/400\n",
      "157/157 [==============================] - 26s 168ms/step - loss: 3.2996e-06 - accuracy: 0.3502 - val_loss: 1.0213 - val_accuracy: 0.4000\n",
      "Epoch 20/400\n",
      "157/157 [==============================] - 27s 169ms/step - loss: 3.0393e-06 - accuracy: 0.3442 - val_loss: 1.0281 - val_accuracy: 0.4083\n",
      "Epoch 21/400\n",
      "157/157 [==============================] - 27s 169ms/step - loss: 2.7291e-06 - accuracy: 0.3603 - val_loss: 1.0341 - val_accuracy: 0.4000\n",
      "Epoch 22/400\n",
      "157/157 [==============================] - 26s 168ms/step - loss: 2.5448e-06 - accuracy: 0.3511 - val_loss: 1.0418 - val_accuracy: 0.3917\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f843041d3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f843041d3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6337301587301584 0.7498611111111114 0.5805555555555556 0.8091666666666668 0.6460587326120556\n"
     ]
    }
   ],
   "source": [
    "random.seed(1000)\n",
    "biased_ratio = 64\n",
    "num_of_letters = 10\n",
    "array = [500,1000,2000,2500,3000,5000,10000]\n",
    "f1scoreVal = []\n",
    "precVal = []\n",
    "recaVal = []\n",
    "TestDataset = create_dataset(testImagesVal, testLabelsVal, False)\n",
    "for i in range(len(array)):\n",
    "    num_of_train_images = array[i]\n",
    "\n",
    "    trainImagesComb, trainLabelsComb = createOddEvenNew(trainImagesVal, trainLabelsVal)\n",
    "    trainImages, trainLabels = incNumOfImages(trainImagesComb, trainLabelsComb, num_of_train_images)\n",
    "    trainLabels = convertToArray(trainLabels)\n",
    "    TrainDataset = create_dataset(trainImages, trainLabels)\n",
    "\n",
    "    model = modelLatest()\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, min_delta=0.0001, restore_best_weights=True)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    history = model.fit(TrainDataset, epochs=400, validation_data=TestDataset, callbacks=[callback], batch_size = batch_size)\n",
    "\n",
    "    ypred = model.predict(TestDataset)\n",
    "    #acc , combList, correctness = accuracyFunc(ypred, testLabelsVal, array)\n",
    "\n",
    "    temp_f1 = 0\n",
    "    temp_recall = 0\n",
    "    temp_precision = 0\n",
    "    temp_acc = 0\n",
    "    f1Val = []\n",
    "    distVal = []\n",
    "    y_test = testLabelsVal\n",
    "    y_predicted = calculate_labels(ypred)\n",
    "    for i in range(len(y_test)):\n",
    "        temp_f1 = temp_f1 + f1_score(y_test[i], y_predicted[i])\n",
    "        temp_recall = temp_recall + recall_score(y_test[i], y_predicted[i])\n",
    "        temp_precision = temp_precision + precision_score(y_test[i], y_predicted[i])\n",
    "        temp_acc = temp_acc + accuracy_score(y_test[i], y_predicted[i])\n",
    "        f1Val.append([f1_score(y_test[i], y_predicted[i])])\n",
    "        distVal.append([f1_score(y_test[i], y_predicted[i])])\n",
    "    f1score = temp_f1 / len(y_test)\n",
    "    precision = temp_precision / len(y_test)\n",
    "    recall = temp_recall / len(y_test)\n",
    "    acc = temp_acc / len(y_test)\n",
    "    f1scoreVal.append(f1score)\n",
    "    precVal.append(precision)\n",
    "    recaVal.append(recall)\n",
    "    print(f1score, precision, recall, acc, f1_score(y_test, y_predicted, average='micro'))\n",
    "\n",
    "#arrayVal = correlationmt(trainLabelsComb)\n",
    "#array = calculateDist(arrayVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odd+even\n",
      "[0.3419047619047618, 0.3811507936507935, 0.5403174603174598, 0.4830952380952377, 0.4994047619047614, 0.6009920634920631, 0.6337301587301584]\n",
      "[0.52875, 0.5755555555555555, 0.7493055555555557, 0.6979166666666666, 0.7270833333333335, 0.7819444444444446, 0.7498611111111114]\n",
      "[0.2749999999999998, 0.31388888888888894, 0.4555555555555556, 0.40000000000000013, 0.40833333333333344, 0.5277777777777778, 0.5805555555555556]\n"
     ]
    }
   ],
   "source": [
    "print(\"odd+even\")\n",
    "print(f1scoreVal)\n",
    "print(precVal)\n",
    "print(recaVal)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "random.seed(1000)\n",
    "biased_ratio = 32\n",
    "num_of_letters = 10\n",
    "array = [500,1000,2000,2500,3000,5000,10000]\n",
    "f1scoreVal = []\n",
    "precVal = []\n",
    "recaVal = []\n",
    "TestDataset = create_dataset(testImagesVal, testLabelsVal, False)\n",
    "for i in range(len(array)):\n",
    "    num_of_train_images = array[i]\n",
    "\n",
    "    trainImagesComb, trainLabelsComb = createRatioBased(biased_ratio, trainImagesVal, trainLabelsVal)\n",
    "    trainImages, trainLabels = incNumOfImages(trainImagesComb, trainLabelsComb, num_of_train_images)\n",
    "    trainLabels = convertToArray(trainLabels)\n",
    "    TrainDataset = create_dataset(trainImages, trainLabels)\n",
    "\n",
    "    model = modelLatest()\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, min_delta=0.0001, restore_best_weights=True)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    history = model.fit(TrainDataset, epochs=400, validation_data=TestDataset, callbacks=[callback], batch_size = batch_size)\n",
    "\n",
    "    ypred = model.predict(TestDataset)\n",
    "    #acc , combList, correctness = accuracyFunc(ypred, testLabelsVal, array)\n",
    "\n",
    "    temp_f1 = 0\n",
    "    temp_recall = 0\n",
    "    temp_precision = 0\n",
    "    temp_acc = 0\n",
    "    f1Val = []\n",
    "    distVal = []\n",
    "    y_test = testLabelsVal\n",
    "    y_predicted = calculate_labels(ypred)\n",
    "    for i in range(len(y_test)):\n",
    "        temp_f1 = temp_f1 + f1_score(y_test[i], y_predicted[i])\n",
    "        temp_recall = temp_recall + recall_score(y_test[i], y_predicted[i])\n",
    "        temp_precision = temp_precision + precision_score(y_test[i], y_predicted[i])\n",
    "        temp_acc = temp_acc + accuracy_score(y_test[i], y_predicted[i])\n",
    "        f1Val.append([f1_score(y_test[i], y_predicted[i])])\n",
    "        distVal.append([f1_score(y_test[i], y_predicted[i])])\n",
    "    f1score = temp_f1 / len(y_test)\n",
    "    precision = temp_precision / len(y_test)\n",
    "    recall = temp_recall / len(y_test)\n",
    "    acc = temp_acc / len(y_test)\n",
    "    f1scoreVal.append(f1score)\n",
    "    precVal.append(precision)\n",
    "    recaVal.append(recall)\n",
    "    print(f1score, precision, recall, acc, f1_score(y_test, y_predicted, average='micro'))\n",
    "\n",
    "#arrayVal = correlationmt(trainLabelsComb)\n",
    "#array = calculateDist(arrayVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "[0.3446031746031743, 0.4317857142857141, 0.4592857142857142, 0.41948412698412685, 0.4803174603174602, 0.4592063492063492, 0.4536507936507936]\n",
      "[0.44930555555555546, 0.4625000000000001, 0.48958333333333337, 0.43263888888888885, 0.49513888888888896, 0.4694444444444445, 0.476388888888889]\n",
      "[0.2972222222222221, 0.4166666666666668, 0.44722222222222235, 0.41388888888888903, 0.4722222222222224, 0.45277777777777783, 0.4388888888888891]\n"
     ]
    }
   ],
   "source": [
    "print(\"32\")\n",
    "print(f1scoreVal)\n",
    "print(precVal)\n",
    "print(recaVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "Epoch 1/400\n",
      "94/94 [==============================] - 22s 192ms/step - loss: 0.3480 - accuracy: 0.3686 - val_loss: 0.4521 - val_accuracy: 0.4400\n",
      "Epoch 2/400\n",
      "94/94 [==============================] - 18s 189ms/step - loss: 0.0546 - accuracy: 0.4468 - val_loss: 0.4478 - val_accuracy: 0.3983\n",
      "Epoch 3/400\n",
      "94/94 [==============================] - 18s 190ms/step - loss: 0.0269 - accuracy: 0.3858 - val_loss: 0.4702 - val_accuracy: 0.3667\n",
      "Epoch 4/400\n",
      "94/94 [==============================] - 17s 183ms/step - loss: 0.0135 - accuracy: 0.3685 - val_loss: 0.4342 - val_accuracy: 0.4850\n",
      "Epoch 5/400\n",
      "94/94 [==============================] - 17s 185ms/step - loss: 0.0110 - accuracy: 0.3459 - val_loss: 0.5277 - val_accuracy: 0.4267\n",
      "Epoch 6/400\n",
      "94/94 [==============================] - 17s 180ms/step - loss: 0.0064 - accuracy: 0.3645 - val_loss: 0.4651 - val_accuracy: 0.4133\n",
      "Epoch 7/400\n",
      "94/94 [==============================] - 17s 180ms/step - loss: 0.0018 - accuracy: 0.4129 - val_loss: 0.5032 - val_accuracy: 0.4017\n",
      "Epoch 8/400\n",
      "94/94 [==============================] - 17s 180ms/step - loss: 0.0024 - accuracy: 0.3845 - val_loss: 0.4402 - val_accuracy: 0.4067\n",
      "Epoch 9/400\n",
      "94/94 [==============================] - 17s 180ms/step - loss: 0.0021 - accuracy: 0.3848 - val_loss: 0.4827 - val_accuracy: 0.3950\n",
      "Epoch 10/400\n",
      "94/94 [==============================] - 17s 181ms/step - loss: 0.0014 - accuracy: 0.3882 - val_loss: 0.5874 - val_accuracy: 0.3367\n",
      "Epoch 11/400\n",
      "94/94 [==============================] - 17s 184ms/step - loss: 0.0079 - accuracy: 0.3325 - val_loss: 0.4898 - val_accuracy: 0.3233\n",
      "Epoch 12/400\n",
      "94/94 [==============================] - 17s 180ms/step - loss: 0.0033 - accuracy: 0.2814 - val_loss: 0.4652 - val_accuracy: 0.4033\n",
      "Epoch 13/400\n",
      "94/94 [==============================] - 17s 185ms/step - loss: 5.9991e-04 - accuracy: 0.3078 - val_loss: 0.4933 - val_accuracy: 0.2917\n",
      "Epoch 14/400\n",
      "94/94 [==============================] - 17s 185ms/step - loss: 2.5624e-04 - accuracy: 0.2868 - val_loss: 0.5080 - val_accuracy: 0.3367\n",
      "0.7918650793650703 0.8738888888888859 0.7455555555555501 0.8871666666666596 0.7985718536149956\n",
      "7000\n",
      "Epoch 1/400\n",
      "110/110 [==============================] - 26s 191ms/step - loss: 0.3595 - accuracy: 0.4112 - val_loss: 0.4575 - val_accuracy: 0.3717\n",
      "Epoch 2/400\n",
      "110/110 [==============================] - 20s 184ms/step - loss: 0.0641 - accuracy: 0.4016 - val_loss: 0.4335 - val_accuracy: 0.3433\n",
      "Epoch 3/400\n",
      "110/110 [==============================] - 20s 184ms/step - loss: 0.0286 - accuracy: 0.4067 - val_loss: 0.4205 - val_accuracy: 0.3150\n",
      "Epoch 4/400\n",
      "110/110 [==============================] - 20s 184ms/step - loss: 0.0211 - accuracy: 0.3851 - val_loss: 0.3921 - val_accuracy: 0.3133\n",
      "Epoch 5/400\n",
      "110/110 [==============================] - 20s 184ms/step - loss: 0.0099 - accuracy: 0.3683 - val_loss: 0.4302 - val_accuracy: 0.2867\n",
      "Epoch 6/400\n",
      "110/110 [==============================] - 20s 185ms/step - loss: 0.0070 - accuracy: 0.3439 - val_loss: 0.3814 - val_accuracy: 0.2850\n",
      "Epoch 7/400\n",
      "110/110 [==============================] - 20s 186ms/step - loss: 0.0025 - accuracy: 0.3099 - val_loss: 0.4189 - val_accuracy: 0.3083\n",
      "Epoch 8/400\n",
      "110/110 [==============================] - 20s 183ms/step - loss: 0.0014 - accuracy: 0.3427 - val_loss: 0.4738 - val_accuracy: 0.2600\n",
      "Epoch 9/400\n",
      "110/110 [==============================] - 20s 184ms/step - loss: 0.0011 - accuracy: 0.3469 - val_loss: 0.4875 - val_accuracy: 0.2533\n",
      "Epoch 10/400\n",
      "110/110 [==============================] - 20s 184ms/step - loss: 0.0066 - accuracy: 0.3247 - val_loss: 0.4468 - val_accuracy: 0.3000\n",
      "Epoch 11/400\n",
      "110/110 [==============================] - 20s 184ms/step - loss: 0.0017 - accuracy: 0.3403 - val_loss: 0.5832 - val_accuracy: 0.1700\n",
      "Epoch 12/400\n",
      "110/110 [==============================] - 20s 181ms/step - loss: 8.1562e-04 - accuracy: 0.3385 - val_loss: 0.4324 - val_accuracy: 0.2467\n",
      "Epoch 13/400\n",
      "110/110 [==============================] - 20s 183ms/step - loss: 2.6637e-04 - accuracy: 0.3204 - val_loss: 0.4542 - val_accuracy: 0.2633\n",
      "Epoch 14/400\n",
      "110/110 [==============================] - 20s 184ms/step - loss: 1.2809e-04 - accuracy: 0.3720 - val_loss: 0.4540 - val_accuracy: 0.2817\n",
      "Epoch 15/400\n",
      "110/110 [==============================] - 20s 184ms/step - loss: 8.0474e-05 - accuracy: 0.3199 - val_loss: 0.4595 - val_accuracy: 0.2450\n",
      "Epoch 16/400\n",
      "110/110 [==============================] - 20s 181ms/step - loss: 4.7199e-05 - accuracy: 0.3294 - val_loss: 0.4480 - val_accuracy: 0.2500\n",
      "0.8321428571428494 0.9112499999999987 0.7877777777777724 0.909999999999993 0.8400473933649288\n",
      "10000\n",
      "Epoch 1/400\n",
      "157/157 [==============================] - 34s 186ms/step - loss: 0.3384 - accuracy: 0.3955 - val_loss: 0.4284 - val_accuracy: 0.3867\n",
      "Epoch 2/400\n",
      "157/157 [==============================] - 29s 182ms/step - loss: 0.0564 - accuracy: 0.4177 - val_loss: 0.4734 - val_accuracy: 0.3500\n",
      "Epoch 3/400\n",
      "157/157 [==============================] - 29s 184ms/step - loss: 0.0356 - accuracy: 0.4081 - val_loss: 0.4144 - val_accuracy: 0.4317\n",
      "Epoch 4/400\n",
      "157/157 [==============================] - 28s 178ms/step - loss: 0.0120 - accuracy: 0.3764 - val_loss: 0.4109 - val_accuracy: 0.3650\n",
      "Epoch 5/400\n",
      "157/157 [==============================] - 28s 179ms/step - loss: 0.0077 - accuracy: 0.4070 - val_loss: 0.5439 - val_accuracy: 0.3400\n",
      "Epoch 6/400\n",
      "157/157 [==============================] - 28s 180ms/step - loss: 0.0057 - accuracy: 0.3823 - val_loss: 0.5502 - val_accuracy: 0.3383\n",
      "Epoch 7/400\n",
      "157/157 [==============================] - 28s 178ms/step - loss: 0.0046 - accuracy: 0.3650 - val_loss: 0.4645 - val_accuracy: 0.3717\n",
      "Epoch 8/400\n",
      "157/157 [==============================] - 28s 177ms/step - loss: 0.0011 - accuracy: 0.3586 - val_loss: 0.4721 - val_accuracy: 0.3717\n",
      "Epoch 9/400\n",
      "157/157 [==============================] - 28s 180ms/step - loss: 0.0051 - accuracy: 0.3413 - val_loss: 0.5656 - val_accuracy: 0.2650\n",
      "Epoch 10/400\n",
      "157/157 [==============================] - 28s 180ms/step - loss: 0.0048 - accuracy: 0.3751 - val_loss: 0.4804 - val_accuracy: 0.3800\n",
      "Epoch 11/400\n",
      "157/157 [==============================] - 28s 180ms/step - loss: 0.0014 - accuracy: 0.3934 - val_loss: 0.4894 - val_accuracy: 0.3467\n",
      "Epoch 12/400\n",
      "157/157 [==============================] - 28s 176ms/step - loss: 7.3660e-04 - accuracy: 0.3230 - val_loss: 0.5032 - val_accuracy: 0.3433\n",
      "Epoch 13/400\n",
      "157/157 [==============================] - 28s 176ms/step - loss: 5.3150e-04 - accuracy: 0.3207 - val_loss: 0.4604 - val_accuracy: 0.4400\n",
      "Epoch 14/400\n",
      "157/157 [==============================] - 28s 177ms/step - loss: 0.0044 - accuracy: 0.4121 - val_loss: 0.4514 - val_accuracy: 0.3150\n",
      "0.8202817460317381 0.8703055555555522 0.7927777777777719 0.8996666666666586 0.8258101851851852\n",
      "20000\n",
      "Epoch 1/400\n",
      "313/313 [==============================] - 60s 178ms/step - loss: 0.2822 - accuracy: 0.4919 - val_loss: 0.3838 - val_accuracy: 0.5233\n",
      "Epoch 2/400\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 0.0295 - accuracy: 0.4201 - val_loss: 0.3027 - val_accuracy: 0.5400\n",
      "Epoch 3/400\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 0.0130 - accuracy: 0.4360 - val_loss: 0.4363 - val_accuracy: 0.4833\n",
      "Epoch 4/400\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 0.0090 - accuracy: 0.4056 - val_loss: 0.3982 - val_accuracy: 0.5550\n",
      "Epoch 5/400\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 0.0060 - accuracy: 0.4141 - val_loss: 0.3942 - val_accuracy: 0.4683\n",
      "Epoch 6/400\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 0.0052 - accuracy: 0.4175 - val_loss: 0.4222 - val_accuracy: 0.4750\n",
      "Epoch 7/400\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 0.0036 - accuracy: 0.3942 - val_loss: 0.3716 - val_accuracy: 0.5117\n",
      "Epoch 8/400\n",
      "313/313 [==============================] - 56s 178ms/step - loss: 0.0050 - accuracy: 0.3871 - val_loss: 0.3865 - val_accuracy: 0.5117\n",
      "Epoch 9/400\n",
      "313/313 [==============================] - 55s 177ms/step - loss: 0.0031 - accuracy: 0.3833 - val_loss: 0.4350 - val_accuracy: 0.5267\n",
      "Epoch 10/400\n",
      "313/313 [==============================] - 56s 177ms/step - loss: 0.0018 - accuracy: 0.4109 - val_loss: 0.5414 - val_accuracy: 0.5167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/400\n",
      "313/313 [==============================] - 55s 176ms/step - loss: 0.0021 - accuracy: 0.4104 - val_loss: 0.4943 - val_accuracy: 0.4783\n",
      "Epoch 12/400\n",
      "313/313 [==============================] - 56s 179ms/step - loss: 0.0053 - accuracy: 0.3799 - val_loss: 0.4653 - val_accuracy: 0.5183\n",
      "0.8055238095238012 0.8886666666666653 0.7627777777777724 0.8969999999999935 0.8162901307966706\n"
     ]
    }
   ],
   "source": [
    "random.seed(1000)\n",
    "biased_ratio = 32\n",
    "num_of_letters = 10\n",
    "array = [1000,2000,5000,15000]\n",
    "f1scoreVal = []\n",
    "precVal = []\n",
    "recaVal = []\n",
    "TestDataset = create_dataset(testImagesVal, testLabelsVal, False)\n",
    "baseLength = 5000\n",
    "for i in range(len(array)):\n",
    "    num_of_train_images = array[i]\n",
    "\n",
    "    #trainImagesComb, trainLabelsComb = createRatioBased(biased_ratio, trainImagesVal, trainLabelsVal)\n",
    "    trainImagesComb, trainLabelsComb = createOddEvenNew(trainImagesVal, trainLabelsVal)\n",
    "    trainImages_un, trainLabels_un = incNumOfImages(trainImagesComb, trainLabelsComb, baseLength)\n",
    "    trainLabels_un = convertToArray(trainLabels_un)\n",
    "    connectedness = connectednessMeasure(trainLabels_un, testLabelsVal)\n",
    "    combList = createCombinationListRand(connectedness, 15)\n",
    "    trainLabels_un = convertToNum(trainLabels_un)\n",
    "    trainImages, trainLabels = addLessConnectedComb(combList, trainImages_un, trainLabels_un, baseLength + num_of_train_images)\n",
    "    trainLabels = convertToArray(trainLabels)\n",
    "    print(len(trainLabels))\n",
    "    TrainDataset = create_dataset(trainImages, trainLabels)\n",
    "\n",
    "    model = modelLatest()\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=0.0001, restore_best_weights=True)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    history = model.fit(TrainDataset, epochs=400, validation_data=TestDataset, callbacks=[callback], batch_size = batch_size)\n",
    "\n",
    "    ypred = model.predict(TestDataset)\n",
    "    #acc , combList, correctness = accuracyFunc(ypred, testLabelsVal, array)\n",
    "\n",
    "    temp_f1 = 0\n",
    "    temp_recall = 0\n",
    "    temp_precision = 0\n",
    "    temp_acc = 0\n",
    "    f1Val = []\n",
    "    distVal = []\n",
    "    y_test = testLabelsVal\n",
    "    y_predicted = calculate_labels(ypred)\n",
    "    for i in range(len(y_test)):\n",
    "        temp_f1 = temp_f1 + f1_score(y_test[i], y_predicted[i])\n",
    "        temp_recall = temp_recall + recall_score(y_test[i], y_predicted[i])\n",
    "        temp_precision = temp_precision + precision_score(y_test[i], y_predicted[i])\n",
    "        temp_acc = temp_acc + accuracy_score(y_test[i], y_predicted[i])\n",
    "        f1Val.append([f1_score(y_test[i], y_predicted[i])])\n",
    "        distVal.append([f1_score(y_test[i], y_predicted[i])])\n",
    "    f1score = temp_f1 / len(y_test)\n",
    "    precision = temp_precision / len(y_test)\n",
    "    recall = temp_recall / len(y_test)\n",
    "    acc = temp_acc / len(y_test)\n",
    "    f1scoreVal.append(f1score)\n",
    "    precVal.append(precision)\n",
    "    recaVal.append(recall)\n",
    "    print(f1score, precision, recall, acc, f1_score(y_test, y_predicted, average='micro'))\n",
    "\n",
    "#arrayVal = correlationmt(trainLabelsComb)\n",
    "#array = calculateDist(arrayVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odd+even\n",
      "[0.7918650793650703, 0.8321428571428494, 0.8202817460317381, 0.8055238095238012]\n",
      "[0.8738888888888859, 0.9112499999999987, 0.8703055555555522, 0.8886666666666653]\n",
      "[0.7455555555555501, 0.7877777777777724, 0.7927777777777719, 0.7627777777777724]\n"
     ]
    }
   ],
   "source": [
    "print(\"odd+even\")\n",
    "print(f1scoreVal)\n",
    "print(precVal)\n",
    "print(recaVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(convertToNum(testLabelsVal)[120:240])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "157/157 [==============================] - 31s 177ms/step - loss: 0.2593 - accuracy: 0.4185 - val_loss: 0.7472 - val_accuracy: 0.3500\n",
      "Epoch 2/400\n",
      "157/157 [==============================] - 27s 175ms/step - loss: 0.0038 - accuracy: 0.4091 - val_loss: 0.9166 - val_accuracy: 0.2450\n",
      "Epoch 3/400\n",
      "157/157 [==============================] - 27s 173ms/step - loss: 0.0018 - accuracy: 0.3493 - val_loss: 0.8829 - val_accuracy: 0.2967\n",
      "Epoch 4/400\n",
      "157/157 [==============================] - 27s 173ms/step - loss: 0.0151 - accuracy: 0.3318 - val_loss: 0.9825 - val_accuracy: 0.3383\n",
      "Epoch 5/400\n",
      "157/157 [==============================] - 27s 175ms/step - loss: 4.1976e-04 - accuracy: 0.3154 - val_loss: 0.9595 - val_accuracy: 0.4550\n",
      "Epoch 6/400\n",
      "157/157 [==============================] - 27s 173ms/step - loss: 8.5758e-04 - accuracy: 0.3877 - val_loss: 0.9381 - val_accuracy: 0.4217\n",
      "Epoch 7/400\n",
      "157/157 [==============================] - 28s 176ms/step - loss: 0.0013 - accuracy: 0.3573 - val_loss: 0.9970 - val_accuracy: 0.3750\n",
      "Epoch 8/400\n",
      "157/157 [==============================] - 28s 176ms/step - loss: 7.0041e-05 - accuracy: 0.3545 - val_loss: 1.0179 - val_accuracy: 0.4033\n",
      "Epoch 9/400\n",
      "157/157 [==============================] - 28s 175ms/step - loss: 2.4072e-05 - accuracy: 0.3919 - val_loss: 1.0389 - val_accuracy: 0.3300\n",
      "Epoch 10/400\n",
      "157/157 [==============================] - 28s 179ms/step - loss: 2.4644e-05 - accuracy: 0.3186 - val_loss: 1.1111 - val_accuracy: 0.4400\n",
      "Epoch 11/400\n",
      "157/157 [==============================] - 28s 180ms/step - loss: 3.0658e-05 - accuracy: 0.3177 - val_loss: 1.0454 - val_accuracy: 0.4167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sidharth/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5904285714285669 0.7141666666666662 0.5322222222222224 0.7966666666666653 0.610969387755102\n"
     ]
    }
   ],
   "source": [
    "random.seed(1000)\n",
    "biased_ratio = 64\n",
    "num_of_letters = 10\n",
    "TestDataset = create_dataset(testImagesVal, testLabelsVal, False)\n",
    "f1scoreVal = []\n",
    "num_of_train_images = 10000\n",
    "\n",
    "trainImagesComb, trainLabelsComb = createOddEvenNew(trainImagesVal, trainLabelsVal)\n",
    "trainImages, trainLabels = incNumOfImages(trainImagesComb, trainLabelsComb, num_of_train_images)\n",
    "trainLabels = convertToArray(trainLabels)\n",
    "TrainDataset = create_dataset(trainImages, trainLabels)\n",
    "\n",
    "model = modelLatest()\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=0.0001, restore_best_weights=True)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "history = model.fit(TrainDataset, epochs=400, validation_data=TestDataset, callbacks=[callback], batch_size = batch_size)\n",
    "\n",
    "ypred = model.predict(TestDataset)\n",
    "#acc , combList, correctness = accuracyFunc(ypred, testLabelsVal, array)\n",
    "\n",
    "temp_f1 = 0\n",
    "temp_recall = 0\n",
    "temp_precision = 0\n",
    "temp_acc = 0\n",
    "f1Val = []\n",
    "precVal = []\n",
    "recaVal = []\n",
    "distVal = []\n",
    "y_test = testLabelsVal\n",
    "y_predicted = calculate_labels(ypred)\n",
    "for i in range(len(y_test)):\n",
    "    temp_f1 = temp_f1 + f1_score(y_test[i], y_predicted[i])\n",
    "    temp_recall = temp_recall + recall_score(y_test[i], y_predicted[i])\n",
    "    temp_precision = temp_precision + precision_score(y_test[i], y_predicted[i])\n",
    "    temp_acc = temp_acc + accuracy_score(y_test[i], y_predicted[i])\n",
    "    f1Val.append(f1_score(y_test[i], y_predicted[i]))\n",
    "    precVal.append(precision_score(y_test[i], y_predicted[i]))\n",
    "    recaVal.append(recall_score(y_test[i], y_predicted[i]))\n",
    "    distVal.append([f1_score(y_test[i], y_predicted[i])])\n",
    "f1score = temp_f1 / len(y_test)\n",
    "precision = temp_precision / len(y_test)\n",
    "recall = temp_recall / len(y_test)\n",
    "acc = temp_acc / len(y_test)\n",
    "print(f1score, precision, recall, acc, f1_score(y_test, y_predicted, average='micro'))\n",
    "\n",
    "#arrayVal = correlationmt(trainLabelsComb)\n",
    "#array = calculateDist(arrayVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 [0, 1, 2]\n",
      "1 1 [0, 1, 3]\n",
      "2 2 [0, 1, 4]\n",
      "3 3 [0, 1, 5]\n",
      "4 4 [0, 1, 6]\n",
      "5 5 [0, 1, 7]\n",
      "6 6 [0, 1, 8]\n",
      "7 7 [0, 1, 9]\n",
      "8 8 [0, 2, 3]\n",
      "9 9 [0, 2, 4]\n",
      "10 10 [0, 2, 5]\n",
      "11 11 [0, 2, 6]\n",
      "12 12 [0, 2, 7]\n",
      "13 13 [0, 2, 8]\n",
      "14 14 [0, 2, 9]\n",
      "15 15 [0, 3, 4]\n",
      "16 16 [0, 3, 5]\n",
      "17 17 [0, 3, 6]\n",
      "18 18 [0, 3, 7]\n",
      "19 19 [0, 3, 8]\n",
      "20 20 [0, 3, 9]\n",
      "21 21 [0, 4, 5]\n",
      "22 22 [0, 4, 6]\n",
      "23 23 [0, 4, 7]\n",
      "24 24 [0, 4, 8]\n",
      "25 25 [0, 4, 9]\n",
      "26 26 [0, 5, 6]\n",
      "27 27 [0, 5, 7]\n",
      "28 28 [0, 5, 8]\n",
      "29 29 [0, 5, 9]\n",
      "30 30 [0, 6, 7]\n",
      "31 31 [0, 6, 8]\n",
      "32 32 [0, 6, 9]\n",
      "33 33 [0, 7, 8]\n",
      "34 34 [0, 7, 9]\n",
      "35 35 [0, 8, 9]\n",
      "36 36 [1, 2, 3]\n",
      "37 37 [1, 2, 4]\n",
      "38 38 [1, 2, 5]\n",
      "39 39 [1, 2, 6]\n",
      "40 40 [1, 2, 7]\n",
      "41 41 [1, 2, 8]\n",
      "42 42 [1, 2, 9]\n",
      "43 43 [1, 3, 4]\n",
      "44 44 [1, 3, 5]\n",
      "45 45 [1, 3, 6]\n",
      "46 46 [1, 3, 7]\n",
      "47 47 [1, 3, 8]\n",
      "48 48 [1, 3, 9]\n",
      "49 49 [1, 4, 5]\n",
      "50 50 [1, 4, 6]\n",
      "51 51 [1, 4, 7]\n",
      "52 52 [1, 4, 8]\n",
      "53 53 [1, 4, 9]\n",
      "54 54 [1, 5, 6]\n",
      "55 55 [1, 5, 7]\n",
      "56 56 [1, 5, 8]\n",
      "57 57 [1, 5, 9]\n",
      "58 58 [1, 6, 7]\n",
      "59 59 [1, 6, 8]\n",
      "60 60 [1, 6, 9]\n",
      "61 61 [1, 7, 8]\n",
      "62 62 [1, 7, 9]\n",
      "63 63 [1, 8, 9]\n",
      "64 64 [2, 3, 4]\n",
      "65 65 [2, 3, 5]\n",
      "66 66 [2, 3, 6]\n",
      "67 67 [2, 3, 7]\n",
      "68 68 [2, 3, 8]\n",
      "69 69 [2, 3, 9]\n",
      "70 70 [2, 4, 5]\n",
      "71 71 [2, 4, 6]\n",
      "72 72 [2, 4, 7]\n",
      "73 73 [2, 4, 8]\n",
      "74 74 [2, 4, 9]\n",
      "75 75 [2, 5, 6]\n",
      "76 76 [2, 5, 7]\n",
      "77 77 [2, 5, 8]\n",
      "78 78 [2, 5, 9]\n",
      "79 79 [2, 6, 7]\n",
      "80 80 [2, 6, 8]\n",
      "81 81 [2, 6, 9]\n",
      "82 82 [2, 7, 8]\n",
      "83 83 [2, 7, 9]\n",
      "84 84 [2, 8, 9]\n",
      "85 85 [3, 4, 5]\n",
      "86 86 [3, 4, 6]\n",
      "87 87 [3, 4, 7]\n",
      "88 88 [3, 4, 8]\n",
      "89 89 [3, 4, 9]\n",
      "90 90 [3, 5, 6]\n",
      "91 91 [3, 5, 7]\n",
      "92 92 [3, 5, 8]\n",
      "93 93 [3, 5, 9]\n",
      "94 94 [3, 6, 7]\n",
      "95 95 [3, 6, 8]\n",
      "96 96 [3, 6, 9]\n",
      "97 97 [3, 7, 8]\n",
      "98 98 [3, 7, 9]\n",
      "99 99 [3, 8, 9]\n",
      "100 100 [4, 5, 6]\n",
      "101 101 [4, 5, 7]\n",
      "102 102 [4, 5, 8]\n",
      "103 103 [4, 5, 9]\n",
      "104 104 [4, 6, 7]\n",
      "105 105 [4, 6, 8]\n",
      "106 106 [4, 6, 9]\n",
      "107 107 [4, 7, 8]\n",
      "108 108 [4, 7, 9]\n",
      "109 109 [4, 8, 9]\n",
      "110 110 [5, 6, 7]\n",
      "111 111 [5, 6, 8]\n",
      "112 112 [5, 6, 9]\n",
      "113 113 [5, 7, 8]\n",
      "114 114 [5, 7, 9]\n",
      "115 115 [5, 8, 9]\n",
      "116 116 [6, 7, 8]\n",
      "117 117 [6, 7, 9]\n",
      "118 118 [6, 8, 9]\n",
      "119 119 [7, 8, 9]\n",
      "0 120 [0, 1, 2]\n",
      "1 121 [0, 1, 3]\n",
      "2 122 [0, 1, 4]\n",
      "3 123 [0, 1, 5]\n",
      "4 124 [0, 1, 6]\n",
      "5 125 [0, 1, 7]\n",
      "6 126 [0, 1, 8]\n",
      "7 127 [0, 1, 9]\n",
      "8 128 [0, 2, 3]\n",
      "9 129 [0, 2, 4]\n",
      "10 130 [0, 2, 5]\n",
      "11 131 [0, 2, 6]\n",
      "12 132 [0, 2, 7]\n",
      "13 133 [0, 2, 8]\n",
      "14 134 [0, 2, 9]\n",
      "15 135 [0, 3, 4]\n",
      "16 136 [0, 3, 5]\n",
      "17 137 [0, 3, 6]\n",
      "18 138 [0, 3, 7]\n",
      "19 139 [0, 3, 8]\n",
      "20 140 [0, 3, 9]\n",
      "21 141 [0, 4, 5]\n",
      "22 142 [0, 4, 6]\n",
      "23 143 [0, 4, 7]\n",
      "24 144 [0, 4, 8]\n",
      "25 145 [0, 4, 9]\n",
      "26 146 [0, 5, 6]\n",
      "27 147 [0, 5, 7]\n",
      "28 148 [0, 5, 8]\n",
      "29 149 [0, 5, 9]\n",
      "30 150 [0, 6, 7]\n",
      "31 151 [0, 6, 8]\n",
      "32 152 [0, 6, 9]\n",
      "33 153 [0, 7, 8]\n",
      "34 154 [0, 7, 9]\n",
      "35 155 [0, 8, 9]\n",
      "36 156 [1, 2, 3]\n",
      "37 157 [1, 2, 4]\n",
      "38 158 [1, 2, 5]\n",
      "39 159 [1, 2, 6]\n",
      "40 160 [1, 2, 7]\n",
      "41 161 [1, 2, 8]\n",
      "42 162 [1, 2, 9]\n",
      "43 163 [1, 3, 4]\n",
      "44 164 [1, 3, 5]\n",
      "45 165 [1, 3, 6]\n",
      "46 166 [1, 3, 7]\n",
      "47 167 [1, 3, 8]\n",
      "48 168 [1, 3, 9]\n",
      "49 169 [1, 4, 5]\n",
      "50 170 [1, 4, 6]\n",
      "51 171 [1, 4, 7]\n",
      "52 172 [1, 4, 8]\n",
      "53 173 [1, 4, 9]\n",
      "54 174 [1, 5, 6]\n",
      "55 175 [1, 5, 7]\n",
      "56 176 [1, 5, 8]\n",
      "57 177 [1, 5, 9]\n",
      "58 178 [1, 6, 7]\n",
      "59 179 [1, 6, 8]\n",
      "60 180 [1, 6, 9]\n",
      "61 181 [1, 7, 8]\n",
      "62 182 [1, 7, 9]\n",
      "63 183 [1, 8, 9]\n",
      "64 184 [2, 3, 4]\n",
      "65 185 [2, 3, 5]\n",
      "66 186 [2, 3, 6]\n",
      "67 187 [2, 3, 7]\n",
      "68 188 [2, 3, 8]\n",
      "69 189 [2, 3, 9]\n",
      "70 190 [2, 4, 5]\n",
      "71 191 [2, 4, 6]\n",
      "72 192 [2, 4, 7]\n",
      "73 193 [2, 4, 8]\n",
      "74 194 [2, 4, 9]\n",
      "75 195 [2, 5, 6]\n",
      "76 196 [2, 5, 7]\n",
      "77 197 [2, 5, 8]\n",
      "78 198 [2, 5, 9]\n",
      "79 199 [2, 6, 7]\n",
      "80 200 [2, 6, 8]\n",
      "81 201 [2, 6, 9]\n",
      "82 202 [2, 7, 8]\n",
      "83 203 [2, 7, 9]\n",
      "84 204 [2, 8, 9]\n",
      "85 205 [3, 4, 5]\n",
      "86 206 [3, 4, 6]\n",
      "87 207 [3, 4, 7]\n",
      "88 208 [3, 4, 8]\n",
      "89 209 [3, 4, 9]\n",
      "90 210 [3, 5, 6]\n",
      "91 211 [3, 5, 7]\n",
      "92 212 [3, 5, 8]\n",
      "93 213 [3, 5, 9]\n",
      "94 214 [3, 6, 7]\n",
      "95 215 [3, 6, 8]\n",
      "96 216 [3, 6, 9]\n",
      "97 217 [3, 7, 8]\n",
      "98 218 [3, 7, 9]\n",
      "99 219 [3, 8, 9]\n",
      "100 220 [4, 5, 6]\n",
      "101 221 [4, 5, 7]\n",
      "102 222 [4, 5, 8]\n",
      "103 223 [4, 5, 9]\n",
      "104 224 [4, 6, 7]\n",
      "105 225 [4, 6, 8]\n",
      "106 226 [4, 6, 9]\n",
      "107 227 [4, 7, 8]\n",
      "108 228 [4, 7, 9]\n",
      "109 229 [4, 8, 9]\n",
      "110 230 [5, 6, 7]\n",
      "111 231 [5, 6, 8]\n",
      "112 232 [5, 6, 9]\n",
      "113 233 [5, 7, 8]\n",
      "114 234 [5, 7, 9]\n",
      "115 235 [5, 8, 9]\n",
      "116 236 [6, 7, 8]\n",
      "117 237 [6, 7, 9]\n",
      "118 238 [6, 8, 9]\n",
      "119 239 [7, 8, 9]\n",
      "0 240 [0, 1, 2]\n",
      "1 241 [0, 1, 3]\n",
      "2 242 [0, 1, 4]\n",
      "3 243 [0, 1, 5]\n",
      "4 244 [0, 1, 6]\n",
      "5 245 [0, 1, 7]\n",
      "6 246 [0, 1, 8]\n",
      "7 247 [0, 1, 9]\n",
      "8 248 [0, 2, 3]\n",
      "9 249 [0, 2, 4]\n",
      "10 250 [0, 2, 5]\n",
      "11 251 [0, 2, 6]\n",
      "12 252 [0, 2, 7]\n",
      "13 253 [0, 2, 8]\n",
      "14 254 [0, 2, 9]\n",
      "15 255 [0, 3, 4]\n",
      "16 256 [0, 3, 5]\n",
      "17 257 [0, 3, 6]\n",
      "18 258 [0, 3, 7]\n",
      "19 259 [0, 3, 8]\n",
      "20 260 [0, 3, 9]\n",
      "21 261 [0, 4, 5]\n",
      "22 262 [0, 4, 6]\n",
      "23 263 [0, 4, 7]\n",
      "24 264 [0, 4, 8]\n",
      "25 265 [0, 4, 9]\n",
      "26 266 [0, 5, 6]\n",
      "27 267 [0, 5, 7]\n",
      "28 268 [0, 5, 8]\n",
      "29 269 [0, 5, 9]\n",
      "30 270 [0, 6, 7]\n",
      "31 271 [0, 6, 8]\n",
      "32 272 [0, 6, 9]\n",
      "33 273 [0, 7, 8]\n",
      "34 274 [0, 7, 9]\n",
      "35 275 [0, 8, 9]\n",
      "36 276 [1, 2, 3]\n",
      "37 277 [1, 2, 4]\n",
      "38 278 [1, 2, 5]\n",
      "39 279 [1, 2, 6]\n",
      "40 280 [1, 2, 7]\n",
      "41 281 [1, 2, 8]\n",
      "42 282 [1, 2, 9]\n",
      "43 283 [1, 3, 4]\n",
      "44 284 [1, 3, 5]\n",
      "45 285 [1, 3, 6]\n",
      "46 286 [1, 3, 7]\n",
      "47 287 [1, 3, 8]\n",
      "48 288 [1, 3, 9]\n",
      "49 289 [1, 4, 5]\n",
      "50 290 [1, 4, 6]\n",
      "51 291 [1, 4, 7]\n",
      "52 292 [1, 4, 8]\n",
      "53 293 [1, 4, 9]\n",
      "54 294 [1, 5, 6]\n",
      "55 295 [1, 5, 7]\n",
      "56 296 [1, 5, 8]\n",
      "57 297 [1, 5, 9]\n",
      "58 298 [1, 6, 7]\n",
      "59 299 [1, 6, 8]\n",
      "60 300 [1, 6, 9]\n",
      "61 301 [1, 7, 8]\n",
      "62 302 [1, 7, 9]\n",
      "63 303 [1, 8, 9]\n",
      "64 304 [2, 3, 4]\n",
      "65 305 [2, 3, 5]\n",
      "66 306 [2, 3, 6]\n",
      "67 307 [2, 3, 7]\n",
      "68 308 [2, 3, 8]\n",
      "69 309 [2, 3, 9]\n",
      "70 310 [2, 4, 5]\n",
      "71 311 [2, 4, 6]\n",
      "72 312 [2, 4, 7]\n",
      "73 313 [2, 4, 8]\n",
      "74 314 [2, 4, 9]\n",
      "75 315 [2, 5, 6]\n",
      "76 316 [2, 5, 7]\n",
      "77 317 [2, 5, 8]\n",
      "78 318 [2, 5, 9]\n",
      "79 319 [2, 6, 7]\n",
      "80 320 [2, 6, 8]\n",
      "81 321 [2, 6, 9]\n",
      "82 322 [2, 7, 8]\n",
      "83 323 [2, 7, 9]\n",
      "84 324 [2, 8, 9]\n",
      "85 325 [3, 4, 5]\n",
      "86 326 [3, 4, 6]\n",
      "87 327 [3, 4, 7]\n",
      "88 328 [3, 4, 8]\n",
      "89 329 [3, 4, 9]\n",
      "90 330 [3, 5, 6]\n",
      "91 331 [3, 5, 7]\n",
      "92 332 [3, 5, 8]\n",
      "93 333 [3, 5, 9]\n",
      "94 334 [3, 6, 7]\n",
      "95 335 [3, 6, 8]\n",
      "96 336 [3, 6, 9]\n",
      "97 337 [3, 7, 8]\n",
      "98 338 [3, 7, 9]\n",
      "99 339 [3, 8, 9]\n",
      "100 340 [4, 5, 6]\n",
      "101 341 [4, 5, 7]\n",
      "102 342 [4, 5, 8]\n",
      "103 343 [4, 5, 9]\n",
      "104 344 [4, 6, 7]\n",
      "105 345 [4, 6, 8]\n",
      "106 346 [4, 6, 9]\n",
      "107 347 [4, 7, 8]\n",
      "108 348 [4, 7, 9]\n",
      "109 349 [4, 8, 9]\n",
      "110 350 [5, 6, 7]\n",
      "111 351 [5, 6, 8]\n",
      "112 352 [5, 6, 9]\n",
      "113 353 [5, 7, 8]\n",
      "114 354 [5, 7, 9]\n",
      "115 355 [5, 8, 9]\n",
      "116 356 [6, 7, 8]\n",
      "117 357 [6, 7, 9]\n",
      "118 358 [6, 8, 9]\n",
      "119 359 [7, 8, 9]\n",
      "0 360 [0, 1, 2]\n",
      "1 361 [0, 1, 3]\n",
      "2 362 [0, 1, 4]\n",
      "3 363 [0, 1, 5]\n",
      "4 364 [0, 1, 6]\n",
      "5 365 [0, 1, 7]\n",
      "6 366 [0, 1, 8]\n",
      "7 367 [0, 1, 9]\n",
      "8 368 [0, 2, 3]\n",
      "9 369 [0, 2, 4]\n",
      "10 370 [0, 2, 5]\n",
      "11 371 [0, 2, 6]\n",
      "12 372 [0, 2, 7]\n",
      "13 373 [0, 2, 8]\n",
      "14 374 [0, 2, 9]\n",
      "15 375 [0, 3, 4]\n",
      "16 376 [0, 3, 5]\n",
      "17 377 [0, 3, 6]\n",
      "18 378 [0, 3, 7]\n",
      "19 379 [0, 3, 8]\n",
      "20 380 [0, 3, 9]\n",
      "21 381 [0, 4, 5]\n",
      "22 382 [0, 4, 6]\n",
      "23 383 [0, 4, 7]\n",
      "24 384 [0, 4, 8]\n",
      "25 385 [0, 4, 9]\n",
      "26 386 [0, 5, 6]\n",
      "27 387 [0, 5, 7]\n",
      "28 388 [0, 5, 8]\n",
      "29 389 [0, 5, 9]\n",
      "30 390 [0, 6, 7]\n",
      "31 391 [0, 6, 8]\n",
      "32 392 [0, 6, 9]\n",
      "33 393 [0, 7, 8]\n",
      "34 394 [0, 7, 9]\n",
      "35 395 [0, 8, 9]\n",
      "36 396 [1, 2, 3]\n",
      "37 397 [1, 2, 4]\n",
      "38 398 [1, 2, 5]\n",
      "39 399 [1, 2, 6]\n",
      "40 400 [1, 2, 7]\n",
      "41 401 [1, 2, 8]\n",
      "42 402 [1, 2, 9]\n",
      "43 403 [1, 3, 4]\n",
      "44 404 [1, 3, 5]\n",
      "45 405 [1, 3, 6]\n",
      "46 406 [1, 3, 7]\n",
      "47 407 [1, 3, 8]\n",
      "48 408 [1, 3, 9]\n",
      "49 409 [1, 4, 5]\n",
      "50 410 [1, 4, 6]\n",
      "51 411 [1, 4, 7]\n",
      "52 412 [1, 4, 8]\n",
      "53 413 [1, 4, 9]\n",
      "54 414 [1, 5, 6]\n",
      "55 415 [1, 5, 7]\n",
      "56 416 [1, 5, 8]\n",
      "57 417 [1, 5, 9]\n",
      "58 418 [1, 6, 7]\n",
      "59 419 [1, 6, 8]\n",
      "60 420 [1, 6, 9]\n",
      "61 421 [1, 7, 8]\n",
      "62 422 [1, 7, 9]\n",
      "63 423 [1, 8, 9]\n",
      "64 424 [2, 3, 4]\n",
      "65 425 [2, 3, 5]\n",
      "66 426 [2, 3, 6]\n",
      "67 427 [2, 3, 7]\n",
      "68 428 [2, 3, 8]\n",
      "69 429 [2, 3, 9]\n",
      "70 430 [2, 4, 5]\n",
      "71 431 [2, 4, 6]\n",
      "72 432 [2, 4, 7]\n",
      "73 433 [2, 4, 8]\n",
      "74 434 [2, 4, 9]\n",
      "75 435 [2, 5, 6]\n",
      "76 436 [2, 5, 7]\n",
      "77 437 [2, 5, 8]\n",
      "78 438 [2, 5, 9]\n",
      "79 439 [2, 6, 7]\n",
      "80 440 [2, 6, 8]\n",
      "81 441 [2, 6, 9]\n",
      "82 442 [2, 7, 8]\n",
      "83 443 [2, 7, 9]\n",
      "84 444 [2, 8, 9]\n",
      "85 445 [3, 4, 5]\n",
      "86 446 [3, 4, 6]\n",
      "87 447 [3, 4, 7]\n",
      "88 448 [3, 4, 8]\n",
      "89 449 [3, 4, 9]\n",
      "90 450 [3, 5, 6]\n",
      "91 451 [3, 5, 7]\n",
      "92 452 [3, 5, 8]\n",
      "93 453 [3, 5, 9]\n",
      "94 454 [3, 6, 7]\n",
      "95 455 [3, 6, 8]\n",
      "96 456 [3, 6, 9]\n",
      "97 457 [3, 7, 8]\n",
      "98 458 [3, 7, 9]\n",
      "99 459 [3, 8, 9]\n",
      "100 460 [4, 5, 6]\n",
      "101 461 [4, 5, 7]\n",
      "102 462 [4, 5, 8]\n",
      "103 463 [4, 5, 9]\n",
      "104 464 [4, 6, 7]\n",
      "105 465 [4, 6, 8]\n",
      "106 466 [4, 6, 9]\n",
      "107 467 [4, 7, 8]\n",
      "108 468 [4, 7, 9]\n",
      "109 469 [4, 8, 9]\n",
      "110 470 [5, 6, 7]\n",
      "111 471 [5, 6, 8]\n",
      "112 472 [5, 6, 9]\n",
      "113 473 [5, 7, 8]\n",
      "114 474 [5, 7, 9]\n",
      "115 475 [5, 8, 9]\n",
      "116 476 [6, 7, 8]\n",
      "117 477 [6, 7, 9]\n",
      "118 478 [6, 8, 9]\n",
      "119 479 [7, 8, 9]\n",
      "0 480 [0, 1, 2]\n",
      "1 481 [0, 1, 3]\n",
      "2 482 [0, 1, 4]\n",
      "3 483 [0, 1, 5]\n",
      "4 484 [0, 1, 6]\n",
      "5 485 [0, 1, 7]\n",
      "6 486 [0, 1, 8]\n",
      "7 487 [0, 1, 9]\n",
      "8 488 [0, 2, 3]\n",
      "9 489 [0, 2, 4]\n",
      "10 490 [0, 2, 5]\n",
      "11 491 [0, 2, 6]\n",
      "12 492 [0, 2, 7]\n",
      "13 493 [0, 2, 8]\n",
      "14 494 [0, 2, 9]\n",
      "15 495 [0, 3, 4]\n",
      "16 496 [0, 3, 5]\n",
      "17 497 [0, 3, 6]\n",
      "18 498 [0, 3, 7]\n",
      "19 499 [0, 3, 8]\n",
      "20 500 [0, 3, 9]\n",
      "21 501 [0, 4, 5]\n",
      "22 502 [0, 4, 6]\n",
      "23 503 [0, 4, 7]\n",
      "24 504 [0, 4, 8]\n",
      "25 505 [0, 4, 9]\n",
      "26 506 [0, 5, 6]\n",
      "27 507 [0, 5, 7]\n",
      "28 508 [0, 5, 8]\n",
      "29 509 [0, 5, 9]\n",
      "30 510 [0, 6, 7]\n",
      "31 511 [0, 6, 8]\n",
      "32 512 [0, 6, 9]\n",
      "33 513 [0, 7, 8]\n",
      "34 514 [0, 7, 9]\n",
      "35 515 [0, 8, 9]\n",
      "36 516 [1, 2, 3]\n",
      "37 517 [1, 2, 4]\n",
      "38 518 [1, 2, 5]\n",
      "39 519 [1, 2, 6]\n",
      "40 520 [1, 2, 7]\n",
      "41 521 [1, 2, 8]\n",
      "42 522 [1, 2, 9]\n",
      "43 523 [1, 3, 4]\n",
      "44 524 [1, 3, 5]\n",
      "45 525 [1, 3, 6]\n",
      "46 526 [1, 3, 7]\n",
      "47 527 [1, 3, 8]\n",
      "48 528 [1, 3, 9]\n",
      "49 529 [1, 4, 5]\n",
      "50 530 [1, 4, 6]\n",
      "51 531 [1, 4, 7]\n",
      "52 532 [1, 4, 8]\n",
      "53 533 [1, 4, 9]\n",
      "54 534 [1, 5, 6]\n",
      "55 535 [1, 5, 7]\n",
      "56 536 [1, 5, 8]\n",
      "57 537 [1, 5, 9]\n",
      "58 538 [1, 6, 7]\n",
      "59 539 [1, 6, 8]\n",
      "60 540 [1, 6, 9]\n",
      "61 541 [1, 7, 8]\n",
      "62 542 [1, 7, 9]\n",
      "63 543 [1, 8, 9]\n",
      "64 544 [2, 3, 4]\n",
      "65 545 [2, 3, 5]\n",
      "66 546 [2, 3, 6]\n",
      "67 547 [2, 3, 7]\n",
      "68 548 [2, 3, 8]\n",
      "69 549 [2, 3, 9]\n",
      "70 550 [2, 4, 5]\n",
      "71 551 [2, 4, 6]\n",
      "72 552 [2, 4, 7]\n",
      "73 553 [2, 4, 8]\n",
      "74 554 [2, 4, 9]\n",
      "75 555 [2, 5, 6]\n",
      "76 556 [2, 5, 7]\n",
      "77 557 [2, 5, 8]\n",
      "78 558 [2, 5, 9]\n",
      "79 559 [2, 6, 7]\n",
      "80 560 [2, 6, 8]\n",
      "81 561 [2, 6, 9]\n",
      "82 562 [2, 7, 8]\n",
      "83 563 [2, 7, 9]\n",
      "84 564 [2, 8, 9]\n",
      "85 565 [3, 4, 5]\n",
      "86 566 [3, 4, 6]\n",
      "87 567 [3, 4, 7]\n",
      "88 568 [3, 4, 8]\n",
      "89 569 [3, 4, 9]\n",
      "90 570 [3, 5, 6]\n",
      "91 571 [3, 5, 7]\n",
      "92 572 [3, 5, 8]\n",
      "93 573 [3, 5, 9]\n",
      "94 574 [3, 6, 7]\n",
      "95 575 [3, 6, 8]\n",
      "96 576 [3, 6, 9]\n",
      "97 577 [3, 7, 8]\n",
      "98 578 [3, 7, 9]\n",
      "99 579 [3, 8, 9]\n",
      "100 580 [4, 5, 6]\n",
      "101 581 [4, 5, 7]\n",
      "102 582 [4, 5, 8]\n",
      "103 583 [4, 5, 9]\n",
      "104 584 [4, 6, 7]\n",
      "105 585 [4, 6, 8]\n",
      "106 586 [4, 6, 9]\n",
      "107 587 [4, 7, 8]\n",
      "108 588 [4, 7, 9]\n",
      "109 589 [4, 8, 9]\n",
      "110 590 [5, 6, 7]\n",
      "111 591 [5, 6, 8]\n",
      "112 592 [5, 6, 9]\n",
      "113 593 [5, 7, 8]\n",
      "114 594 [5, 7, 9]\n",
      "115 595 [5, 8, 9]\n",
      "116 596 [6, 7, 8]\n",
      "117 597 [6, 7, 9]\n",
      "118 598 [6, 8, 9]\n",
      "119 599 [7, 8, 9]\n",
      "0 600 [0, 1, 2]\n",
      "1 601 [0, 1, 3]\n",
      "2 602 [0, 1, 4]\n",
      "3 603 [0, 1, 5]\n",
      "4 604 [0, 1, 6]\n",
      "5 605 [0, 1, 7]\n",
      "6 606 [0, 1, 8]\n",
      "7 607 [0, 1, 9]\n",
      "8 608 [0, 2, 3]\n",
      "9 609 [0, 2, 4]\n",
      "10 610 [0, 2, 5]\n",
      "11 611 [0, 2, 6]\n",
      "12 612 [0, 2, 7]\n",
      "13 613 [0, 2, 8]\n",
      "14 614 [0, 2, 9]\n",
      "15 615 [0, 3, 4]\n",
      "16 616 [0, 3, 5]\n",
      "17 617 [0, 3, 6]\n",
      "18 618 [0, 3, 7]\n",
      "19 619 [0, 3, 8]\n",
      "20 620 [0, 3, 9]\n",
      "21 621 [0, 4, 5]\n",
      "22 622 [0, 4, 6]\n",
      "23 623 [0, 4, 7]\n",
      "24 624 [0, 4, 8]\n",
      "25 625 [0, 4, 9]\n",
      "26 626 [0, 5, 6]\n",
      "27 627 [0, 5, 7]\n",
      "28 628 [0, 5, 8]\n",
      "29 629 [0, 5, 9]\n",
      "30 630 [0, 6, 7]\n",
      "31 631 [0, 6, 8]\n",
      "32 632 [0, 6, 9]\n",
      "33 633 [0, 7, 8]\n",
      "34 634 [0, 7, 9]\n",
      "35 635 [0, 8, 9]\n",
      "36 636 [1, 2, 3]\n",
      "37 637 [1, 2, 4]\n",
      "38 638 [1, 2, 5]\n",
      "39 639 [1, 2, 6]\n",
      "40 640 [1, 2, 7]\n",
      "41 641 [1, 2, 8]\n",
      "42 642 [1, 2, 9]\n",
      "43 643 [1, 3, 4]\n",
      "44 644 [1, 3, 5]\n",
      "45 645 [1, 3, 6]\n",
      "46 646 [1, 3, 7]\n",
      "47 647 [1, 3, 8]\n",
      "48 648 [1, 3, 9]\n",
      "49 649 [1, 4, 5]\n",
      "50 650 [1, 4, 6]\n",
      "51 651 [1, 4, 7]\n",
      "52 652 [1, 4, 8]\n",
      "53 653 [1, 4, 9]\n",
      "54 654 [1, 5, 6]\n",
      "55 655 [1, 5, 7]\n",
      "56 656 [1, 5, 8]\n",
      "57 657 [1, 5, 9]\n",
      "58 658 [1, 6, 7]\n",
      "59 659 [1, 6, 8]\n",
      "60 660 [1, 6, 9]\n",
      "61 661 [1, 7, 8]\n",
      "62 662 [1, 7, 9]\n",
      "63 663 [1, 8, 9]\n",
      "64 664 [2, 3, 4]\n",
      "65 665 [2, 3, 5]\n",
      "66 666 [2, 3, 6]\n",
      "67 667 [2, 3, 7]\n",
      "68 668 [2, 3, 8]\n",
      "69 669 [2, 3, 9]\n",
      "70 670 [2, 4, 5]\n",
      "71 671 [2, 4, 6]\n",
      "72 672 [2, 4, 7]\n",
      "73 673 [2, 4, 8]\n",
      "74 674 [2, 4, 9]\n",
      "75 675 [2, 5, 6]\n",
      "76 676 [2, 5, 7]\n",
      "77 677 [2, 5, 8]\n",
      "78 678 [2, 5, 9]\n",
      "79 679 [2, 6, 7]\n",
      "80 680 [2, 6, 8]\n",
      "81 681 [2, 6, 9]\n",
      "82 682 [2, 7, 8]\n",
      "83 683 [2, 7, 9]\n",
      "84 684 [2, 8, 9]\n",
      "85 685 [3, 4, 5]\n",
      "86 686 [3, 4, 6]\n",
      "87 687 [3, 4, 7]\n",
      "88 688 [3, 4, 8]\n",
      "89 689 [3, 4, 9]\n",
      "90 690 [3, 5, 6]\n",
      "91 691 [3, 5, 7]\n",
      "92 692 [3, 5, 8]\n",
      "93 693 [3, 5, 9]\n",
      "94 694 [3, 6, 7]\n",
      "95 695 [3, 6, 8]\n",
      "96 696 [3, 6, 9]\n",
      "97 697 [3, 7, 8]\n",
      "98 698 [3, 7, 9]\n",
      "99 699 [3, 8, 9]\n",
      "100 700 [4, 5, 6]\n",
      "101 701 [4, 5, 7]\n",
      "102 702 [4, 5, 8]\n",
      "103 703 [4, 5, 9]\n",
      "104 704 [4, 6, 7]\n",
      "105 705 [4, 6, 8]\n",
      "106 706 [4, 6, 9]\n",
      "107 707 [4, 7, 8]\n",
      "108 708 [4, 7, 9]\n",
      "109 709 [4, 8, 9]\n",
      "110 710 [5, 6, 7]\n",
      "111 711 [5, 6, 8]\n",
      "112 712 [5, 6, 9]\n",
      "113 713 [5, 7, 8]\n",
      "114 714 [5, 7, 9]\n",
      "115 715 [5, 8, 9]\n",
      "116 716 [6, 7, 8]\n",
      "117 717 [6, 7, 9]\n",
      "118 718 [6, 8, 9]\n",
      "119 719 [7, 8, 9]\n",
      "0 720 [0, 1, 2]\n",
      "1 721 [0, 1, 3]\n",
      "2 722 [0, 1, 4]\n",
      "3 723 [0, 1, 5]\n",
      "4 724 [0, 1, 6]\n",
      "5 725 [0, 1, 7]\n",
      "6 726 [0, 1, 8]\n",
      "7 727 [0, 1, 9]\n",
      "8 728 [0, 2, 3]\n",
      "9 729 [0, 2, 4]\n",
      "10 730 [0, 2, 5]\n",
      "11 731 [0, 2, 6]\n",
      "12 732 [0, 2, 7]\n",
      "13 733 [0, 2, 8]\n",
      "14 734 [0, 2, 9]\n",
      "15 735 [0, 3, 4]\n",
      "16 736 [0, 3, 5]\n",
      "17 737 [0, 3, 6]\n",
      "18 738 [0, 3, 7]\n",
      "19 739 [0, 3, 8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 740 [0, 3, 9]\n",
      "21 741 [0, 4, 5]\n",
      "22 742 [0, 4, 6]\n",
      "23 743 [0, 4, 7]\n",
      "24 744 [0, 4, 8]\n",
      "25 745 [0, 4, 9]\n",
      "26 746 [0, 5, 6]\n",
      "27 747 [0, 5, 7]\n",
      "28 748 [0, 5, 8]\n",
      "29 749 [0, 5, 9]\n",
      "30 750 [0, 6, 7]\n",
      "31 751 [0, 6, 8]\n",
      "32 752 [0, 6, 9]\n",
      "33 753 [0, 7, 8]\n",
      "34 754 [0, 7, 9]\n",
      "35 755 [0, 8, 9]\n",
      "36 756 [1, 2, 3]\n",
      "37 757 [1, 2, 4]\n",
      "38 758 [1, 2, 5]\n",
      "39 759 [1, 2, 6]\n",
      "40 760 [1, 2, 7]\n",
      "41 761 [1, 2, 8]\n",
      "42 762 [1, 2, 9]\n",
      "43 763 [1, 3, 4]\n",
      "44 764 [1, 3, 5]\n",
      "45 765 [1, 3, 6]\n",
      "46 766 [1, 3, 7]\n",
      "47 767 [1, 3, 8]\n",
      "48 768 [1, 3, 9]\n",
      "49 769 [1, 4, 5]\n",
      "50 770 [1, 4, 6]\n",
      "51 771 [1, 4, 7]\n",
      "52 772 [1, 4, 8]\n",
      "53 773 [1, 4, 9]\n",
      "54 774 [1, 5, 6]\n",
      "55 775 [1, 5, 7]\n",
      "56 776 [1, 5, 8]\n",
      "57 777 [1, 5, 9]\n",
      "58 778 [1, 6, 7]\n",
      "59 779 [1, 6, 8]\n",
      "60 780 [1, 6, 9]\n",
      "61 781 [1, 7, 8]\n",
      "62 782 [1, 7, 9]\n",
      "63 783 [1, 8, 9]\n",
      "64 784 [2, 3, 4]\n",
      "65 785 [2, 3, 5]\n",
      "66 786 [2, 3, 6]\n",
      "67 787 [2, 3, 7]\n",
      "68 788 [2, 3, 8]\n",
      "69 789 [2, 3, 9]\n",
      "70 790 [2, 4, 5]\n",
      "71 791 [2, 4, 6]\n",
      "72 792 [2, 4, 7]\n",
      "73 793 [2, 4, 8]\n",
      "74 794 [2, 4, 9]\n",
      "75 795 [2, 5, 6]\n",
      "76 796 [2, 5, 7]\n",
      "77 797 [2, 5, 8]\n",
      "78 798 [2, 5, 9]\n",
      "79 799 [2, 6, 7]\n",
      "80 800 [2, 6, 8]\n",
      "81 801 [2, 6, 9]\n",
      "82 802 [2, 7, 8]\n",
      "83 803 [2, 7, 9]\n",
      "84 804 [2, 8, 9]\n",
      "85 805 [3, 4, 5]\n",
      "86 806 [3, 4, 6]\n",
      "87 807 [3, 4, 7]\n",
      "88 808 [3, 4, 8]\n",
      "89 809 [3, 4, 9]\n",
      "90 810 [3, 5, 6]\n",
      "91 811 [3, 5, 7]\n",
      "92 812 [3, 5, 8]\n",
      "93 813 [3, 5, 9]\n",
      "94 814 [3, 6, 7]\n",
      "95 815 [3, 6, 8]\n",
      "96 816 [3, 6, 9]\n",
      "97 817 [3, 7, 8]\n",
      "98 818 [3, 7, 9]\n",
      "99 819 [3, 8, 9]\n",
      "100 820 [4, 5, 6]\n",
      "101 821 [4, 5, 7]\n",
      "102 822 [4, 5, 8]\n",
      "103 823 [4, 5, 9]\n",
      "104 824 [4, 6, 7]\n",
      "105 825 [4, 6, 8]\n",
      "106 826 [4, 6, 9]\n",
      "107 827 [4, 7, 8]\n",
      "108 828 [4, 7, 9]\n",
      "109 829 [4, 8, 9]\n",
      "110 830 [5, 6, 7]\n",
      "111 831 [5, 6, 8]\n",
      "112 832 [5, 6, 9]\n",
      "113 833 [5, 7, 8]\n",
      "114 834 [5, 7, 9]\n",
      "115 835 [5, 8, 9]\n",
      "116 836 [6, 7, 8]\n",
      "117 837 [6, 7, 9]\n",
      "118 838 [6, 8, 9]\n",
      "119 839 [7, 8, 9]\n",
      "0 840 [0, 1, 2]\n",
      "1 841 [0, 1, 3]\n",
      "2 842 [0, 1, 4]\n",
      "3 843 [0, 1, 5]\n",
      "4 844 [0, 1, 6]\n",
      "5 845 [0, 1, 7]\n",
      "6 846 [0, 1, 8]\n",
      "7 847 [0, 1, 9]\n",
      "8 848 [0, 2, 3]\n",
      "9 849 [0, 2, 4]\n",
      "10 850 [0, 2, 5]\n",
      "11 851 [0, 2, 6]\n",
      "12 852 [0, 2, 7]\n",
      "13 853 [0, 2, 8]\n",
      "14 854 [0, 2, 9]\n",
      "15 855 [0, 3, 4]\n",
      "16 856 [0, 3, 5]\n",
      "17 857 [0, 3, 6]\n",
      "18 858 [0, 3, 7]\n",
      "19 859 [0, 3, 8]\n",
      "20 860 [0, 3, 9]\n",
      "21 861 [0, 4, 5]\n",
      "22 862 [0, 4, 6]\n",
      "23 863 [0, 4, 7]\n",
      "24 864 [0, 4, 8]\n",
      "25 865 [0, 4, 9]\n",
      "26 866 [0, 5, 6]\n",
      "27 867 [0, 5, 7]\n",
      "28 868 [0, 5, 8]\n",
      "29 869 [0, 5, 9]\n",
      "30 870 [0, 6, 7]\n",
      "31 871 [0, 6, 8]\n",
      "32 872 [0, 6, 9]\n",
      "33 873 [0, 7, 8]\n",
      "34 874 [0, 7, 9]\n",
      "35 875 [0, 8, 9]\n",
      "36 876 [1, 2, 3]\n",
      "37 877 [1, 2, 4]\n",
      "38 878 [1, 2, 5]\n",
      "39 879 [1, 2, 6]\n",
      "40 880 [1, 2, 7]\n",
      "41 881 [1, 2, 8]\n",
      "42 882 [1, 2, 9]\n",
      "43 883 [1, 3, 4]\n",
      "44 884 [1, 3, 5]\n",
      "45 885 [1, 3, 6]\n",
      "46 886 [1, 3, 7]\n",
      "47 887 [1, 3, 8]\n",
      "48 888 [1, 3, 9]\n",
      "49 889 [1, 4, 5]\n",
      "50 890 [1, 4, 6]\n",
      "51 891 [1, 4, 7]\n",
      "52 892 [1, 4, 8]\n",
      "53 893 [1, 4, 9]\n",
      "54 894 [1, 5, 6]\n",
      "55 895 [1, 5, 7]\n",
      "56 896 [1, 5, 8]\n",
      "57 897 [1, 5, 9]\n",
      "58 898 [1, 6, 7]\n",
      "59 899 [1, 6, 8]\n",
      "60 900 [1, 6, 9]\n",
      "61 901 [1, 7, 8]\n",
      "62 902 [1, 7, 9]\n",
      "63 903 [1, 8, 9]\n",
      "64 904 [2, 3, 4]\n",
      "65 905 [2, 3, 5]\n",
      "66 906 [2, 3, 6]\n",
      "67 907 [2, 3, 7]\n",
      "68 908 [2, 3, 8]\n",
      "69 909 [2, 3, 9]\n",
      "70 910 [2, 4, 5]\n",
      "71 911 [2, 4, 6]\n",
      "72 912 [2, 4, 7]\n",
      "73 913 [2, 4, 8]\n",
      "74 914 [2, 4, 9]\n",
      "75 915 [2, 5, 6]\n",
      "76 916 [2, 5, 7]\n",
      "77 917 [2, 5, 8]\n",
      "78 918 [2, 5, 9]\n",
      "79 919 [2, 6, 7]\n",
      "80 920 [2, 6, 8]\n",
      "81 921 [2, 6, 9]\n",
      "82 922 [2, 7, 8]\n",
      "83 923 [2, 7, 9]\n",
      "84 924 [2, 8, 9]\n",
      "85 925 [3, 4, 5]\n",
      "86 926 [3, 4, 6]\n",
      "87 927 [3, 4, 7]\n",
      "88 928 [3, 4, 8]\n",
      "89 929 [3, 4, 9]\n",
      "90 930 [3, 5, 6]\n",
      "91 931 [3, 5, 7]\n",
      "92 932 [3, 5, 8]\n",
      "93 933 [3, 5, 9]\n",
      "94 934 [3, 6, 7]\n",
      "95 935 [3, 6, 8]\n",
      "96 936 [3, 6, 9]\n",
      "97 937 [3, 7, 8]\n",
      "98 938 [3, 7, 9]\n",
      "99 939 [3, 8, 9]\n",
      "100 940 [4, 5, 6]\n",
      "101 941 [4, 5, 7]\n",
      "102 942 [4, 5, 8]\n",
      "103 943 [4, 5, 9]\n",
      "104 944 [4, 6, 7]\n",
      "105 945 [4, 6, 8]\n",
      "106 946 [4, 6, 9]\n",
      "107 947 [4, 7, 8]\n",
      "108 948 [4, 7, 9]\n",
      "109 949 [4, 8, 9]\n",
      "110 950 [5, 6, 7]\n",
      "111 951 [5, 6, 8]\n",
      "112 952 [5, 6, 9]\n",
      "113 953 [5, 7, 8]\n",
      "114 954 [5, 7, 9]\n",
      "115 955 [5, 8, 9]\n",
      "116 956 [6, 7, 8]\n",
      "117 957 [6, 7, 9]\n",
      "118 958 [6, 8, 9]\n",
      "119 959 [7, 8, 9]\n",
      "0 960 [0, 1, 2]\n",
      "1 961 [0, 1, 3]\n",
      "2 962 [0, 1, 4]\n",
      "3 963 [0, 1, 5]\n",
      "4 964 [0, 1, 6]\n",
      "5 965 [0, 1, 7]\n",
      "6 966 [0, 1, 8]\n",
      "7 967 [0, 1, 9]\n",
      "8 968 [0, 2, 3]\n",
      "9 969 [0, 2, 4]\n",
      "10 970 [0, 2, 5]\n",
      "11 971 [0, 2, 6]\n",
      "12 972 [0, 2, 7]\n",
      "13 973 [0, 2, 8]\n",
      "14 974 [0, 2, 9]\n",
      "15 975 [0, 3, 4]\n",
      "16 976 [0, 3, 5]\n",
      "17 977 [0, 3, 6]\n",
      "18 978 [0, 3, 7]\n",
      "19 979 [0, 3, 8]\n",
      "20 980 [0, 3, 9]\n",
      "21 981 [0, 4, 5]\n",
      "22 982 [0, 4, 6]\n",
      "23 983 [0, 4, 7]\n",
      "24 984 [0, 4, 8]\n",
      "25 985 [0, 4, 9]\n",
      "26 986 [0, 5, 6]\n",
      "27 987 [0, 5, 7]\n",
      "28 988 [0, 5, 8]\n",
      "29 989 [0, 5, 9]\n",
      "30 990 [0, 6, 7]\n",
      "31 991 [0, 6, 8]\n",
      "32 992 [0, 6, 9]\n",
      "33 993 [0, 7, 8]\n",
      "34 994 [0, 7, 9]\n",
      "35 995 [0, 8, 9]\n",
      "36 996 [1, 2, 3]\n",
      "37 997 [1, 2, 4]\n",
      "38 998 [1, 2, 5]\n",
      "39 999 [1, 2, 6]\n",
      "40 1000 [1, 2, 7]\n",
      "41 1001 [1, 2, 8]\n",
      "42 1002 [1, 2, 9]\n",
      "43 1003 [1, 3, 4]\n",
      "44 1004 [1, 3, 5]\n",
      "45 1005 [1, 3, 6]\n",
      "46 1006 [1, 3, 7]\n",
      "47 1007 [1, 3, 8]\n",
      "48 1008 [1, 3, 9]\n",
      "49 1009 [1, 4, 5]\n",
      "50 1010 [1, 4, 6]\n",
      "51 1011 [1, 4, 7]\n",
      "52 1012 [1, 4, 8]\n",
      "53 1013 [1, 4, 9]\n",
      "54 1014 [1, 5, 6]\n",
      "55 1015 [1, 5, 7]\n",
      "56 1016 [1, 5, 8]\n",
      "57 1017 [1, 5, 9]\n",
      "58 1018 [1, 6, 7]\n",
      "59 1019 [1, 6, 8]\n",
      "60 1020 [1, 6, 9]\n",
      "61 1021 [1, 7, 8]\n",
      "62 1022 [1, 7, 9]\n",
      "63 1023 [1, 8, 9]\n",
      "64 1024 [2, 3, 4]\n",
      "65 1025 [2, 3, 5]\n",
      "66 1026 [2, 3, 6]\n",
      "67 1027 [2, 3, 7]\n",
      "68 1028 [2, 3, 8]\n",
      "69 1029 [2, 3, 9]\n",
      "70 1030 [2, 4, 5]\n",
      "71 1031 [2, 4, 6]\n",
      "72 1032 [2, 4, 7]\n",
      "73 1033 [2, 4, 8]\n",
      "74 1034 [2, 4, 9]\n",
      "75 1035 [2, 5, 6]\n",
      "76 1036 [2, 5, 7]\n",
      "77 1037 [2, 5, 8]\n",
      "78 1038 [2, 5, 9]\n",
      "79 1039 [2, 6, 7]\n",
      "80 1040 [2, 6, 8]\n",
      "81 1041 [2, 6, 9]\n",
      "82 1042 [2, 7, 8]\n",
      "83 1043 [2, 7, 9]\n",
      "84 1044 [2, 8, 9]\n",
      "85 1045 [3, 4, 5]\n",
      "86 1046 [3, 4, 6]\n",
      "87 1047 [3, 4, 7]\n",
      "88 1048 [3, 4, 8]\n",
      "89 1049 [3, 4, 9]\n",
      "90 1050 [3, 5, 6]\n",
      "91 1051 [3, 5, 7]\n",
      "92 1052 [3, 5, 8]\n",
      "93 1053 [3, 5, 9]\n",
      "94 1054 [3, 6, 7]\n",
      "95 1055 [3, 6, 8]\n",
      "96 1056 [3, 6, 9]\n",
      "97 1057 [3, 7, 8]\n",
      "98 1058 [3, 7, 9]\n",
      "99 1059 [3, 8, 9]\n",
      "100 1060 [4, 5, 6]\n",
      "101 1061 [4, 5, 7]\n",
      "102 1062 [4, 5, 8]\n",
      "103 1063 [4, 5, 9]\n",
      "104 1064 [4, 6, 7]\n",
      "105 1065 [4, 6, 8]\n",
      "106 1066 [4, 6, 9]\n",
      "107 1067 [4, 7, 8]\n",
      "108 1068 [4, 7, 9]\n",
      "109 1069 [4, 8, 9]\n",
      "110 1070 [5, 6, 7]\n",
      "111 1071 [5, 6, 8]\n",
      "112 1072 [5, 6, 9]\n",
      "113 1073 [5, 7, 8]\n",
      "114 1074 [5, 7, 9]\n",
      "115 1075 [5, 8, 9]\n",
      "116 1076 [6, 7, 8]\n",
      "117 1077 [6, 7, 9]\n",
      "118 1078 [6, 8, 9]\n",
      "119 1079 [7, 8, 9]\n",
      "0 1080 [0, 1, 2]\n",
      "1 1081 [0, 1, 3]\n",
      "2 1082 [0, 1, 4]\n",
      "3 1083 [0, 1, 5]\n",
      "4 1084 [0, 1, 6]\n",
      "5 1085 [0, 1, 7]\n",
      "6 1086 [0, 1, 8]\n",
      "7 1087 [0, 1, 9]\n",
      "8 1088 [0, 2, 3]\n",
      "9 1089 [0, 2, 4]\n",
      "10 1090 [0, 2, 5]\n",
      "11 1091 [0, 2, 6]\n",
      "12 1092 [0, 2, 7]\n",
      "13 1093 [0, 2, 8]\n",
      "14 1094 [0, 2, 9]\n",
      "15 1095 [0, 3, 4]\n",
      "16 1096 [0, 3, 5]\n",
      "17 1097 [0, 3, 6]\n",
      "18 1098 [0, 3, 7]\n",
      "19 1099 [0, 3, 8]\n",
      "20 1100 [0, 3, 9]\n",
      "21 1101 [0, 4, 5]\n",
      "22 1102 [0, 4, 6]\n",
      "23 1103 [0, 4, 7]\n",
      "24 1104 [0, 4, 8]\n",
      "25 1105 [0, 4, 9]\n",
      "26 1106 [0, 5, 6]\n",
      "27 1107 [0, 5, 7]\n",
      "28 1108 [0, 5, 8]\n",
      "29 1109 [0, 5, 9]\n",
      "30 1110 [0, 6, 7]\n",
      "31 1111 [0, 6, 8]\n",
      "32 1112 [0, 6, 9]\n",
      "33 1113 [0, 7, 8]\n",
      "34 1114 [0, 7, 9]\n",
      "35 1115 [0, 8, 9]\n",
      "36 1116 [1, 2, 3]\n",
      "37 1117 [1, 2, 4]\n",
      "38 1118 [1, 2, 5]\n",
      "39 1119 [1, 2, 6]\n",
      "40 1120 [1, 2, 7]\n",
      "41 1121 [1, 2, 8]\n",
      "42 1122 [1, 2, 9]\n",
      "43 1123 [1, 3, 4]\n",
      "44 1124 [1, 3, 5]\n",
      "45 1125 [1, 3, 6]\n",
      "46 1126 [1, 3, 7]\n",
      "47 1127 [1, 3, 8]\n",
      "48 1128 [1, 3, 9]\n",
      "49 1129 [1, 4, 5]\n",
      "50 1130 [1, 4, 6]\n",
      "51 1131 [1, 4, 7]\n",
      "52 1132 [1, 4, 8]\n",
      "53 1133 [1, 4, 9]\n",
      "54 1134 [1, 5, 6]\n",
      "55 1135 [1, 5, 7]\n",
      "56 1136 [1, 5, 8]\n",
      "57 1137 [1, 5, 9]\n",
      "58 1138 [1, 6, 7]\n",
      "59 1139 [1, 6, 8]\n",
      "60 1140 [1, 6, 9]\n",
      "61 1141 [1, 7, 8]\n",
      "62 1142 [1, 7, 9]\n",
      "63 1143 [1, 8, 9]\n",
      "64 1144 [2, 3, 4]\n",
      "65 1145 [2, 3, 5]\n",
      "66 1146 [2, 3, 6]\n",
      "67 1147 [2, 3, 7]\n",
      "68 1148 [2, 3, 8]\n",
      "69 1149 [2, 3, 9]\n",
      "70 1150 [2, 4, 5]\n",
      "71 1151 [2, 4, 6]\n",
      "72 1152 [2, 4, 7]\n",
      "73 1153 [2, 4, 8]\n",
      "74 1154 [2, 4, 9]\n",
      "75 1155 [2, 5, 6]\n",
      "76 1156 [2, 5, 7]\n",
      "77 1157 [2, 5, 8]\n",
      "78 1158 [2, 5, 9]\n",
      "79 1159 [2, 6, 7]\n",
      "80 1160 [2, 6, 8]\n",
      "81 1161 [2, 6, 9]\n",
      "82 1162 [2, 7, 8]\n",
      "83 1163 [2, 7, 9]\n",
      "84 1164 [2, 8, 9]\n",
      "85 1165 [3, 4, 5]\n",
      "86 1166 [3, 4, 6]\n",
      "87 1167 [3, 4, 7]\n",
      "88 1168 [3, 4, 8]\n",
      "89 1169 [3, 4, 9]\n",
      "90 1170 [3, 5, 6]\n",
      "91 1171 [3, 5, 7]\n",
      "92 1172 [3, 5, 8]\n",
      "93 1173 [3, 5, 9]\n",
      "94 1174 [3, 6, 7]\n",
      "95 1175 [3, 6, 8]\n",
      "96 1176 [3, 6, 9]\n",
      "97 1177 [3, 7, 8]\n",
      "98 1178 [3, 7, 9]\n",
      "99 1179 [3, 8, 9]\n",
      "100 1180 [4, 5, 6]\n",
      "101 1181 [4, 5, 7]\n",
      "102 1182 [4, 5, 8]\n",
      "103 1183 [4, 5, 9]\n",
      "104 1184 [4, 6, 7]\n",
      "105 1185 [4, 6, 8]\n",
      "106 1186 [4, 6, 9]\n",
      "107 1187 [4, 7, 8]\n",
      "108 1188 [4, 7, 9]\n",
      "109 1189 [4, 8, 9]\n",
      "110 1190 [5, 6, 7]\n",
      "111 1191 [5, 6, 8]\n",
      "112 1192 [5, 6, 9]\n",
      "113 1193 [5, 7, 8]\n",
      "114 1194 [5, 7, 9]\n",
      "115 1195 [5, 8, 9]\n",
      "116 1196 [6, 7, 8]\n",
      "117 1197 [6, 7, 9]\n",
      "118 1198 [6, 8, 9]\n",
      "119 1199 [7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "y_test_num = convertToNum(testLabelsVal)\n",
    "\n",
    "f1Final = np.zeros(120)\n",
    "precFinal = np.zeros(120)\n",
    "recaFinal = np.zeros(120)\n",
    "for i in range(10):\n",
    "    for j in range(120):\n",
    "        index = (i*120)+j\n",
    "        print(j, ((i*120)+j),y_test_num[index])\n",
    "        f1Final[j] = f1Final[j] + f1Val[index]\n",
    "        precFinal[j] = precFinal[j] + precVal[index]\n",
    "        recaFinal[j] = recaFinal[j] + recaVal[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33999999999999997 0.5 0.26666666666666666\n",
      "0.6133333333333334 0.7333333333333333 0.5333333333333332\n",
      "0.5599999999999999 0.7 0.4666666666666668\n",
      "0.6333333333333333 0.8333333333333333 0.5333333333333332\n",
      "0.6933333333333335 0.7333333333333334 0.6666666666666667\n",
      "0.72 0.8 0.6666666666666666\n",
      "0.47333333333333333 0.6333333333333333 0.4\n",
      "0.5933333333333334 0.9333333333333332 0.4666666666666666\n",
      "0.5533333333333333 0.7333333333333333 0.4666666666666666\n",
      "0.9333333333333332 0.9333333333333332 0.9333333333333332\n",
      "0.5466666666666666 0.6666666666666666 0.4666666666666666\n",
      "1.0 1.0 1.0\n",
      "0.49333333333333335 0.5333333333333334 0.4666666666666667\n",
      "0.9199999999999999 1.0 0.8666666666666666\n",
      "0.4666666666666666 0.6666666666666666 0.4\n",
      "0.4999999999999999 0.7 0.39999999999999997\n",
      "0.52 0.8 0.4\n",
      "0.72 0.8 0.6666666666666667\n",
      "0.6133333333333333 0.7333333333333333 0.5333333333333333\n",
      "0.5133333333333333 0.6333333333333333 0.4666666666666666\n",
      "0.45999999999999996 0.6 0.4\n",
      "0.58 0.8 0.4666666666666667\n",
      "1.0 1.0 1.0\n",
      "0.64 0.8 0.5333333333333332\n",
      "0.96 1.0 0.9333333333333332\n",
      "0.6 0.9 0.4666666666666666\n",
      "0.5333333333333333 0.6333333333333333 0.4666666666666668\n",
      "0.4666666666666667 0.6666666666666666 0.4000000000000001\n",
      "0.36 0.6 0.26666666666666666\n",
      "0.4066666666666666 0.5666666666666667 0.33333333333333337\n",
      "0.5333333333333334 0.6333333333333333 0.4666666666666666\n",
      "0.9333333333333332 0.9333333333333332 0.9333333333333332\n",
      "0.6599999999999999 0.9 0.5333333333333333\n",
      "0.58 0.8 0.4666666666666666\n",
      "0.6599999999999999 1.0 0.5333333333333334\n",
      "0.5399999999999999 0.9 0.39999999999999997\n",
      "0.6019047619047619 0.6333333333333333 0.5999999999999999\n",
      "0.4733333333333333 0.6333333333333333 0.4\n",
      "0.21333333333333332 0.2333333333333333 0.2\n",
      "0.39333333333333326 0.5333333333333333 0.33333333333333337\n",
      "0.7542857142857142 0.9 0.6666666666666667\n",
      "0.16 0.2 0.13333333333333333\n",
      "0.62 0.8 0.5333333333333333\n",
      "0.6533333333333333 0.7333333333333334 0.6\n",
      "1.0 1.0 1.0\n",
      "0.26666666666666666 0.4666666666666666 0.19999999999999998\n",
      "1.0 1.0 1.0\n",
      "0.7466666666666666 0.8666666666666666 0.6666666666666667\n",
      "0.8095238095238095 0.7666666666666666 0.8666666666666666\n",
      "0.44666666666666677 0.5666666666666667 0.4\n",
      "0.7599999999999999 0.9 0.6666666666666667\n",
      "0.8399999999999999 1.0 0.7333333333333334\n",
      "0.58 0.8 0.4666666666666668\n",
      "0.6399999999999999 0.7 0.6\n",
      "0.44000000000000006 0.7 0.33333333333333337\n",
      "0.9314285714285713 0.95 0.9333333333333332\n",
      "0.3466666666666666 0.36666666666666664 0.33333333333333337\n",
      "0.8933333333333333 0.9333333333333332 0.8666666666666666\n",
      "0.7466666666666667 0.8666666666666668 0.6666666666666667\n",
      "0.5133333333333333 0.8333333333333333 0.4\n",
      "0.37333333333333335 0.4333333333333334 0.33333333333333337\n",
      "0.6133333333333334 0.7333333333333334 0.5333333333333333\n",
      "1.0 1.0 1.0\n",
      "0.5 0.7 0.39999999999999997\n",
      "0.4076190476190476 0.4333333333333333 0.4000000000000001\n",
      "0.7314285714285714 0.75 0.7333333333333333\n",
      "0.3885714285714286 0.4 0.39999999999999997\n",
      "0.5619047619047619 0.5333333333333332 0.5999999999999999\n",
      "0.39999999999999997 0.5 0.33333333333333337\n",
      "0.19999999999999998 0.19999999999999998 0.19999999999999998\n",
      "0.6133333333333334 0.6333333333333334 0.6\n",
      "0.9333333333333332 0.9333333333333332 0.9333333333333332\n",
      "0.4266666666666666 0.6666666666666666 0.33333333333333337\n",
      "0.9199999999999999 1.0 0.8666666666666668\n",
      "0.3666666666666667 0.4666666666666667 0.33333333333333337\n",
      "0.45999999999999996 0.6000000000000001 0.39999999999999997\n",
      "0.6333333333333334 0.7333333333333334 0.6\n",
      "0.56 0.6 0.5333333333333334\n",
      "0.5276190476190475 0.5333333333333333 0.5333333333333333\n",
      "0.45999999999999996 0.8 0.3333333333333333\n",
      "0.8133333333333332 0.9333333333333332 0.7333333333333333\n",
      "0.39999999999999997 0.5 0.33333333333333337\n",
      "0.36 0.6 0.26666666666666666\n",
      "0.6866666666666666 0.8666666666666666 0.6\n",
      "0.18 0.3 0.13333333333333333\n",
      "0.6933333333333332 0.7333333333333333 0.6666666666666667\n",
      "0.5276190476190477 0.7333333333333333 0.4666666666666666\n",
      "0.6866666666666666 0.8666666666666666 0.6\n",
      "0.7933333333333332 0.9333333333333332 0.7333333333333334\n",
      "0.5133333333333334 0.8333333333333334 0.39999999999999997\n",
      "0.13333333333333333 0.13333333333333333 0.13333333333333333\n",
      "1.0 1.0 1.0\n",
      "0.5466666666666666 0.7666666666666666 0.4666666666666666\n",
      "1.0 1.0 1.0\n",
      "0.48 0.9 0.33333333333333337\n",
      "0.4809523809523809 0.5666666666666667 0.4666666666666666\n",
      "0.5066666666666666 0.5666666666666667 0.4666666666666666\n",
      "0.43999999999999995 0.4999999999999999 0.4\n",
      "0.8666666666666668 0.8666666666666668 0.8666666666666668\n",
      "0.39999999999999997 0.5 0.33333333333333337\n",
      "0.5285714285714285 0.5999999999999999 0.5333333333333332\n",
      "0.49333333333333335 0.7333333333333334 0.39999999999999997\n",
      "0.72 0.9 0.6\n",
      "0.5266666666666666 0.6666666666666667 0.4666666666666667\n",
      "0.5 0.7 0.4\n",
      "0.9 1.0 0.8666666666666666\n",
      "0.7466666666666667 0.8666666666666668 0.6666666666666667\n",
      "0.43999999999999995 0.4999999999999999 0.4000000000000001\n",
      "0.7133333333333334 0.9333333333333332 0.6\n",
      "0.6133333333333334 0.7333333333333334 0.5333333333333333\n",
      "0.18 0.3 0.13333333333333333\n",
      "0.3933333333333333 0.5333333333333333 0.33333333333333337\n",
      "0.30666666666666664 0.36666666666666664 0.26666666666666666\n",
      "0.43999999999999995 0.7 0.33333333333333337\n",
      "1.0 1.0 1.0\n",
      "0.42666666666666675 0.4666666666666667 0.4\n",
      "0.3066666666666667 0.3666666666666667 0.26666666666666666\n",
      "0.5533333333333335 0.7333333333333333 0.4666666666666666\n",
      "0.58 0.8 0.4666666666666666\n",
      "0.7466666666666667 0.8666666666666668 0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(f1Final)):\n",
    "    print(f1Final[i]/10, precFinal[i]/10, recaFinal[i]/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[[5, 6, 7], [5, 6, 8], [5, 6, 9], [5, 7, 8], [5, 7, 9], [5, 8, 9], [6, 7, 8], [6, 7, 9], [6, 8, 9], [7, 8, 9], [0, 5, 6], [0, 5, 7], [0, 5, 8], [0, 5, 9], [0, 6, 7], [0, 6, 8], [0, 6, 9], [0, 7, 8], [0, 7, 9], [0, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "random.seed(1000)\n",
    "biased_ratio = 64\n",
    "num_of_letters = 10\n",
    "TestDataset = create_dataset(testImagesVal, testLabelsVal, False)\n",
    "f1scoreVal = []\n",
    "num_of_train_images = 1000\n",
    "\n",
    "trainImagesComb, trainLabelsComb = createRatioBased(biased_ratio,trainImagesVal, trainLabelsVal)\n",
    "#trainImagesComb, trainLabelsComb =createOddEvenNew(trainImagesVal, trainLabelsVal)\n",
    "trainImages, trainLabels = incNumOfImages(trainImagesComb, trainLabelsComb, num_of_train_images)\n",
    "trainLabels = convertToArray(trainLabels)\n",
    "connectedness = connectednessMeasure(trainLabels, testLabelsVal)\n",
    "combList = createCombinationList(connectedness, 20)\n",
    "print(combList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "-0.11111111111111102 -0.11111111111111102 -0.11111111111111102\n",
      "-0.11111111111111102 -0.11111111111111105 -0.11111111111111105\n",
      "-0.11111111111111102 -0.11111111111111105 -0.11111111111111105\n",
      "-0.11111111111111102 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111102 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111102 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111102 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111102 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111102 -0.11111111111111105 -0.11111111111111105\n",
      "-0.11111111111111102 -0.11111111111111105 -0.11111111111111105\n",
      "-0.11111111111111102 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111102 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111102 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111102 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111102 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111105 -0.11111111111111105\n",
      "-0.11111111111111105 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111105 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111105 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111102 -0.11111111111111105 -0.11111111111111105\n",
      "-0.11111111111111102 -0.11111111111111105 -0.11111111111111105\n",
      "-0.11111111111111102 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111102 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111102 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111102 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111102 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111105 -0.11111111111111105\n",
      "-0.11111111111111105 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111105 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111105 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111105 -0.11111111111111105\n",
      "-0.11111111111111105 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111105 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111105 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111105 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111105 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111109 -0.11111111111111109\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111109 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n",
      "-0.11111111111111112 -0.11111111111111112 -0.11111111111111112\n"
     ]
    }
   ],
   "source": [
    "random.seed(1000)\n",
    "biased_ratio = 1\n",
    "num_of_letters = 10\n",
    "TestDataset = create_dataset(testImagesVal, testLabelsVal, False)\n",
    "f1scoreVal = []\n",
    "num_of_train_images = 1000\n",
    "\n",
    "trainImagesComb, trainLabelsComb = createRatioBased(biased_ratio, trainImagesVal, trainLabelsVal)\n",
    "trainImages, trainLabels = incNumOfImages(trainImagesComb, trainLabelsComb, num_of_train_images)\n",
    "trainLabels = convertToArray(trainLabels)\n",
    "arrayVal = correlationmt(trainLabels)\n",
    "y_test_num = convertToNum(testLabelsVal)\n",
    "for w in range(120):\n",
    "    i = y_test_num[w][0]\n",
    "    j = y_test_num[w][1]\n",
    "    k = y_test_num[w][2]\n",
    "    corr = np.corrcoef(arrayVal[i], arrayVal[j])[0][1]\n",
    "    cor1 = np.corrcoef(arrayVal[i], arrayVal[k])[0][1]\n",
    "    cor2 = np.corrcoef(arrayVal[k], arrayVal[j])[0][1]\n",
    "    abscorr = np.absolute(corr) + np.absolute(cor1) + np.absolute(cor2)\n",
    "    maxcorr = max(corr, cor1, cor2)\n",
    "    mincorr = min(corr, cor1, cor2)\n",
    "    if (np.absolute(maxcorr) >= np.absolute(mincorr)):\n",
    "        tempcorr = maxcorr\n",
    "    else:\n",
    "        tempcorr = (mincorr)\n",
    "    maxmincorr = abs(corr-cor1) + abs(cor1-cor2) + abs(cor2-corr)\n",
    "    print(corr, cor1, cor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 0.33333333333333304 -0.11111111111111102 -0.11111111111111102\n",
      "0 1 3 0.3333333333333331 -0.11111111111111102 -0.11111111111111105\n",
      "0 1 4 0.3333333333333331 -0.11111111111111102 -0.11111111111111105\n",
      "0 1 5 0.33333333333333315 -0.11111111111111102 -0.11111111111111109\n",
      "0 1 6 0.33333333333333315 -0.11111111111111102 -0.11111111111111109\n",
      "0 1 7 0.33333333333333315 -0.11111111111111102 -0.11111111111111112\n",
      "0 1 8 0.33333333333333315 -0.11111111111111102 -0.11111111111111112\n",
      "0 1 9 0.33333333333333315 -0.11111111111111102 -0.11111111111111112\n",
      "0 2 1 0.33333333333333304 -0.11111111111111102 -0.11111111111111102\n",
      "0 2 3 0.3333333333333331 -0.11111111111111102 -0.11111111111111105\n",
      "0 2 4 0.3333333333333331 -0.11111111111111102 -0.11111111111111105\n",
      "0 2 5 0.33333333333333315 -0.11111111111111102 -0.11111111111111109\n",
      "0 2 6 0.33333333333333315 -0.11111111111111102 -0.11111111111111109\n",
      "0 2 7 0.33333333333333315 -0.11111111111111102 -0.11111111111111112\n",
      "0 2 8 0.33333333333333315 -0.11111111111111102 -0.11111111111111112\n",
      "0 2 9 0.33333333333333315 -0.11111111111111102 -0.11111111111111112\n",
      "0 3 1 0.33333333333333315 -0.11111111111111102 -0.11111111111111105\n",
      "0 3 2 0.33333333333333315 -0.11111111111111102 -0.11111111111111105\n",
      "0 3 4 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "0 3 5 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "0 3 6 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "0 3 7 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "0 3 8 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "0 3 9 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "0 4 1 0.33333333333333315 -0.11111111111111102 -0.11111111111111105\n",
      "0 4 2 0.33333333333333315 -0.11111111111111102 -0.11111111111111105\n",
      "0 4 3 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "0 4 5 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "0 4 6 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "0 4 7 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "0 4 8 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "0 4 9 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "0 5 1 0.3333333333333332 -0.11111111111111102 -0.11111111111111109\n",
      "0 5 2 0.3333333333333332 -0.11111111111111102 -0.11111111111111109\n",
      "0 5 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "0 5 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "0 5 6 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "0 5 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "0 5 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "0 5 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "0 6 1 0.3333333333333332 -0.11111111111111102 -0.11111111111111109\n",
      "0 6 2 0.3333333333333332 -0.11111111111111102 -0.11111111111111109\n",
      "0 6 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "0 6 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "0 6 5 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "0 6 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "0 6 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "0 6 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "0 7 1 0.33333333333333326 -0.11111111111111102 -0.11111111111111112\n",
      "0 7 2 0.33333333333333326 -0.11111111111111102 -0.11111111111111112\n",
      "0 7 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "0 7 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "0 7 5 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "0 7 6 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "0 7 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "0 7 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "0 8 1 0.33333333333333326 -0.11111111111111102 -0.11111111111111112\n",
      "0 8 2 0.33333333333333326 -0.11111111111111102 -0.11111111111111112\n",
      "0 8 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "0 8 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "0 8 5 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "0 8 6 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "0 8 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "0 8 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "0 9 1 0.33333333333333326 -0.11111111111111102 -0.11111111111111112\n",
      "0 9 2 0.33333333333333326 -0.11111111111111102 -0.11111111111111112\n",
      "0 9 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "0 9 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "0 9 5 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "0 9 6 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "0 9 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "0 9 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "1 0 2 0.33333333333333304 -0.11111111111111102 -0.11111111111111102\n",
      "1 0 3 0.3333333333333331 -0.11111111111111102 -0.11111111111111105\n",
      "1 0 4 0.3333333333333331 -0.11111111111111102 -0.11111111111111105\n",
      "1 0 5 0.33333333333333315 -0.11111111111111102 -0.11111111111111109\n",
      "1 0 6 0.33333333333333315 -0.11111111111111102 -0.11111111111111109\n",
      "1 0 7 0.33333333333333315 -0.11111111111111102 -0.11111111111111112\n",
      "1 0 8 0.33333333333333315 -0.11111111111111102 -0.11111111111111112\n",
      "1 0 9 0.33333333333333315 -0.11111111111111102 -0.11111111111111112\n",
      "1 2 0 0.33333333333333304 -0.11111111111111102 -0.11111111111111102\n",
      "1 2 3 0.3333333333333331 -0.11111111111111102 -0.11111111111111105\n",
      "1 2 4 0.3333333333333331 -0.11111111111111102 -0.11111111111111105\n",
      "1 2 5 0.33333333333333315 -0.11111111111111102 -0.11111111111111109\n",
      "1 2 6 0.33333333333333315 -0.11111111111111102 -0.11111111111111109\n",
      "1 2 7 0.33333333333333315 -0.11111111111111102 -0.11111111111111112\n",
      "1 2 8 0.33333333333333315 -0.11111111111111102 -0.11111111111111112\n",
      "1 2 9 0.33333333333333315 -0.11111111111111102 -0.11111111111111112\n",
      "1 3 0 0.33333333333333315 -0.11111111111111102 -0.11111111111111105\n",
      "1 3 2 0.33333333333333315 -0.11111111111111102 -0.11111111111111105\n",
      "1 3 4 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "1 3 5 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "1 3 6 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "1 3 7 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "1 3 8 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "1 3 9 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "1 4 0 0.33333333333333315 -0.11111111111111102 -0.11111111111111105\n",
      "1 4 2 0.33333333333333315 -0.11111111111111102 -0.11111111111111105\n",
      "1 4 3 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "1 4 5 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "1 4 6 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "1 4 7 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "1 4 8 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "1 4 9 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "1 5 0 0.3333333333333332 -0.11111111111111102 -0.11111111111111109\n",
      "1 5 2 0.3333333333333332 -0.11111111111111102 -0.11111111111111109\n",
      "1 5 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "1 5 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "1 5 6 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "1 5 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "1 5 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "1 5 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "1 6 0 0.3333333333333332 -0.11111111111111102 -0.11111111111111109\n",
      "1 6 2 0.3333333333333332 -0.11111111111111102 -0.11111111111111109\n",
      "1 6 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "1 6 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "1 6 5 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "1 6 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "1 6 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "1 6 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "1 7 0 0.33333333333333326 -0.11111111111111102 -0.11111111111111112\n",
      "1 7 2 0.33333333333333326 -0.11111111111111102 -0.11111111111111112\n",
      "1 7 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "1 7 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "1 7 5 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "1 7 6 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "1 7 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "1 7 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "1 8 0 0.33333333333333326 -0.11111111111111102 -0.11111111111111112\n",
      "1 8 2 0.33333333333333326 -0.11111111111111102 -0.11111111111111112\n",
      "1 8 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "1 8 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "1 8 5 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "1 8 6 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "1 8 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "1 8 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "1 9 0 0.33333333333333326 -0.11111111111111102 -0.11111111111111112\n",
      "1 9 2 0.33333333333333326 -0.11111111111111102 -0.11111111111111112\n",
      "1 9 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "1 9 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "1 9 5 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "1 9 6 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "1 9 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "1 9 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "2 0 1 0.33333333333333304 -0.11111111111111102 -0.11111111111111102\n",
      "2 0 3 0.3333333333333331 -0.11111111111111102 -0.11111111111111105\n",
      "2 0 4 0.3333333333333331 -0.11111111111111102 -0.11111111111111105\n",
      "2 0 5 0.33333333333333315 -0.11111111111111102 -0.11111111111111109\n",
      "2 0 6 0.33333333333333315 -0.11111111111111102 -0.11111111111111109\n",
      "2 0 7 0.33333333333333315 -0.11111111111111102 -0.11111111111111112\n",
      "2 0 8 0.33333333333333315 -0.11111111111111102 -0.11111111111111112\n",
      "2 0 9 0.33333333333333315 -0.11111111111111102 -0.11111111111111112\n",
      "2 1 0 0.33333333333333304 -0.11111111111111102 -0.11111111111111102\n",
      "2 1 3 0.3333333333333331 -0.11111111111111102 -0.11111111111111105\n",
      "2 1 4 0.3333333333333331 -0.11111111111111102 -0.11111111111111105\n",
      "2 1 5 0.33333333333333315 -0.11111111111111102 -0.11111111111111109\n",
      "2 1 6 0.33333333333333315 -0.11111111111111102 -0.11111111111111109\n",
      "2 1 7 0.33333333333333315 -0.11111111111111102 -0.11111111111111112\n",
      "2 1 8 0.33333333333333315 -0.11111111111111102 -0.11111111111111112\n",
      "2 1 9 0.33333333333333315 -0.11111111111111102 -0.11111111111111112\n",
      "2 3 0 0.33333333333333315 -0.11111111111111102 -0.11111111111111105\n",
      "2 3 1 0.33333333333333315 -0.11111111111111102 -0.11111111111111105\n",
      "2 3 4 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "2 3 5 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "2 3 6 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "2 3 7 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "2 3 8 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "2 3 9 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "2 4 0 0.33333333333333315 -0.11111111111111102 -0.11111111111111105\n",
      "2 4 1 0.33333333333333315 -0.11111111111111102 -0.11111111111111105\n",
      "2 4 3 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "2 4 5 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "2 4 6 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "2 4 7 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "2 4 8 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "2 4 9 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "2 5 0 0.3333333333333332 -0.11111111111111102 -0.11111111111111109\n",
      "2 5 1 0.3333333333333332 -0.11111111111111102 -0.11111111111111109\n",
      "2 5 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "2 5 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "2 5 6 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "2 5 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "2 5 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "2 5 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "2 6 0 0.3333333333333332 -0.11111111111111102 -0.11111111111111109\n",
      "2 6 1 0.3333333333333332 -0.11111111111111102 -0.11111111111111109\n",
      "2 6 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "2 6 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "2 6 5 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "2 6 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "2 6 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "2 6 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "2 7 0 0.33333333333333326 -0.11111111111111102 -0.11111111111111112\n",
      "2 7 1 0.33333333333333326 -0.11111111111111102 -0.11111111111111112\n",
      "2 7 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "2 7 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "2 7 5 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "2 7 6 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "2 7 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "2 7 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "2 8 0 0.33333333333333326 -0.11111111111111102 -0.11111111111111112\n",
      "2 8 1 0.33333333333333326 -0.11111111111111102 -0.11111111111111112\n",
      "2 8 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "2 8 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "2 8 5 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "2 8 6 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "2 8 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "2 8 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "2 9 0 0.33333333333333326 -0.11111111111111102 -0.11111111111111112\n",
      "2 9 1 0.33333333333333326 -0.11111111111111102 -0.11111111111111112\n",
      "2 9 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "2 9 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "2 9 5 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "2 9 6 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "2 9 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "2 9 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "3 0 1 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "3 0 2 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "3 0 4 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "3 0 5 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "3 0 6 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "3 0 7 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "3 0 8 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "3 0 9 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "3 1 0 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "3 1 2 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "3 1 4 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "3 1 5 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "3 1 6 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "3 1 7 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "3 1 8 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "3 1 9 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "3 2 0 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "3 2 1 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "3 2 4 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "3 2 5 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "3 2 6 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "3 2 7 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "3 2 8 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "3 2 9 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "3 4 0 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "3 4 1 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "3 4 2 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "3 4 5 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "3 4 6 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "3 4 7 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "3 4 8 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "3 4 9 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "3 5 0 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "3 5 1 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "3 5 2 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "3 5 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "3 5 6 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "3 5 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "3 5 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "3 5 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "3 6 0 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "3 6 1 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "3 6 2 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "3 6 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "3 6 5 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "3 6 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "3 6 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "3 6 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "3 7 0 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "3 7 1 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "3 7 2 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "3 7 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "3 7 5 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "3 7 6 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "3 7 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "3 7 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "3 8 0 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "3 8 1 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "3 8 2 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "3 8 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "3 8 5 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "3 8 6 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "3 8 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "3 8 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "3 9 0 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "3 9 1 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "3 9 2 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "3 9 4 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "3 9 5 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "3 9 6 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "3 9 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "3 9 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "4 0 1 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "4 0 2 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "4 0 3 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "4 0 5 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "4 0 6 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "4 0 7 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "4 0 8 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "4 0 9 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "4 1 0 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "4 1 2 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "4 1 3 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1 5 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "4 1 6 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "4 1 7 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "4 1 8 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "4 1 9 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "4 2 0 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "4 2 1 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "4 2 3 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "4 2 5 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "4 2 6 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "4 2 7 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "4 2 8 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "4 2 9 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "4 3 0 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "4 3 1 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "4 3 2 0.33333333333333315 -0.11111111111111105 -0.11111111111111105\n",
      "4 3 5 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "4 3 6 0.3333333333333332 -0.11111111111111105 -0.11111111111111109\n",
      "4 3 7 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "4 3 8 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "4 3 9 0.3333333333333332 -0.11111111111111105 -0.11111111111111112\n",
      "4 5 0 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "4 5 1 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "4 5 2 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "4 5 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "4 5 6 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "4 5 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "4 5 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "4 5 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "4 6 0 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "4 6 1 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "4 6 2 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "4 6 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111109\n",
      "4 6 5 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "4 6 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "4 6 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "4 6 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "4 7 0 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "4 7 1 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "4 7 2 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "4 7 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "4 7 5 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "4 7 6 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "4 7 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "4 7 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "4 8 0 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "4 8 1 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "4 8 2 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "4 8 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "4 8 5 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "4 8 6 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "4 8 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "4 8 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "4 9 0 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "4 9 1 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "4 9 2 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "4 9 3 0.33333333333333326 -0.11111111111111105 -0.11111111111111112\n",
      "4 9 5 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "4 9 6 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "4 9 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "4 9 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "5 0 1 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 0 2 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 0 3 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 0 4 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 0 6 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 0 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 0 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 0 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 1 0 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 1 2 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 1 3 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 1 4 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 1 6 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 1 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 1 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 1 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 2 0 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 2 1 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 2 3 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 2 4 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 2 6 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 2 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 2 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 2 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 3 0 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 3 1 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 3 2 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 3 4 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 3 6 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 3 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 3 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 3 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 4 0 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 4 1 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 4 2 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 4 3 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 4 6 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 4 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 4 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 4 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 6 0 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 6 1 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 6 2 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 6 3 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 6 4 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "5 6 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 6 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 6 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 7 0 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 7 1 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 7 2 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 7 3 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 7 4 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 7 6 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 7 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "5 7 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "5 8 0 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 8 1 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 8 2 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 8 3 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 8 4 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 8 6 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 8 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "5 8 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "5 9 0 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 9 1 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 9 2 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 9 3 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 9 4 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 9 6 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "5 9 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "5 9 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "6 0 1 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 0 2 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 0 3 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 0 4 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 0 5 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 0 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 0 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 0 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 1 0 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 1 2 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 1 3 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 1 4 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 1 5 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 1 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 1 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 1 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 2 0 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 2 1 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 2 3 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 2 4 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 2 5 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 2 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 2 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 2 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 3 0 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 3 1 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 3 2 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 3 4 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 3 5 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 3 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 3 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 3 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 4 0 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 4 1 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 4 2 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 4 3 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 4 5 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 4 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 4 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 4 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 5 0 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 5 1 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 5 2 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 5 3 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 5 4 0.33333333333333326 -0.11111111111111109 -0.11111111111111109\n",
      "6 5 7 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 5 8 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 5 9 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 7 0 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 7 1 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 7 2 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 7 3 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 7 4 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 7 5 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 7 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "6 7 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "6 8 0 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 8 1 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 8 2 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 8 3 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 8 4 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 8 5 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 8 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "6 8 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "6 9 0 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 9 1 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 9 2 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 9 3 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 9 4 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 9 5 0.3333333333333333 -0.11111111111111109 -0.11111111111111112\n",
      "6 9 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "6 9 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 0 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 0 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 0 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 0 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 0 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 0 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 0 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 0 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 1 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 1 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 1 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 1 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 1 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 1 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 1 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 1 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 2 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 2 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 2 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 2 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 2 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 2 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 2 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 2 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 3 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 3 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 3 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 3 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 3 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 3 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 3 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 3 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 4 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 4 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 4 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 4 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 4 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 4 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 4 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 4 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 5 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 5 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 5 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 5 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 5 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 5 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 5 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 5 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 6 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 6 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 6 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 6 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 6 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 6 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 6 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 6 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 8 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 8 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 8 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 8 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 8 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 8 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 8 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 8 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 9 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 9 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 9 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 9 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 9 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 9 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 9 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "7 9 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 0 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 0 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 0 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 0 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 0 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 0 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 0 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 1 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 1 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 1 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 1 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 1 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 1 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 1 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 1 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 2 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 2 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 2 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 2 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 2 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 2 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 2 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 2 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 3 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 3 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 3 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 3 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 3 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 3 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 3 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 3 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 4 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 4 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 4 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 4 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 4 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 4 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 4 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 4 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 5 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 5 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 5 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 5 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 5 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 5 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 5 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 5 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 6 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 6 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 6 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 6 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 6 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 6 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 6 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 6 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 7 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 7 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 7 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 7 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 7 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 7 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 7 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 7 9 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 9 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 9 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 9 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 9 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 9 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 9 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 9 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "8 9 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 0 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 0 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 0 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 0 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 0 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 0 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 0 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 0 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 1 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 1 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 1 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 1 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 1 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 1 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 1 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 1 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 2 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 2 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 2 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 2 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 2 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 2 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 2 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 2 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 3 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 3 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 3 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 3 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 3 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 3 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 3 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 3 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 4 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 4 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 4 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 4 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 4 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 4 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 4 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 4 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 5 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 5 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 5 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 5 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 5 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 5 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 5 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 5 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 6 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 6 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 6 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 6 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 6 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 6 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 6 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 6 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 7 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 7 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 7 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 7 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 7 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 7 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 7 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 7 8 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 8 0 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 8 1 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 8 2 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 8 3 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 8 4 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 8 5 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 8 6 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n",
      "9 8 7 0.33333333333333337 -0.11111111111111112 -0.11111111111111112\n"
     ]
    }
   ],
   "source": [
    "arrayVal = correlationmt(trainLabels)\n",
    "for i in range(num_of_letters):\n",
    "    for j in range(num_of_letters):\n",
    "        print(i, j ,np.corrcoef(arrayVal[i], arrayVal[j])[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "['../pics/0_1_2_1.jpg', '../pics/0_1_3_1.jpg', '../pics/0_1_4_1.jpg', '../pics/0_1_5_1.jpg']\n"
     ]
    }
   ],
   "source": [
    "biased_ratio = 1\n",
    "num_of_letters = 5000\n",
    "trainImagesComb, trainLabelsComb = createRatioBased(biased_ratio, trainImagesVal, trainLabelsVal)\n",
    "trainImages, trainLabels = incNumOfImages(trainImagesComb, trainLabelsComb, num_of_train_images)\n",
    "trainLabels = convertToArray(trainLabels)\n",
    "print(trainImagesComb[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756 746\n"
     ]
    }
   ],
   "source": [
    "countA = 0\n",
    "countZ = 0\n",
    "for i in range(len(trainLabels)):\n",
    "    if trainLabels[i][0] == 1:\n",
    "        countA = countA + 1\n",
    "    if trainLabels[i][9] == 1:\n",
    "        countZ = countZ + 1\n",
    "print(countA, countZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1.0\n",
      "0 1 0.3749999999999999\n",
      "0 2 0.10206207261596577\n",
      "0 3 0.37499999999999994\n",
      "0 4 0.3749999999999999\n",
      "0 5 nan\n",
      "0 6 nan\n",
      "0 7 nan\n",
      "0 8 nan\n",
      "0 9 nan\n",
      "1 0 0.3749999999999999\n",
      "1 1 1.0\n",
      "1 2 0.10206207261596577\n",
      "1 3 0.37499999999999994\n",
      "1 4 0.3749999999999999\n",
      "1 5 nan\n",
      "1 6 nan\n",
      "1 7 nan\n",
      "1 8 nan\n",
      "1 9 nan\n",
      "2 0 0.10206207261596575\n",
      "2 1 0.10206207261596575\n",
      "2 2 1.0\n",
      "2 3 0.10206207261596575\n",
      "2 4 0.10206207261596575\n",
      "2 5 nan\n",
      "2 6 nan\n",
      "2 7 nan\n",
      "2 8 nan\n",
      "2 9 nan\n",
      "3 0 0.37499999999999994\n",
      "3 1 0.37499999999999994\n",
      "3 2 0.10206207261596577\n",
      "3 3 1.0\n",
      "3 4 0.37499999999999994\n",
      "3 5 nan\n",
      "3 6 nan\n",
      "3 7 nan\n",
      "3 8 nan\n",
      "3 9 nan\n",
      "4 0 0.3749999999999999\n",
      "4 1 0.3749999999999999\n",
      "4 2 0.10206207261596577\n",
      "4 3 0.37499999999999994\n",
      "4 4 1.0\n",
      "4 5 nan\n",
      "4 6 nan\n",
      "4 7 nan\n",
      "4 8 nan\n",
      "4 9 nan\n",
      "5 0 nan\n",
      "5 1 nan\n",
      "5 2 nan\n",
      "5 3 nan\n",
      "5 4 nan\n",
      "5 5 nan\n",
      "5 6 nan\n",
      "5 7 nan\n",
      "5 8 nan\n",
      "5 9 nan\n",
      "6 0 nan\n",
      "6 1 nan\n",
      "6 2 nan\n",
      "6 3 nan\n",
      "6 4 nan\n",
      "6 5 nan\n",
      "6 6 nan\n",
      "6 7 nan\n",
      "6 8 nan\n",
      "6 9 nan\n",
      "7 0 nan\n",
      "7 1 nan\n",
      "7 2 nan\n",
      "7 3 nan\n",
      "7 4 nan\n",
      "7 5 nan\n",
      "7 6 nan\n",
      "7 7 nan\n",
      "7 8 nan\n",
      "7 9 nan\n",
      "8 0 nan\n",
      "8 1 nan\n",
      "8 2 nan\n",
      "8 3 nan\n",
      "8 4 nan\n",
      "8 5 nan\n",
      "8 6 nan\n",
      "8 7 nan\n",
      "8 8 nan\n",
      "8 9 nan\n",
      "9 0 nan\n",
      "9 1 nan\n",
      "9 2 nan\n",
      "9 3 nan\n",
      "9 4 nan\n",
      "9 5 nan\n",
      "9 6 nan\n",
      "9 7 nan\n",
      "9 8 nan\n",
      "9 9 nan\n"
     ]
    }
   ],
   "source": [
    "arrayVal = correlationmt(trainLabels)\n",
    "for i in range(num_of_letters):\n",
    "    for j in range(num_of_letters):\n",
    "        print(i, j ,np.corrcoef(arrayVal[i], arrayVal[j])[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_num = convertToNum(testLabelsVal) \n",
    "cols = num_of_letters\n",
    "rows = num_of_letters\n",
    "f1matrix = np.zeros((cols, rows))\n",
    "accmatrix = np.zeros((cols, rows))\n",
    "precisionMat = np.zeros((cols, rows))\n",
    "recallMat = np.zeros((cols, rows))\n",
    "count = np.zeros((cols, rows))\n",
    "for i in range(len(y_test_num)):\n",
    "    for j in range(len(y_test_num[i])):\n",
    "        for k in range(j, len(y_test_num[i])):\n",
    "            t = y_test_num[i][j]\n",
    "            w = y_test_num[i][k] \n",
    "            if y_test_num[i][j] != y_test_num[i][k]:\n",
    "                if y_predicted[i][t] == 1 and y_predicted[i][w] == 1:\n",
    "                    f1Value = 1\n",
    "                elif y_predicted[i][t] == 0 and y_predicted[i][w] == 0:\n",
    "                    f1Value = 0\n",
    "                elif (y_predicted[i][t] == 1 and y_predicted[i][w] == 0) or (y_predicted[i][t] == 0 and y_predicted[i][w] == 1):\n",
    "                    f1Value = 0.5\n",
    "                accmatrix[t][w] = accmatrix[t][w] + f1Value\n",
    "                f1matrix[t][w] = f1matrix[t][w] + f1Val[i]\n",
    "                precisionMat[t][w] = precisionMat[t][w]  + precVal[i]\n",
    "                count[t][w] = count[t][w] + 1\n",
    "                recallMat[t][w]  = recallMat[t][w] + recaVal[i]\n",
    "                f1matrix[w][t] = f1matrix[w][t] + f1Val[i]\n",
    "                accmatrix[w][t] = accmatrix[w][t] + f1Value\n",
    "                precisionMat[w][t] = precisionMat[w][t]  + precVal[i]\n",
    "                count[w][t] = count[w][t] + 1\n",
    "                recallMat[w][t]  = recallMat[w][t] + recaVal[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LatestExperiments.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python385jvsc74a57bd04129fa68b755e195f0d5fcb3e59c778ef0a4b04af35956d9f07a82e099c4068e",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "4129fa68b755e195f0d5fcb3e59c778ef0a4b04af35956d9f07a82e099c4068e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}